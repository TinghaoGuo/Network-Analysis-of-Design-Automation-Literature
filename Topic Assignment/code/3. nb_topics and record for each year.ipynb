{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file_path = \"../data/DAC_Entire_DataBase_2016.json\"\n",
    "\n",
    "with open(file_path, \"r\") as f:\n",
    "    database= json.load(f)\n",
    "    \n",
    "with open('r2_keyword_list_post.csv') as r2_keyword_list:\n",
    "    reader = csv.reader(r2_keyword_list, delimiter=',')\n",
    "    w_id = 0\n",
    "    nTopic_id = 0\n",
    "    bTopic_id = 0\n",
    "    w_dic = {}\n",
    "    nTopic_dic = {}\n",
    "    bTopic_dic = {}\n",
    "    w_nTopic = {}\n",
    "    w_bTopic = {}\n",
    "    for row in reader:\n",
    "        if row[0] not in w_dic:\n",
    "            w_id += 1\n",
    "            w_dic[row[0]] = w_id\n",
    "            w_nTopic[w_id] = []\n",
    "            w_bTopic[w_id] = []\n",
    "        for nTopic in [row[1], row[2], row[3]]:\n",
    "            if nTopic:\n",
    "                if nTopic not in nTopic_dic:\n",
    "                    nTopic_id += 1\n",
    "                    nTopic_dic[nTopic] = nTopic_id\n",
    "                w_nTopic[w_id].append(nTopic_dic[nTopic])\n",
    "        for bTopic in [row[4], row[5], row[6]]:\n",
    "            if len(bTopic)>0:\n",
    "                if bTopic not in bTopic_dic:\n",
    "                    bTopic_id += 1\n",
    "                    bTopic_dic[bTopic] = bTopic_id\n",
    "                w_bTopic[w_id].append(bTopic_dic[bTopic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_candidate_chunks(text, grammar=r'KT: {(<JJ|VBG|VBD|VBN|RB>* <NN.*>+ <IN>)? <JJ|VBG|VBD|VBN|RB>* <NN.*>+}'):\n",
    "    import itertools, nltk, string\n",
    "    \n",
    "    # exclude candidates that are stop words or entirely punctuation\n",
    "    punct = set(string.punctuation)\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    # tokenize, POS-tag, and chunk using regular expressions\n",
    "    chunker = nltk.chunk.regexp.RegexpParser(grammar)\n",
    "    tagged_sents = nltk.pos_tag_sents(nltk.word_tokenize(sent) for sent in nltk.sent_tokenize(text))\n",
    "    all_chunks = list(itertools.chain.from_iterable(nltk.chunk.tree2conlltags(chunker.parse(tagged_sent))\n",
    "                                                    for tagged_sent in tagged_sents))\n",
    "    # join constituent chunk words into a single chunked phrase\n",
    "    candidates = [' '.join(word for word, pos, chunk in group).lower()\n",
    "                  for key, group in itertools.groupby(all_chunks, lambda (word,pos,chunk): chunk != 'O') if key]\n",
    "    \n",
    "    cand_list = []\n",
    "    for cand in candidates:\n",
    "        if cand not in stop_words and not all(char in punct for char in cand):\n",
    "            cand_list.append(cand)\n",
    "    return cand_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#check if extracted chunk should be counted towards key_chunk\n",
    "def measure_similarity(key_chunk, cur_chunk):\n",
    "    stemmer = nltk.stem.porter.PorterStemmer()\n",
    "    same = True\n",
    "    words_in_key = word_tokenize(key_chunk)\n",
    "    words_in_cur = word_tokenize(cur_chunk)\n",
    "    key_list = []\n",
    "    cur_list = []\n",
    "    for word in words_in_key:\n",
    "        stem_word = stemmer.stem_word(word)\n",
    "        stem_word = stem_word.lower()\n",
    "        key_list.append(stem_word)\n",
    "    for cur_word in words_in_cur:\n",
    "        stem_cur_word = stemmer.stem_word(cur_word)\n",
    "        stem_cur_word = stem_cur_word.lower()\n",
    "        cur_list.append(stem_cur_word)\n",
    "    for key_part in key_list:\n",
    "        if key_part not in set(cur_list):\n",
    "            same = False\n",
    "    return same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import sys\n",
    "from nltk import word_tokenize\n",
    "years = range(2002, 2016)\n",
    "year_file_name = '{}_with_id.csv'\n",
    "year_record_name = '{}_record.txt'\n",
    "labels14 = []\n",
    "w_labels14 = []\n",
    "nTopic_labels14 = []\n",
    "bTopic_labels14 = []\n",
    "\n",
    "for year in years:\n",
    "    file_name = year_file_name.format(year)\n",
    "    \n",
    "    texts = []\n",
    "    ind_id_dic = {}\n",
    "    labels = {}\n",
    "    w_labels = {}\n",
    "    nTopic_labels = {}\n",
    "    bTopic_labels = {}\n",
    "    index = 0\n",
    "    whole_record = []\n",
    "    with open(file_name) as paper:\n",
    "        reader = csv.DictReader(paper)\n",
    "        for row in reader:\n",
    "            text_list = []\n",
    "            text_list.append(row['title'])\n",
    "            text_list.append(row['abstract'])\n",
    "            text = \". \".join(text_list)\n",
    "            texts.append(text)\n",
    "            ind_id_dic[index] = row['id']\n",
    "            index += 1\n",
    "    whole_records = []\n",
    "    for i in range(len(texts)):\n",
    "        text = texts[i]\n",
    "        nTopic_scores = {}\n",
    "        bTopic_scores = {}\n",
    "        keyword_scores = {}\n",
    "        records = []\n",
    "        try:\n",
    "            extracted_chunks = extract_candidate_chunks(text)\n",
    "        except UnicodeDecodeError:\n",
    "            text = text.decode('ascii', 'ignore').encode('ascii')\n",
    "            extracted_chunks = extract_candidate_chunks(text)\n",
    "        for extracted_chunk in extracted_chunks:\n",
    "            nTopic_set = set([])\n",
    "            bTopic_set = set([])\n",
    "            for phrase in w_dic.keys():\n",
    "                try: \n",
    "                    same = measure_similarity(phrase, extracted_chunk)\n",
    "                except UnicodeDecodeError:\n",
    "                    extracted_chunk = extracted_chunk.decode('ascii', 'ignore').encode('ascii')\n",
    "                    phrase = phrase.decode('ascii', 'ignore').encode('ascii')\n",
    "                    same = measure_similarity(phrase, extracted_chunk)\n",
    "                if same:\n",
    "                    records.append((extracted_chunk, phrase))\n",
    "                    w_id = w_dic[phrase]\n",
    "                    keyword_scores.setdefault(w_id, 0)\n",
    "                    keyword_scores[w_id] += 1\n",
    "                    nTopics = w_nTopic[w_id]\n",
    "                    nTopic_set = nTopic_set.union(set(nTopics))\n",
    "                    bTopics = w_bTopic[w_id]\n",
    "                    bTopic_set = bTopic_set.union(set(bTopics))\n",
    "            for nTopic in nTopic_set:\n",
    "                nTopic_scores.setdefault(nTopic, 0)\n",
    "                nTopic_scores[nTopic] += 1\n",
    "            for bTopic in bTopic_set:\n",
    "                bTopic_scores.setdefault(bTopic, 0)\n",
    "                bTopic_scores[bTopic] += 1\n",
    "        topics_for_text = [keyword_scores, nTopic_scores, bTopic_scores]\n",
    "        labels[ind_id_dic[i]] = topics_for_text\n",
    "        w_labels[ind_id_dic[i]] = keyword_scores\n",
    "        nTopic_labels[ind_id_dic[i]] = nTopic_scores\n",
    "        bTopic_labels[ind_id_dic[i]] = bTopic_scores\n",
    "        whole_record.append(records)\n",
    "        \n",
    "        record_name = year_record_name.format(year)\n",
    "        with open(record_name, 'w') as outfile:\n",
    "            index = 1\n",
    "            writer = csv.writer(outfile)\n",
    "            for records in whole_record:\n",
    "                writer.writerow([index])\n",
    "                index += 1\n",
    "                for record in records:\n",
    "                    writer.writerow([record])\n",
    "    \n",
    "    labels14.append(labels)\n",
    "    w_labels14.append(w_labels)\n",
    "    nTopic_labels14.append(nTopic_labels)\n",
    "    bTopic_labels14.append(bTopic_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print w_dic.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# nt_ids = []\n",
    "# for paper_id, paper_ntopic in nTopic_labels.items():\n",
    "#     for ntopic_id, freq in paper_ntopic.items():\n",
    "#         if ntopic_id not in nt_ids:\n",
    "#             nt_ids.append(ntopic_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# max(nt_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# nTopic_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "num_nTopic = 50\n",
    "year = 2002\n",
    "year_file_name = '{}_nTopic_Freq.xlsx'\n",
    "for nTopic_labels in nTopic_labels14:\n",
    "    list_for_df = []\n",
    "    for paper_id, nTopic_label_dic in nTopic_labels.items():\n",
    "        ini_nTopic_label_list = [0] * (num_nTopic +1)\n",
    "        ini_nTopic_label_list[0] = paper_id\n",
    "        for nTopic, freq in nTopic_label_dic.items():\n",
    "            ini_nTopic_label_list[nTopic] = freq\n",
    "        list_for_df.append(ini_nTopic_label_list)\n",
    "    table = pd.DataFrame(list_for_df)\n",
    "    table = table.sort_values(by=[0], ascending = True)\n",
    "    file_name = year_file_name.format(year)\n",
    "    writer = pd.ExcelWriter(file_name)\n",
    "    table.to_excel(writer)\n",
    "    writer.save()\n",
    "    year += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bt_ids = []\n",
    "for paper_id, paper_btopic in bTopic_labels.items():\n",
    "    for btopic_id, freq in paper_btopic.items():\n",
    "        if btopic_id not in bt_ids:\n",
    "            bt_ids.append(btopic_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# nt_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "num_bTopic = 10\n",
    "year = 2002\n",
    "year_file_name = '{}_bTopic_Freq.xlsx'\n",
    "for bTopic_labels in bTopic_labels14:\n",
    "    list_for_df = []\n",
    "    for paper_id, bTopic_label_dic in bTopic_labels.items():\n",
    "        ini_bTopic_label_list = [0] * (num_bTopic +1)\n",
    "        ini_bTopic_label_list[0] = paper_id\n",
    "        for bTopic, freq in bTopic_label_dic.items():\n",
    "            ini_bTopic_label_list[bTopic] = freq\n",
    "        list_for_df.append(ini_bTopic_label_list)\n",
    "    table = pd.DataFrame(list_for_df)\n",
    "    table = table.sort_values(by=[0], ascending = True)\n",
    "    file_name = year_file_name.format(year)\n",
    "    writer = pd.ExcelWriter(file_name)\n",
    "    table.to_excel(writer)\n",
    "    writer.save()\n",
    "    year += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('50_nTopics.txt', 'w') as outfile:\n",
    "    writer = csv.writer(outfile)\n",
    "    nTopic_header = [\"\"]*num_nTopic\n",
    "    for nTopic, index in nTopic_dic.items():\n",
    "        nTopic_header[index-1] = (index, nTopic)\n",
    "    for topic_index in nTopic_header:\n",
    "        writer.writerow([topic_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#write broad topic dic into txt file\n",
    "with open('10_bTopics.txt', 'w') as outfile:\n",
    "    writer = csv.writer(outfile)\n",
    "    bTopic_header = [\"\"]*num_bTopic\n",
    "    for bTopic, index in bTopic_dic.items():\n",
    "        bTopic_header[index-1] = (index, bTopic)\n",
    "    for topic_index in bTopic_header:\n",
    "        writer.writerow([topic_index])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
