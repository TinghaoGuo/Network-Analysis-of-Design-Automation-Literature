id,title,abstract,year
1102,Electromagnetic Design of In-Vehicle Reactor Using a Level-Set Based Topology Optimization Method,"In this paper, we propose a level-set based topology optimization method for designing a reactor, which is used as a part of the DC-DC converter in electric and hybrid vehicles. Since it realizes a high-power driving motor and its performance relies on its component, i.e., reactor core, it is valuable to establish a reasonable design method for the reactor core. Boundary tracking type level-set topology optimization is suitable for this purpose, because the shape and topology of the target structure is clearly represented by the zero boundary of the level-set function, and the state variables are accurately computed using the zero boundary tracking mesh. We formulate the design problem on the basis of electromagnetics, and derive the design sensitivities. The derived sensitivities are linked with boundary tracking type level-set topology optimization, and as a result, a useful structural optimization method for the reactor core design problem is developed.",2014
1103,Image Matching Assessment of Attainable Topology via Kriging-Interpolated Level-Sets,"Level-set methods are domain classification techniques that are gaining popularity in the recent years for structural topology optimization. Level sets classify a domain into two or more categories (such as material and void) by examining the value of a scalar level-set function (LSF) defined in the entire design domain. In most level-set formulations, a large number of design variables, or degrees of freedom is used to define the LSF, which implicitly defines the structure. The large number of design variables makes non-gradient optimization techniques all but ineffective. Kriging-interpolated level sets (KLS) on the other hand are formulated with an objective to enable non-gradient optimization by defining the design variables as the LSF values at few select locations (knot points) and using a Kriging model to interpolate the LSF in the rest of the design domain. A downside of concern when adopting KLS, is that using too few knot points may limit the capability to represent complex shapes, while using too many knot points may cause difficulty for non-gradient optimization. This paper presents a study of the effect of number and layout of the knot points in KLS on the capability to represent complex topologies in single and multi-component structures. Image matching error metrics are employed to assess the degree of mismatch between target topologies and those best-attainable via KLS. Results are presented in a catalogue-style in order to facilitate appropriate selection of knot-points by designers wishing to apply KLS for topology optimization.",2014
1104,Multi-Objective Selection of Cutting Conditions in Advanced Machining Processes via an Efficient Global Optimization Approach,"Optimum selection of cutting conditions in high-speed and ultra-precision machining processes often poses a challenging task due to several reasons; such as the need for costly experimental setup and the limitation on the number of experiments that can be performed before tool degradation starts becoming a source of noise in the readings. Moreover, oftentimes there are several objectives to consider, some of which may be conflicting, while others may be somewhat correlated. Pareto-optimality analysis is needed for conflicting objectives; however the existence of several objectives (high-dimension Pareto space) makes the generation and interpretation of Pareto solutions difficult. The approach adopted in this paper is a modified multi-objective efficient global optimization (m-EGO). In m-EGO, sample data points from experiments are used to construct Kriging meta-models, which act as predictors for the performance objectives. Evolutionary multi-objective optimization is then conducted to spread a population of new candidate experiments towards the zones of search space that are predicted by the Kriging models to have favorable performance, as well as zones that are under-explored. New experiments are then used to update the Kriging models, and the process is repeated until termination criteria are met. Handling a large number of objectives is improved via a special selection operator based on principle component analysis (PCA) within the evolutionary optimization. PCA is used to automatically detect correlations among objectives and perform the selection within a reduced space in order to achieve a better distribution of experimental sample points on the Pareto frontier. Case studies show favorable results in ultra-precision diamond turning of Aluminum alloy as well as high-speed drilling of woven composites.",2014
1105,Development of a Design Tool for Flow Rate Optimization in the Tata Swach Water Filter,"When developing a first-generation product, an iterative approach often yields the shortest time-to-market. In order to optimize its performance, however, a fundamental understanding of the theory governing its operation becomes necessary. This paper details the optimization of the Tata Swach, a consumer water purifier produced for India. The primary objective of the work was to increase flow rate while considering other factors such as cost, manufacturability, and efficacy. A mathematical model of the flow characteristics through the filter was developed. Based on this model, a design tool was created to allow designers to predict flow behavior without prototyping, significantly reducing the necessity of iteration. Sensitivity analysis was used to identify simple ways to increase flow rate as well as potential weak points in the design. Finally, it was demonstrated that maximum flow rate can be increased by 50% by increasing the diameter of a flow-restricting feature while simultaneously increasing the length of the active purification zone. This can be accomplished without significantly affecting cost, manufacturability, and efficacy.",2014
1106,Sketching in Air: A Single Stroke Classification Framework,"We describe a trainable, hand drawn, single stroke 3D sketch–based classification system, using a motion detecting depth sense camera. Our system captures data from a user, who is free to sketch any desired shape in a 3D environment. The overall system is based on a set of previously defined and well developed classifiers, which are, the Rubine Classifier, $1 recognizer and the Image based classifier. The novelty of this paper comes from 1) the classification of sketches drawn in a 3D environment; 2) extending the pixel based image representation to a voxel–based scheme; and 3) combining the results from individual classifiers using a sensitivity matrix. To evaluate the performance of the system, user studies were performed. To validate the significance of results obtained from the user studies, we performed a t–test. Our system outperforms the individual classifiers and is able to achieve an average overall accuracy of 93+%.",2014
1107,Large-Scale Topology Optimization Using Parameterized Boolean Networks,"A novel parameterization concept for structural truss topology optimization is presented in this article that enables the use of evolutionary algorithms in design of large-scale structures. The representational power of Boolean networks is used here to parameterize truss topology. A genetic algorithm then operates on parameters that govern the generation of truss topologies using this random network instead of operating directly on design variables. A genetic algorithm implementation is also presented that is congruent with the local rule application of the random network. The primary advantage of using a Boolean random network representation is that a relatively large number of ground structure nodes can be used, enabling successful exploration of a large-scale design space. In the classical binary representation of ground structures, the number of optimization variables increases quadratically with the number of nodes, restricting the maximum number of nodes that can be considered using a ground structure approach. The Boolean random network representation proposed here allows for the exploration of the entire topology space in a systematic way using only a linear number of variables. The number of nodes in the design domain, therefore, can be increased significantly. Truss member geometry and size optimization is performed here in a nested manner where an inner loop size optimization problem is solved for every candidate topology using sequential linear programming with move-limits. The Boolean random network and nested inner-loop optimization allows for the concurrent optimization of truss topology, geometry, and size. The effectiveness of this method is demonstrated using a planar truss design optimization benchmark problem.",2014
1108,Strategies for Topologic and Parametric Rule Application in Automated Design Synthesis Using Graph Grammars,"Computational Design Synthesis (CDS) is used to enable the computer to generate valid and even creative solutions for an engineering task. Graph grammars are a CDS approach in which engineering knowledge is formalized using graphs to represent designs and rules that describe possible graph transformations, i.e. changes of designs. For most engineering tasks two different kinds of rules are required: rules that change the topology and rules that change parameters of a design. One of the main challenges in CDS using both topologic and parametric rules is to decide ",2014
1109,Restart Strategies for Constraint-Handling in Generative Design Systems,"Product alternatives suggested by a generative design system often need to be evaluated on qualitative criteria. This evaluation necessitates that several feasible solutions which fulfill all technical constraints can be proposed to the user of the system. Also, as concept development is an iterative process, it is important that these solutions are generated quickly; i.e., the system must have a low convergence time. A problem, however, is that stochastic constraint-handling techniques can have highly unpredictable convergence times, spanning several orders of magnitude, and might sometimes not converge at all. A possible solution to avoid the lengthy runs is to restart the search after a certain time, with the hope that a new starting point will lead to a lower overall convergence time, but selecting an optimal restart-time is not trivial. In this paper, two strategies are investigated for such selection, and their performance is evaluated on two constraint-handling techniques for a product design problem. The results show that both restart strategies can greatly reduce the overall convergence time. Moreover, it is shown that one of the restart strategies can be applied to a wide range of constraint-handling techniques and problems, without requiring any fine-tuning of problem-specific parameters.",2014
1110,Visualization Tool for Interpreting User Needs From User-Generated Content via Text Mining and Classification,"The amount of user-generated content related to consumer products continues to grow as users increasingly take advantage of forums, product review sites, and social media platforms. The content is a promising source of insight into users’ needs and experiences. However, the challenge remains as to how concise and useful insights can be extracted from large quantities of unstructured data. We propose a visualization tool which allows designers to quickly and intuitively sift through large amounts of user-generated content and derive useful insights regarding users’ perceptions of product features. The tool leverages machine learning algorithms to automate labor-intensive portions of the process, and no manual labeling is required by the designer. Language processing techniques are arranged in a novel way to guide the designer in selecting the appropriate inputs, and multidimensional scaling enables presentation of the results in concise 2D plots. To demonstrate the efficacy of the tool, a case study is performed on action cameras. Product reviews from Amazon.com are analyzed as the user-generated content. Results from the case study show that the tool is helpful in condensing large amounts of user-generated content into useful insights, such as the key differentiations that users perceive among similar products.",2014
1111,Facilitating Design-by-Analogy: Development of a Complete Functional Vocabulary and Functional Vector Approach to Analogical Search,"Design-by-analogy is an effective approach to innovative concept generation, but can be elusive at times due to the fact that few methods and tools exist to assist designers in systematically seeking and identifying analogies from general data sources, databases, or repositories, such as patent databases. A new method for extracting analogies from data sources has been developed to provide this capability. Building on past research, we utilize a functional vector space model to quantify analogous similarity between a design problem and the data source of potential analogies. We quantitatively evaluate the functional similarity between represented design problems and, in this case, patent descriptions of products. We develop a complete functional vocabulary to map the patent database to applicable functionally critical terms, using document parsing algorithms to reduce text descriptions of the data sources down to the key functions, and applying Zipf’s law on word count order reduction to reduce the words within the documents. The reduction of a document (in this case a patent) into functional analogous words enables the matching to novel ideas that are functionally similar, which can be customized in various ways. This approach thereby provides relevant sources of design-by-analogy inspiration. Although our implementation of the technique focuses on functional descriptions of patents and the mapping of these functions to those of the design problem, resulting in a set of analogies, we believe that this technique is applicable to other analogy data sources as well. As a verification of the approach, an original design problem for an automated window washer illustrates the distance range of analogical solutions that can be extracted, extending from very near-field, literal solutions to far-field cross-domain analogies. Finally, a comparison with a current patent search tool is performed to draw a contrast to the status quo and evaluate the effectiveness of this work.",2014
1112,"Predictive, Data-Driven Product Family Design","Predictive design analytics is a new paradigm to enable design engineers to extract knowledge from large-scale, multi-dimensional, unstructured, volatile data, and transform the knowledge and its trend into design decision making. Predictive, data-driven family design (PDFD) is proposed as one of the predictive design analytics methods to tackle some issues in family design. First, a number and specifications of product architectures are determined by data (not by pre-defined market segments) in order to maximize expected profit. A trade-off between price and cost in terms of the quantity and specifications of architectures helps to set the target in the enterprise level. k-means clustering is used to find architectures that minimize within architecture sum of squared errors. Second, a price prediction method as a function of product performance and deviations between performance and customer requirements is suggested with exponential smoothing based on innovations state space models. Regression coefficients are treated as customer preferences over product performance, and analyzed as a time series. Prediction intervals are proposed to show market uncertainties. Third, multiple values for common parameters in family design can be identified using the expectation maximization clustering so that multiple-platform design can be explored. Last, large-scale data can be handled by the PDFD algorithm. A data set which contains a total of 14 million instances is used in the case study. The design of a family of universal electronic motors demonstrates the proposed approach and highlights its benefits and limitations.",2014
1113,Improving Preference Prediction Accuracy With Feature Learning,"Motivated by continued interest within the design community to model design preferences, this paper investigates the question of predicting preferences with particular application to consumer purchase behavior: How can we obtain high prediction accuracy in a consumer preference model using market purchase data? To this end, we employ sparse coding and sparse restricted Boltzmann machines, recent methods from machine learning, to transform the original market data into a sparse and high-dimensional representation. We show that these ‘feature learning’ techniques, which are independent from the preference model itself (e.g., logit model), can complement existing efforts towards high-accuracy preference prediction. Using actual passenger car market data, we achieve significant improvement in prediction accuracy on a binary preference task by properly transforming the original consumer variables and passenger car variables to a sparse and high-dimensional representation.",2014
1114,Effect of Expert Data Variability in the Change Prediction Method,"The Change Prediction Method is an approach that has been proposed in the literature as a way to assess the risk of change propagation. This approach requires experts to define the elements of the design structure matrix and provide both impact and likelihood values for each subsystem interaction. The combined risk values produced by the Change Prediction Method indicate where high probabilities of propagation may exist, but the results rely heavily on the supplied expert data. This study explores how potential variability in expert data impacts the rank order of returned risk values from the Change Prediction Method. Results are presented that indicate significant changes in rank order, highlighting both the importance of expert data accuracy and the insights that can be gained from the Change Prediction Method as a design tool.",2014
1115,The Effect of Product Representation in Visual Conjoint Analysis,"When most designers set out to develop a new product they solicit feedback from potential consumers. These data are incorporated into the design process in an effort to more effectively meet customer requirements. Often these data are used to construct a model of consumer preference capable of evaluating candidate designs. Although the mechanics of these models have been extensively studied there are still some open questions, particularly with respect to models of aesthetic preference. When constructing preference models, simplistic product representations are often favored over high fidelity product models in order to save time and expense. This work investigates how choice of product representation can affect model performance in visual conjoint analysis. Preference models for a single product, a table knife, are derived using three different representation schemes; simple sketches, solid models, and 3D printed models. Each of these representations is used in a separate conjoint analysis survey. The results from this study showed that consumer responses were inconsistent and potentially contradictory between different representations. Consequently, when using conjoint analysis for product innovation, obtaining a true understanding of consumer preference requires selecting representations based on how accurately they convey the product details in question.",2014
1116,Independence of Irrelevant Alternatives in Engineering Design,"When discussing Arrow’s Impossibility Theorem (AIT) in engineering design, we find that one condition, Independence of Irrelevant Alternatives (IIA), has been misunderstood generally. In this paper, two types of IIA are distinguished. One is based on Kenneth Arrow (IIA-A) that concerns the rationality condition of a collective choice rule (CCR). Another one is based on Amartya Sen (IIA-S) that is a condition for a choice function (CF). Through the analysis of IIA-A, this paper revisits three decision methods (i.e., Pugh matrix, Borda count and Quality Function Deployment) that have been criticized for their failures in some situations. It is argued that the violation of IIA-A does not immediately imply irrationality in engineering design, and more detailed analysis should be applied to examine the meaning of “irrelevant information”. Alternatively, IIA-S is concerned with the transitivity of CF, and it is associated with contraction consistency (Property α) and expansion consistency (Property β). It is shown that IIA-A and IIA-S are technically distinct and should not be confused in the rationality arguments. Other versions of IIA-A are also introduced to emphasize the significance of mathematical clarity in the discussion of AIT-related issues.",2014
1117,Decision Support Systems Design for Data-Driven Management,"This article discusses a design methodology for a Decision Support System (DSS) in the area of Data-Driven Management (DDM). We partition the DSS into an offline and an online system. Through rigorous testing, the offline system finds the best combination of Data Mining (DM) and Artificial Intelligence (AI) algorithms. Only the best algorithms are used in the online system to extract information from data and to make sense of this information by providing an objective second opinion on a decision result. To support the proposed design methodology, we construct a DSS that uses DM methods for market segmentation and AI methods for product positioning. As part of the offline system construction, we evaluate four intrinsic dimension estimation, three dimension reduction and four clustering algorithms. The performance is evaluated with statistical methods, silhouette mean and 10-fold stratified cross validated classification accuracy. We find that every DSS problem requires us to search a suitable algorithm structure, because different algorithms, for the same task, have different merits and shortcomings and it is impossible to know a priory which combination of algorithms gives the best results. Therefore, to select the best algorithms is empirical science where the possible combinations are tested. With this study, we deliver a blueprint on how to construct a DSS for product positioning. The proposed design methodology can be easily adopted to serve in a wide range of DDM problems.",2014
1118,A New Multi-Objective Mixed-Discrete Particle Swarm Optimization Algorithm,"Complex system design problems tend to be high dimensional and nonlinear, and also often involve multiple objectives and mixed-integer variables. Heuristic optimization algorithms have the potential to address the typical (if not most) characteristics of such complex problems. Among them, the Particle Swarm Optimization (PSO) algorithm has gained significant popularity due to its maturity and fast convergence abilities. This paper seeks to translate the unique benefits of PSO from solving typical continuous single-objective optimization problems to solving multi-objective mixed-discrete problems, which is a relatively new ground for PSO application. The previously developed Mixed-Discrete Particle Swarm Optimization (MDPSO) algorithm, which includes an exclusive diversity preservation technique to prevent premature particle clustering, has been shown to be a powerful single-objective solver for highly constrained MINLP problems. In this paper, we make fundamental advancements to the MDPSO algorithm, enabling it to solve challenging multi-objective problems with mixed-discrete design variables. In the velocity update equation, the explorative term is modified to point towards the non-dominated solution that is the closest to the corresponding particle (at any iteration). The fractional domain in the diversity preservation technique, which was previously defined in terms of a single global leader, is now applied to multiple global leaders in the intermediate Pareto front. The multi-objective MDPSO (MO-MDPSO) algorithm is tested using a suite of diverse benchmark problems and a disc-brake design problem. To illustrate the advantages of the new MO-MDPSO algorithm, the results are compared with those given by the popular Elitist Non-dominated Sorting Genetic Algorithm-II (NSGA-II).",2014
1119,Power System Identification Through Simultaneous Model Selection and Bayesian Calibration,"Computer models are very important to planning, operation, and control of power system. Although elements such as generators and transmission lines have been relatively well understood, developing a comprehensive power system model is a daunting task because challenges associated with loads modeling (they change all the time and utilities have very little control on). Unfortunately, inaccurate load models have serious implications such as unsafe operating conditions, power outages, under-utilization of system capacity, or inappropriate capital investment. This paper presents the use of state-of-the art Bayesian calibration framework for simultaneous load model selection and calibration. The approach aims at identifying configuration and reducing parameters uncertainty of the Western Electricity Coordinating Council’s (WECC) composite load model in the presence of measured field data. The success of the approach is illustrated with synthetic field data and a simplified model.",2014
1120,Component Size Optimization of a Wind-Integrated Microgrid System With Dispatch Strategy and Resource Uncertainty,"The global quest for energy sustainability has motivated the development of technology for efficiently transforming various natural resources into energy. Combining these alternative energy sources with existing power systems requires systematic assessments and planning. The present study investigates the conversion of an existing power system into one with a wind-integrated microgrid. The standard approach applies wind resource assessment to determine suitable wind farm locations with high potential energy and then develops specific dispatch strategies to meet the power demand for the wind-integrated system with low cost, high reliability, and low impact on the environment. However, the uncertainty in wind resource results in fluctuating power generation. The installation of additional energy storage devices is thus needed in the dispatch strategy to ensure a stable power supply. The present work proposes a design procedure for obtaining the optimal sizing of wind turbines and storage devices considering wind resource assessment and dispatch strategy under uncertainty. Two wind models are developed from real-world wind data and apply in the proposed optimization framework. Based on comparisons of system reliability between the optimal results and real operating states, an appropriate wind model can be chosen to represent the wind characteristics of a particular region. Results show that the trend model of wind data is insufficient for wind-integrated microgrid planning because it does not consider the large variation of wind data. The wind model should include the uncertainties of wind resource in the design of a wind-integrated microgrid system to ensure high reliability of optimal results.",2014
1121,Optimum Solar HDH Desalination for Semi-Isolated Communities Using HGP and GA’s,"Modeling and unit-cost optimization of a water-heated humidification-dehumidification (HDH) desalination system were presented in previous work of the authors. The system controlled the saline water flow rate to prevent salts from precipitating at higher water temperatures. It was then realized that this scheme had a negative impact on condensation performance when the controlled flow rate was not sufficiently high. This work builds on the previous system by disconnecting the condenser from the saline water cycle and by introducing a solar air heater to further augment the humidification performance. In addition, improved models for the condenser and the humidifier were used to obtain more accurate productivity estimations. The Heuristic Gradient Projection (HGP) optimization procedure was also refactored to result in reduced number of function evaluations to reach the global optimum compared to Genetic Algorithms (GA’s). A case study which assumes a desalination plant on the Red Sea near the city of Hurghada is presented. The unit-cost of produced fresh water for the new optimum system is $0.5/m",2014
1122,Multi-Level Design Optimization of Reverse Osmosis Water Desalination Powered via Photovoltaic Panels With Battery Storage,"Reverse osmosis (RO) is one of the main commercial technologies for desalination of water with salinity content too high for human consumption in order to produce fresh water. RO may hold promise for remote areas with scarce fresh water resources, however, RO energy requirements being in the form of electric power have few options in such areas. Fortunately, scarce rainfall is often associated with abundant sunshine, which makes solar photovoltaic (PV) power an attractive option. Equipping a photovoltaic powered reverse osmosis (PV-RO) desalination plant with battery storage has an advantage of steadier and longer hours of operation, thereby making better use of the investments in RO system components, but the additional cost from including batteries may end up increasing the overall cost of fresh water. It is therefore of paramount importance to consider the overall cost-effectiveness of the PV-RO system when designing the desalination plant. Recent work by the authors has generalized the steady operation model of RO systems to hourly adjusted power-dispatch via a proportional-derivative (PD) controller that depends on the state of charge (SOC) of the battery, yet the operating conditions; namely pressure and flow for a given power dispatch were only empirically selected. This paper considers a multi-level optimization model for PV-RO systems with battery storage by considering a “sub-loop” optimization of the feed pressure and flow given power dispatch for a fixed RO system configuration, as well as a “top-level” optimization where the system configuration itself is adjusted by the design variables. Effect of the sub-loop optimization is assessed by comparing the obtained cost of fresh water with the previous empirically adjusted system for locations and weather conditions near the city of Hurgada on the Red Sea.",2014
1123,Ramp Forecasting Performance From Improved Short-Term Wind Power Forecasting,"The variable and uncertain nature of wind generation presents a new concern to power system operators. One of the biggest concerns associated with integrating a large amount of wind power into the grid is the ability to handle large ramps in wind power output. Large ramps can significantly influence system economics and reliability, on which power system operators place primary emphasis. The Wind Forecasting Improvement Project (WFIP) was performed to improve wind power forecasts and determine the value of these improvements to grid operators. This paper evaluates the performance of improved short-term wind power ramp forecasting. The study is performed for the Electric Reliability Council of Texas (ERCOT) by comparing the experimental WFIP forecast to the current short-term wind power forecast (STWPF). Four types of significant wind power ramps are employed in the study; these are based on the power change magnitude, direction, and duration. The swinging door algorithm is adopted to extract ramp events from actual and forecasted wind power time series. The results show that the experimental short-term wind power forecasts improve the accuracy of the wind power ramp forecasting, especially during the summer.",2014
1124,Energy Optimization in Net-Zero Energy Building Clusters,"Traditionally viewed as mere energy consumers, buildings have in recent years adapted, capitalizing on smart grid technologies and distributed energy resources to not only efficiently use energy, but to also output energy. This has led to the development of net-zero energy buildings, a concept which encapsulates the synergy of energy efficient buildings, smart grids, and renewable energy utilization to reach a balanced energy budget over an annual cycle. This work looks to further expand on this idea, moving beyond just individual buildings and considering net-zero at a community scale. We hypothesize that applying net-zero concepts to building communities, also known as building clusters, instead of individual buildings will result in cost effective building systems which in turn will be resilient to power disruption. To this end, this paper develops an intelligent energy optimization algorithm for demand side energy management, taking into account a multitude of factors affecting cost including comfort, energy price, Heating, Ventilation, and Air Conditioning (HVAC) system, energy storage, weather, and on-site renewable resources. A bi-level operation decision framework is presented to study the energy tradeoffs within the building cluster, with individual building energy optimization on one level and an overall net-zero energy optimization handled on the next level. The experimental results demonstrate that the proposed approach is capable of significantly shifting demand, and when viable, reducing the total energy demand within net-zero building clusters. Furthermore, the optimization framework is capable of deriving Pareto solutions for the cluster which provide valuable insight for determining suitable energy strategies.",2014
1125,Multi-Disciplinary Design Optimization for Large-Scale Reverse Osmosis Systems,"Large-scale desalination plants are complex systems with many inter-disciplinary interactions and different levels of sub-system hierarchy. Advanced complex systems design tools have been shown to have a positive impact on design in aerospace and automotive, but have generally not been used in the design of water systems. This work presents a multi-disciplinary design optimization approach to desalination system design to minimize the total water production cost of a 30,000m",2014
1126,Modular Design of Community-Scale Photovoltaic Reverse Osmosis Systems Under Uncertainty,"Photovoltaic reverse osmosis (PVRO) systems can provide a viable clean water source for many remote communities. To be cost-effective, PVRO systems need to be custom-tailored for the local water demand, solar insolation, and water characteristics. Designing a custom system composed of modular components is not simple due to the large number of design choices and the variations in the sunlight and demand. This paper presents a modular design architecture, which when implemented on a low-cost PC, would enable users to configure systems from inventories of modular components. The method uses a hierarchy of filters or design rules, which can be provided in the form of an expert system, to limit the design space. The architecture then configures a system from the reduced design space using a genetic algorithm to minimize the system lifetime cost subject to system constraints. The genetic algorithm uses a detailed cost model and physics-based PVRO system model which determines the ability of the system to meet demand. Determining the ability to meet demand is challenging due to variations in water demand and solar radiation. Here, the community’s historical water demand, solar radiation history, and PVRO system physics are used in a Markov model to quantify the ability of a system to meet demand or the loss-of-water probability (LOWP). Case studies demonstrate the approach and the cost-reliability trade-off for community-scale PVRO systems. In addition, long-duration simulations are used to demonstrate the Markov model appropriately captures the uncertainty.",2014
1127,Energetic and Socioeconomic Justification for Solar-Powered Desalination Technology for Rural Indian Villages,"This paper provides justification for solar-powered electrodialysis desalination systems for rural Indian villages. It is estimated that 11% of India’s 800 million people living in rural areas do not have access to an improved water source. If the source’s quality in regards to biological, chemical, or physical contaminants is also considered, this percentage is even higher. User interviews conducted by the authors and in literature reveal that users judge the quality of their water source based on its aesthetic quality (taste, odor, and temperature). Seventy-three percent of Indian villages rely on groundwater as their primary drinking supply. However, saline groundwater underlies approximately 60% of the land area in India. Desalination is necessary in order to improve the aesthetics of this water (by reducing salinity below the taste threshold) and remove contaminants that cause health risks.",2014
1128,Selection of Sustainable Wind Turbine Tower Geometry and Material Using Multi-Level Decision Making,"Wind turbine tower design looks primarily at the structural integrity and durability of the tower. Optimization techniques are sometimes employed to maximize the loading capability while reducing material use and cost. Still, the tower is a dynamic part of a complex wind energy conversion system. During system operation, the tower is excited and sways back and forth. This undesirable movement increases cyclical loading on the tower and drivetrain components. To minimize this motion the tower frequency must be offset from the natural frequency of other components. Hence, it is necessary to look at the relationships that exist between the tower and other wind turbine components, such as the rotor, nacelle, and foundation. In addition, tradeoffs between cost, structural performance, and environmental impact can be examined to guide the designer toward a truly sustainable alternative to fossil fuels. Ultimately, an optimal design technique can be implemented and used to automate tower design. This work will introduce the analytical model and decision-making architecture that can be used to incorporate greater considerations in future studies. In this paper, nine wind turbine tower designs with different materials and geometries are analyzed using Finite Element Analysis (FEA). The optimal tower design is selected using a multi-level variation of the Hypothetical Equivalents and Inequivalents Method (HEIM). Using this analysis, a steel tower with variable thickness has been chosen. The findings reaffirm that steel is a favorable choice for turbine tower construction as it performs well on environmental, performance, and cost objectives. The method proposed in this work can be expanded to examine additional design goals and present a higher fidelity model of the wind turbine tower system in future work.",2014
1129,Policy and Demand as Drivers for Product Quality and Sustainability: A Market Systems Approach,"The market is a complex system with many different stakeholders and interactions. A number of decisions within this system affect the design of new products, not only from design teams but also from consumers, producers, and policy-makers. Market systems studies have shown how profit-optimal producer decisions regarding product design and pricing can influence a number of different factors including the quality, environmental impact, production costs, and ultimately consumer demand for the product. This study models the ways that policies and consumer demand combine in a market systems framework to influence optimal product design and, in particular, product quality and environmental sustainability. Implementing this model for the design of a mobile phone case shows how different environmental impact assessment methods, levels of taxation, and factors introduced to the consumer decision-making process will influence producer profits and overall environmental impacts. This demonstrates how different types of policies might be evaluated for their effectiveness in achieving economic success for the producer and reduced environmental impacts for society, and a “win-win” scenario was uncovered in the case of the mobile phone.",2014
1130,Should Optimal Designers Worry About Consideration?,"Consideration set formation using non-compensatory screening rules is a vital component of real purchasing decisions with decades of experimental validation. Marketers have recently developed statistical methods that can estimate quantitative choice models that include consideration set formation via non-compensatory screening rules. But is capturing consideration within models of choice important for design? This paper reports on a simulation study of a vehicle portfolio design when households screen over vehicle body style built to explore the importance of capturing consideration rules for optimal designers. We generate synthetic market share data, fit a variety of discrete choice models to this data, and then optimize design decisions using the estimated models. Model predictive power, design “error”, and profitability relative to ideal profits are compared as the amount of market data available increases. We find that even when estimated compensatory models provide relatively good predictive accuracy, they can lead to sub-optimal design decisions when the population uses consideration behavior; convergence of compensatory models to non-compensatory behavior is likely to require unrealistic amounts of data; and modeling heterogeneity in non-compensatory screening is more valuable than heterogeneity in compensatory trade-offs. This supports the claim that designers should carefully identify consideration behaviors before optimizing product portfolios. We also find that higher model predictive power does not necessarily imply better design decisions; that is, different model forms can provide “descriptive” rather than “predictive” information that is useful for design.",2014
1131,"A Socio-Academic Approach in Providing Renewable Energy Sources: Experiences of Mechanical Engineering Department, Mapua Institute of Technology","Mapua Institute of Technology has been constantly engaged in providing free, renewable energy to rural and under privileged communities. Guided by the mission and vision of the School of Mechanical and Manufacturing Engineering Department and of the Office of Social Orientation and Community Involvement, the school had implemented several renewable energy activities. This paper showcases 8 different projects — 6 hydropower plant projects, 1 human kinetic harvesting demonstration facility, and 1 wind turbine project. In this paper, implemented projects are presented briefly with emphasis on the different locations, local cultural settings and different experiences encountered. It will also share how students have changed from being participants to autonomous implementers of renewable energy projects for communities.",2014
1132,Design for Upstream and Downstream Market Systems With Interoperability Considerations,"Product design decision makers are frequently challenged by the difficulty of ensuring compatibility of parts sourced from various suppliers, including the services that the product is designed to integrate. The crux of the difficulty is in analyzing the ability of sourced parts (and services) to interoperate under uncertainty, and the impact of such compatibility on the overall marketing objectives. Therefore, the decisions in a design for market system problem can be closely related to the considerations along both the upstream (e.g., suppliers) and downstream (e.g., service providers and customers) market systems. This paper fills a gap in the existing research by exploring a design decision method that integrates upstream and downstream market systems with interoperability considerations. The proposed method is based on a mathematical model and metric for interoperability presented for the first time in the literature and particularly in the context of engineering design. The design decision framework is demonstrated using three examples: a mechanical design tolerance problem, a power tool design problem, and a tablet computer design problem. The mechanical design problem demonstrates how the interoperability metric can be used as a new way of analyzing tolerances in mechanical systems. The power tool design example involves an integration of upstream and downstream market systems for design selection. The tablet computer design selection problem considers not only the upstream suppliers but also customers and digital service providers along the downstream market system.",2014
1133,A Study of Automotive Greenhouse Gas Emissions and Reduction Opportunities Through Adoption of Electric Drive Vehicles,"This paper explores opportunities for reductions in lifecycle greenhouse gas (GHG) emissions through adoption of electric drive vehicles (EDV), including hybrid, plug-in hybrid and battery electric vehicles. EDVs have generally lower GHG emission rates during operation than similar-class conventional vehicles (CV). However, a key observation is that GHG reductions per mile are much larger during city driving conditions than on the highway. An examination of the estimated GHG emissions is conducted for city and highway driving conditions for several CV and EDV models based on testing results from the US Environmental Protection Agency (EPA), then compared with key findings from the 2009 National Household Travel Survey (NHTS 2009). Through an empirical analysis of actual driving patterns in the U.S., this study highlights potential missed opportunities to reduce transportation GHG emissions through the allocation of incentives and/or regulations. Key findings include the significant potential to reduce GHG emissions of taxis and delivery vehicles, as well as driving pattern-based incentives for individual vehicle owners.",2014
1134,Enhanced Adaptive Choice-Based Conjoint Analysis Incorporating Engineering Knowledge,Conjoint analysis from marketing has been successfully integrated with engineering analysis in design for market systems. The long questionnaires needed for conjoint analysis in relatively complex design decisions can become cumbersome to the human respondents. This paper presents an adaptive questionnaire generation strategy that uses active learning and allows incorporation of engineering knowledge in order to identify efficiently designs with high probability to be optimal. The strategy is based on viewing optimal design as a group identification problem. A running example demonstrates that a good estimation of consumer preference is not always necessary for finding the optimal design and that conjoint analysis could be configured more effectively for the specific purpose of design optimization. Extending the proposed method beyond a homogeneous preference model and noiseless user responses is also discussed.,2014
1135,Proof-of-Concept Evaluation of a Low-Cost and Low-Weight Tractor for Small-Scale Farms,"About 80% of farms in India are less than five acres in size and are cultivated by farmers who use bullocks for farming operations. Even the smallest tractors available in the Indian market are too expensive and large, and not designed to meet the unique requirements of these farmers. To address these needs, we have developed a proof-of-concept lightweight (350 kg) tractor in collaboration with Mahindra and Mahindra Limited, an Indian tractor manufacturer. Given the challenges of accurately predicting traction in Indian soils by applying existing terramechanics models, an alternative design approach based on Mohr-Coulomb soil-failure criterion is presented. Analysis of weight, power and drawbar of existing tractors on the market, a single wheel traction test, and a drawbar test of a proof-of-concept small tractor prototype suggest that ∼200kg is the maximum drawbar force that could be achieved by a 350kg tractor of conventional design. In order to attain higher drawbar performance of 70% of the tractor weight needed for specific agricultural operations, additional design changes are required. An approach for increasing traction by adding tires is investigated and discussed. Additional research on weight distribution, dynamic drawbar testing and tread design is suggested as future work.",2014
1136,Integrated Decision Making in Electric Vehicle and Charging Station Location Network Design,"A major barrier in consumer adoption of electric vehicles (EVs) is ‘range anxiety,’ the concern that the vehicle will run out of power at an inopportune time. Range anxiety is caused by the current relatively low electric-only operational range and sparse public charging station infrastructure. Range anxiety may be significantly mitigated if EV manufacturers and charging station operators work in partnership using a cooperative business model to balance EV performance and charging station coverage. This model is in contrast to a sequential decision making model where manufacturers bring new EVs to the market first and charging station operators decide on charging station deployment given EV specifications and market demand. This paper proposes an integrated decision making framework to assess profitability of a cooperative business models based on a multi-disciplinary optimization model that combines marketing, engineering, and operations. This model is demonstrated in a case study involving battery electric vehicle design and direct-current fast charging station location network in the State of Michigan. The expected benefits can motive both government and private enterprise actions.",2014
1137,Considering Design Prohibitions in Product Line Optimization,"To be competitive in today’s market, firms need to offer a variety of products that appeal to a diverse set of customer needs. Product line optimization provides a simple method to design for this challenge. Using a heterogeneous customer preference model allows the optimization to better explore the diversity in the market. The optimization should also consider aesthetic, engineering, manufacturing, and marketing constraints to ensure the feasibility of the final solution. However, as more constraints are added the difficulty of the optimization increases. There is an opportunity to reduce the difficulty of the optimization by allowing the heterogeneous customer preference model to handle a subset of these constraints termed design prohibitions. Design prohibitions include component incompatibility and dependency. This paper investigates whether design prohibitions should be handled solely in the heterogeneous customer preference model, solely in the optimization formulation, or in both. The effects of including design prohibitions in the creation of a hierarchical Bayes mixed logit model and a genetic algorithm based product line optimization are explored using a bicycle case study.",2014
1138,A Random Process Metamodel for Time-Dependent Reliability of Dynamic Systems,"A new metamodeling approach is proposed to characterize the output (response) random process of a dynamic system with random variables, excited by input random processes. The metamodel is then used to efficiently estimate the time-dependent reliability. The input random processes are decomposed using principal components or wavelets and a few simulations are used to estimate the distributions of the decomposition coefficients. A similar decomposition is performed on the output random process. A Kriging model is then built between the input and output decomposition coefficients and is used subsequently to quantify the output random process corresponding to a realization of the input random variables and random processes. In our approach, the system input is not deterministic but random. We establish therefore, a surrogate model between the input and output random processes. The quantified output random process is finally used to estimate the time-dependent reliability or probability of failure using the total probability theorem. The proposed method is illustrated with a corroding beam example.",2014
1139,Design of Smart Sensing Functions Using Piezoelectric Materials for Failure Diagnostics,"This paper presents a robust design framework to develop piezoelectric materials based structural sensing systems for failure diagnostics and prognostics. At first, a detectability measure is proposed to evaluate the performance of any given sensing system given various uncertainties. Thus, the censoring system design problem can be formulated to maximize the detectability of the censoring system through optimally allocating piezoelectric materials into a target structural system. Secondly, the formulated problem can be conveniently solved using reliability-based robust design framework to ensure design robustness while considering the uncertainties. Two engineering case studies are employed to demonstrate the effectiveness of the design framework in developing multifunctional material sensing systems.",2014
1140,Evolving Design Model Synchronization for System Health Management Using Laplace Approximation,"Lifecycle health management plays an increasingly important role in realizing resilience of aging complex engineered systems since it detects, diagnoses, and predicts system-wide effects of adverse events, therefore enables a proactive approach to deal with system failures. To address an increasing demand to develop high-reliability low-cost systems, this paper presents a new platform for operational stage system health management, referred to as Evolving Design Model Synchronization (EDMS), which enables health management of aging engineered systems by efficiently synchronizing system design models with degrading health conditions of actual physical system in operation. A Laplace approximation approach is employed for the design model updating, which can incorporate heterogeneous operating stage information from multiple sources to update the system design model based on the information theory, thereby increases the updating accuracy compared with traditionally used Bayesian updating methodology. The design models synchronized over time using sensory data acquired from the system in operation can thus reflect system health degradation with evolvingly updated design model parameters, which enables the application of failure prognosis for system health management. One case study is used to demonstrate the efficacy of the proposed approach for system health management.",2014
1141,Resilience Modeling and Quantification for Design of Complex Engineered Systems Using Bayesian Networks,"The concept of engineering resilience has received prevalent attention from academia as well as industry because it contributes a new means of thinking about how to withstand against disruptions and recover properly from them. Although the concept of resilience was scholarly explored in diverse disciplines, there are only few which focus on how to quantitatively measure the engineering resilience. This paper is dedicated to explore the gap between quantitative and qualitative assessment of engineering resilience in the domain of design of complex engineered systems. A conceptual framework is first proposed for the modeling of engineering resilience, and then Bayesian network is employed as a quantitative tool for the assessment and analysis of engineering resilience for complex systems. A case study related to electric motor supply chain is employed to demonstrate the proposed approach. The proposed resilience quantification and analysis approach using Bayesian networks would empower system designers to have a better grasp of the weakness and strength of their own systems against system disruptions induced by adverse failure events.",2014
1142,A Self-Cognizant Dynamic System Approach for Health Management: Lithium-Ion Battery Case Study,"Safe and reliable operation of lithium-ion batteries as major energy storage devices is of vital importance, as unexpected battery failures could result in enormous economic and societal losses. Accurate estimation of the state-of-charge (SoC) and state-of-health (SoH) for an operating battery system, as a critical task for battery health management, greatly depends on the validity and generalizability of battery models. Due to the variability and uncertainties involved in battery design, manufacturing, and operation, developing a generally applicable battery physical model is a big challenge. To eliminate the dependency of SoC and SoH estimation on battery physical models, this paper presents a generic self-cognizant dynamic system approach for lithium-ion battery health management, which integrates an artificial neural network (ANN) with a dual extended Kalman filter (DEKF) algorithm. The ANN is trained offline to model the battery terminal voltages to be used by the DEKF. With the trained ANN, the DEKF algorithm is then employed online for SoC and SoH estimation, where voltage outputs from the trained ANN model are used in DEKF state-space equations to replace the battery physical model. Experimental results are used to demonstrate the effectiveness of the developed self-cognizant dynamic system approach for battery health management.",2014
1143,Estimation of Reliable Replacement Time of Cutting Tool in Tooling Machines Using the Energy-Based Reliability Model (ERM),"Real-time monitoring systems have been developed for tooling machine for the purpose of investigating the time-dependent cutting conditions, to detect instantaneous events, and to estimate life of cutting tools and the machine itself. An Energy-based Reliability Model (ERM) has been developed for real-time monitoring of cutting conditions. A standardized inspection process was defined and the two most sensible signals, vibrational signals and temperature increments, are collected to monitor the accumulation of dissipated energy during the tooling processes. The ERM then computes the normalized accumulative dissipated energy in replace of evaluating surface quality of workpiece at the end of each tooling process. This paper focuses on the implementation of ERM in the turning process on a lathe. The experimental results showed the dissipated energy linearly grows with respect to the amount of volume removal from the workpiece. The ERM built from the experimental results under the same condition were then utilized to estimate the turning performance under different experimental conditions. As a result, similar trends of dissipated energy versus volume removal were found. Therefore, ERM can be utilized to estimate a reliable replacement time of cutting tool in tooling machines.",2014
1144,Statistical Health Diagnostics for Water-Cooled Power Generator Stator Winding Against Water Absorption,"One of most important components in power generator is a stator winding since an unexpected failure of the water absorbed-winding leads to plant shut-down and substantial loss. Typically the stator winding is maintained with a time- or usage-based strategy, which could result in substantial waste of remaining life, high maintenance cost and low plant availability. Recently, the field of prognostics and health management offers general diagnostic and prognostic techniques to precisely assess the health condition and robustly predict the remaining useful life of an engineered system, with an aim to address the aforementioned deficiencies. This research aims at developing health reasoning system of power generator stator winding with physical and statistical analysis against water absorption. And it is based upon the capacitance measurements on winding insulations. In particular, a new health measure, Directional Mahalanobis Distance (DMD), is proposed to quantify the health condition. In addition, the empirical health grade system based upon the proposed technique, DMD, is carried out with the maintenance history. The smart health reasoning system is validated using eight years’ field data from eight generators, each of which contains forty two windings.",2014
1145,Engineering Recoverability: A New Indicator of Design for Engineering Resilience,Design of engineering resilient systems is an emerging research field. The contribution of this paper is to i) define engineering resilience on the basis of various resilience concepts in different fields; ii) propose the engineering recoverability as a new component in the framework of designing engineering resilient systems; and iii) introduce a general mathematical formulation to quantify the engineering resilience. One case study of a CNC machining system is used to demonstrate the value of designing engineering resilient systems.,2014
1146,Toward a Market-Based Lean Startup Product Design Method for the Developing World,"The question of how to effectively design products for consumers in the developing world has been widely debated. Several methodologies have been developed to address this issue focusing on human centered and community centered methods, but few methods are rooted in market-centered approaches. Recent advances in market-centered design from lean startup methodologies hold promise for the development of new methods that allow effective product design for consumers in the developing world. This paper contributes a method from which consumer level products can be designed to effectively supply the under-served markets of the developing world with innovative and sustainable solutions. Utilizing an iterative method based on three fundamental hypotheses, the Lean Design for Developing World Method (LDW) seeks to provide products that are economically viable, have strong market growth potential, and have a net positive impact on the customers and their communities.",2014
1147,Components of a Framework for the Design of Energy Services for Villages in Developing Countries,"The development of energy services for the 40% of the world’s population currently living in energy poverty is a challenging design problem. There are competing and often conflicting objectives between stakeholders from global to user viewpoints, and the confounding effects of real-world performance, rebound, and stacking of technologies makes the determination of optimal strategies for off-grid village energy complicated. Yet there are holistic and lasting solutions that can adequately address the technical, social, economic, and environmental constraints and satisfy the goals of all stakeholders. These solutions can be better identified by systematically considering five major qualitative and quantitative outcomes including 1) energy access and efficiency, 2) climate benefits, 3) health impacts, 4) upfront and recurring economic and opportunity costs, and 5) quality of life for the user in terms of several metrics. Beginning with a comprehensive survey of energy uses in a village and current and potential technological options to meet those needs, this article proposes a methodology to identify and quantify these five outcomes for various intervention scenarios. These evaluations can provide a better understanding of the constraints, trade-offs, sensitivity to various factors, and conditions under which certain designs are appropriate for the village energy system. Ultimately a balance of all five objectives is most likely to result in equitable, user-driven, and sustainable solutions.",2014
1148,A Method for Determining Customer Needs in the Developing World,"There are currently 1.4 billion people in the world living on less than $1.25 a day. Many engineers have designed products intended to alleviate the poverty faced by these individuals but most of these products have failed to have the desired impact. This is largely because we as engineers do not clearly understand the needs of people in poverty, which is understandable as it is particularly hard to determine needs in this context. This lack of understanding is usually because the engineer and the resource-poor individuals are from different cultures, because the engineer has no personal experience with life in poverty, and because the engineer has limited access to suitable market surrogates for testing and validation. This paper presents a method for determining the needs of resource-poor individuals in the developing world. The method presented here is organized into four steps to be completed within three different stages of need finding. Engineers and designers can follow these steps to more accurately determine the needs of resource-poor individuals as they design a product. The paper also includes examples of this method being used to determine customer needs for products in India and Peru.",2014
1149,Single Cell Battery Charger for Portable Electronic Devices in Developing Countries,"Many household electronic devices — flashlights, stereos, radios — require AA, AAA, C, and D size batteries. These batteries are often disposable in remote areas of the world that lack access to grid electricity. In parts of the globe, disposable batteries can account for over 50% of household energy expenditures and amount to 25 or more batteries disposed of per person per year. This amounts to more than 25,000 batteries annually for a village of 1000 people. Solutions to this problem can address economic and environmental concerns. Replacing disposable batteries with rechargeable batteries maintained by a local entrepreneur is one business-driven method to reduce environmental waste and household energy expenditures. This study evaluates technical options for providing rechargeable batteries to a decentralized population, and introduces a prototype portable charging kit that addresses the techno-economic requirements of charging batteries, delivering batteries at a reasonable cost to consumers, providing a profit margin for local entrepreneurs, and allowing for portability during travel between villages or refugee camps. The unit includes a solar PV power source, a lead-acid battery for intermediate energy storage, a battery charger equipped with single cell batteries, a charge controller to manage power flow, and a protective suitcase to house the equipment.",2014
1150,Improving Irrigation in Remote Areas: Multi-Objective Optimization of a Treadle Pump,"Water-lifting technologies in rural areas of the developing world have enormous potential to stimulate agricultural and economic growth. The treadle pump, a human-powered low-cost pump designed for irrigation in developing countries, can help farmers maximize financial return on small plots of land by ending their dependency on rain-fed irrigation systems. The treadle pump uses a suction piston to draw groundwater to the surface by way of a foot-powered treadle attached to each suction piston. Current treadle pump designs lift water from depths up to 7 meters at a flow-rate of 1–5 liters per second. This work seeks to optimize the design of the Dekhi style treadle pump, which has gained significant popularity due to its simplicity. A mathematical model of the working fluid and treadle pump structure has been developed in this study. Deterministic optimization methods are then employed to maximize the flow rate of the groundwater pumped, maximize the lift height, and minimize the volume of material used for manufacturing. Design variables for the optimization included the dimensions of the pump, well depth, and speed of various parts of the system. The solutions are subject to constraints on the geometry of the system, the bending stress in the treadles, and ergonomic factors. Findings indicate that significant technical improvements can be made on the standard Dekhi design, such as increasing the size of the pump cylinders and hose, while maintaining a standard total treadle length. These improvements could allow the Dekhi pump to be implemented in new regions and benefit additional rural farmers in the developing world.",2014
1151,Template Driven Conceptual Design of High Speed Trains,"Conceptual design of multidisciplinary systems begins with a description of requirements and proceeds with a solution at a high abstraction level. A systematic and rigorous approach is required to evaluate complex systems and can be achieved by mapping the interactions between disciplines. Research has shown that the use of geometry in the early stages act as enablers for high fidelity analyses as required information can be extracted from the model. In the paper, Knowledge Based Engineering is used with the aim of managing the added complexity as it supports design automation and reuse. This article describes a configuration tool, which allows for quick generation of train geometry using High Level CAD Templates. The tool was created as part of a research project, with the primary objective of the development of a robust framework for a Multidisciplinary Design Optimization process which can support design of high-speed trains.",2014
1152,Creating Faultable Network Models of Complex Engineered Systems,"This paper presents a new methodology for modeling complex engineered systems using complex networks for failure analysis. Many existing network-based modeling approaches for complex engineered systems “abstract away” the functional details to focus on the topological configuration of the system and thus do not provide adequate insight into system behavior. To model failures more adequately, we present two types of network representations of a complex engineered system: a uni-partite architectural network and a weighted bi-partite behavioral network. Whereas the architectural network describes physical inter-connectivity, the behavioral network represents the interaction between functions and variables in mathematical models of the system and its constituent components. The levels of abstraction for nodes in both network types affords the evaluation of failures involving morphology or behavior, respectively. The approach is shown with respect to a drivetrain model. Architectural and behavioral networks are compared with respect to the types of faults that can be described. We conclude with considerations that should be employed when modeling complex engineered systems as networks for the purpose of failure analysis.",2014
1153,Toward a Value-Driven Design Approach for Complex Engineered Systems Using Trade Space Exploration Tools,"Design decision-making involves trade-offs between many design variables and attributes, which can be difficult to model and capture in complex engineered systems. To choose the best design, the decision-maker is often required to analyze many different combinations of these variables and attributes and process the information internally. Trade Space Exploration (TSE) tools, including interactive and multi-dimensional data visualization, can be used to aid in this process and provide designers with a means to make better decisions, particularly during the design of complex engineered systems. In this paper, we investigate the use of TSE tools to support decision-makers using a Value-Driven Design (VDD) approach for complex engineered systems. A VDD approach necessitates a rethinking of trade space exploration. In this paper, we investigate the different uses of trade space exploration in a VDD context. We map a traditional TSE process into a value-based trade environment to provide greater decision support to a design team during complex systems design. The research leverages existing TSE paradigms and multi-dimensional data visualization tools to identify optimal designs using a value function for a system. The feasibility of using these TSE tools to help formulate value functions is also explored. A satellite design example is used to demonstrate the differences between a VDD approach to design complex engineered systems and a multi-objective approach to capture the Pareto frontier. Ongoing and future work is also discussed.",2014
1154,Optimal Configuration of Valves and Plumbing Lines in Complex Housing,"In engineering design, the volume and weight of a number of systems consisting of valves and plumbing lines often need to be minimized. In current practice, this is facilitated under empirical experience with trial and error, which is time-consuming and may not yield the optimal result. This problem is intrinsically difficult due to the challenge in the formulation of optimization problem that has to be computationally tractable. In this research, we choose a sequential approach towards the design optimization, i.e., first optimizing the placement of valves under prescribed constraints to minimize the volume occupied, and then identifying the shortest paths of plumbing lines to connect the valves. In the first part, the constraints are described by analytical expressions, and two approaches of valve placement optimization are reported, i.e., a two-phase method and a simulated annealing-based method. In the second part, a three-dimensional routing algorithm is explored to connect the valves. Our case study indicates that the design can indeed be automated and design optimization can be achieved under reasonable computational cost. The outcome of this research can benefit both existing manufacturing practice and future additive manufacturing.",2014
1155,Integrated Multiscale Robust Design Considering Microstructure Evolution and Material Properties in the Hot Rolling Process,"A transmission gear is generally produced by a sequence of several processes from steelmaking to final machining and surface treatment. The intermediate processes such as hot rolling induce microstructure evolution and phase transformation which play a significant role in determining the mechanical properties and fatigue strength of gears. Therefore, these intermediate processes should be carefully considered in determining the performance and properties of the end product.",2014
1156,Evolution of Meso-Structures for Non-Pneumatic Tire Development: A Case Study,"The evolution of meso-structures in the development of the shear band of Michelin’s non-pneumatic tire, the Tweel, is presented in this paper. Designers and researchers at Clemson University worked on a research projects with Michelin to support NIST efforts in fuel efficiency improvement and NASA efforts in manned exploration systems. The goal of each was to replace the elastomeric material of shear band with materials which can tolerate harsh temperatures and shear loads or to replace the materials with linear elastic low-hysteretic loss materials. The concepts initially proposed by ideation method were prototyped for physical testing. A case study examining the documentation reports for each project is conducted to provide a reflective understanding of how the evolution in the projects occurred. The goal of developing this retrospective is to try to identify guidelines and approaches that could be integrated into a designer driven systematic approach for custom design of meso-structures.",2014
1157,Hot Forging of Automobile Steel Gear Blanks: An Exploration of the Solution Space,"In this paper, we explore the design space associated with the realization of automobile steel gear blanks manufactured using hot forging. ABAQUS, a commercially available software package, is used to simulate the hot forging process. Surrogate models are developed and used to model the solution space. Then the compromise Decision Support Problem (cDSP) is used to explore the feasible space. Results are presented as ternary plots that allow a designer to visualize the trade-offs between competing goals such as reducing the size of the underfill, reducing the flash (excess material), waste of forged materials, energy required for deformation and crack susceptibility, on various possible solutions. The efficacy of the exploration of the design space and trade-offs between competing solutions is illustrated.",2014
1158,Inverse Design of Manufacturing Process Chains,"Finding the input specifications to obtain the specified performance of a component being designed is an essential activity of a designer. However, obtaining solutions for this inverse problem is a complex task; especially when there are multiple steps with many-to-one mappings at each step in the forward problem. This complexity is further augmented in the presence of uncertainty of the parameters and models used.",2014
1159,An Approach to Robust Process Design for Continuous Casting of Slab,"Continuous casting is a crucial step in the production of a variety of steel products. Its performance is measured in terms of conflicting objectives including productivity, yield, quality and production costs. These are conflicting in the sense that, if the productivity is increased, there is a reduction in other performance parameters. These performance parameters are greatly influenced by operating conditions such as casting speed, superheat, mold oscillation frequency, and secondary cooling conditions. An optimized solution for continuous casting process can be obtained. However uncertainty in operating parameters which affects the performance of caster is rarely considered. Moreover, the solution obtained is optimal with respect to a particular performance measure and does not provide a balance between all. In this paper an integrated design framework has been developed based on metamodels and the compromise Decision Support Problem (cDSP). The framework developed deals with uncertainty and yields robust solutions for performance measures. Further, the design space for continuous casting has been explored for different scenarios to determine satisficing solutions. The utility of the framework has been illustrated for providing decision support when an existing configuration for continuous casting is unable to meet the requirements. This approach can be instantiated for other unit operations involved in steel manufacturing and then may be integrated to simulate the entire production cycle of steel manufacturing. This in turn will enable development of materials with specific properties and reduce the time and cost incurred in the development of new materials and their manufacturing.",2014
1160,Exploration of the Design Space in Continuous Casting Tundish,"Due to the stringent requirements of industry, it has become extremely important to have a careful control over the required performance and properties of steels. Performance and properties of advanced high strength steel depend significantly on its cleanliness. Cleanliness is achieved by restricting the inclusion count to a permissible limit. Over the past few years, there has been increased use of tundish, a device that acts as a buffer between ladle and mold, for controlling inclusions. Apart from facilitating inclusion removal, tundish also maintains low dead volume and thermal and chemical homogeneity, which is required for smooth casting operation. Thus, performance of the tundish operation greatly influences the properties and quality of the cast slab. Tundish performance is generally assessed using parameters such as inclusion removal efficiency, dead volume within tundish and effectiveness in maintaining the desired amount of superheat. But, the aforesaid parameters are conflicting in nature. Managing the conflict and providing a satisficing solution based on the customer requirements become essential.",2014
1161,A Multi-Scale Materials Modeling Method With Seamless Zooming Capability Based on Surfacelets,"In multi-scale materials modeling, it is desirable that different levels of details can be specified in different regions of interest without the separation of scales so that the geometric and physical properties of materials can be designed and characterized. Existing materials modeling approaches focus on the representation of the distributions of material compositions captured from images. In this paper, a multi-scale materials modeling method is proposed to support interactive specification and visualization of material microstructures at multiple levels of details, where designer’s intent at multiple scales is captured. This method provides a feature-based modeling approach based on a recently developed surfacelet basis. It has the capability to support seamless zoom-in and zoom-out. The modeling, operation, and elucidation of materials are realized in both the surfacelet space and the image space.",2014
1162,"Bayesian Network Classifiers and Design Flexibility Metrics for Set-Based, Multiscale Design With Materials Design Applications","A set-based approach is presented for solving multi-scale or multi-level design problems. The approach incorporates Bayesian network classifiers (BNC) for mapping design spaces at each level and flexibility metrics for intelligently narrowing the design space as the design process progresses. The approach is applied to a hierarchical composite materials design problem, specifically, the design of composite materials with macroscopic mechanical stiffness and loss properties surpassing those of conventional composites. This macroscopic performance is achieved by embedding small volume fractions of negative stiffness (NS) inclusions in a host material. To design these materials, the set-based, multilevel design approach is coupled with a hierarchical modeling strategy that spans several scales, from the behavior of microscale NS inclusions to the effective properties of a composite material containing those inclusions and finally to the macroscopic performance of components. The approach is shown to increase the efficiency of multi-level design space exploration, and it is particularly appropriate for top-down, performance-driven design, as opposed to bottom-up, trial-and-error modeling. The design space mappings also build intuitive knowledge of the problem and promising regions of the design space, such that it is almost trivial to identify designs that yield preferred system-level performance.",2014
1163,A Machine Learning-Based Design Representation Method for Designing Heterogeneous Microstructures,"In designing microstructural materials systems, one of the key research questions is how to represent the microstructural design space quantitatively using a descriptor set that is sufficient yet small enough to be tractable. Existing approaches describe complex microstructures either using a small set of descriptors that lack sufficient level of details, or using generic high order microstructure functions of infinite dimensionality without explicit physical meanings. We propose a new machine learning-based method for identifying the key microstructure descriptors from vast candidates as potential microstructural design variables. With a large number of candidate microstructure descriptors collected from literature covering a wide range of microstructural material systems, a 4-step machine learning-based method is developed to eliminate redundant microstructure descriptors via image analyses, to identify key microstructure descriptors based on structure-property data, and to determine the microstructure design variables. The training criteria of the supervised learning process include both microstructure correlation functions and material properties. The proposed methodology effectively reduces the infinite dimension of the microstructure design space to a small set of descriptors without a significant information loss. The benefits are demonstrated by an example of polymer nanocomposites optimization. We compare designs using key microstructure descriptors versus using empirically-chosen microstructure descriptors to validate the proposed method.",2014
1164,"Constraint Satisfaction Approach to the Design of Multi-Component, Multi-Phase Alloys",The development of new materials must start with an understanding of their phase stability. Researchers have used the CALPHAD method to develop self-consistent databases encoding the thermodynamics of phases. In this ,2014
1165,Efficient Filtering in Topology Optimization via B-Splines,"This paper presents a B-spline based approach for topology optimization of three-dimensional (3D) problems where the density representation is based on B-splines. Compared with the usual density filter in topology optimization, the new B-spline based density representation approach is advantageous in both memory usage and CPU time. This is achieved through the use of tensor-product form of B-splines. As such, the storage of the filtered density variables is linear with respect to the effective filter size instead of the cubic order as in the usual density filter. Numerical examples of 3D topology optimization of minimal compliance and heat conduction problems are demonstrated. We further reveal that our B-spline based density representation resolves the bottleneck challenge in multiple density per element optimization scheme where the storage of filtering weights had been prohibitively expensive.",2014
1166,Domain Composition Method for Structural Optimization,"Conventionally, design domain of topology optimization is predefined and is not adjusted in the design optimization process since designers are required to specify the design domain in advance. However, it is difficult for a fixed design domain to satisfy design requirements such as domain sizing adjustment or boundaries change. In this paper, Domain Composition Method (DCM) for structural optimization is presented and it deals with the design domain adjustment and the material distribution optimization in one framework. Instead of treating design domain as a whole, DCM divides domain into several subdomains. Additional scaling factors and subdomain transformations are applied to describe changes between different designs. It then composites subdomains and solve it as a whole in the updated domain. Based on the domain composition, static analysis with DCM and sensitivity analysis are derived. Consequently, the design domain and the topology of the structure are optimized simultaneously. Finally, the effectiveness of the proposed DCM for structural optimization is demonstrated through different numerical examples.",2014
1167,Projection-Based Topology Optimization Using Discrete Object Sets,"We look to expand the reach of continuum topology optimization to include the design of ‘structures’ that gain functionality or are specifically manufactured from discrete, non-overlapping objects. While significant advancements have been made in restricting the geometric properties of topology-optimized structures, including restricting the minimum and maximum length scale of features, continuum topology optimization is still largely limited to monolithic structures. A wide variety of structures and materials, however, gain their stiffness or functionality from discrete objects, such as fiber-reinforced composites. This work examines a recently developed method for optimizing the distribution of discrete objects (2d inclusions) across a design domain and extends the approach to variable shape and variable sized objects that must be selected from a designer-defined set. This essentially enables simultaneous optimization of object sizes, shapes, and/or locations within the projection framework, without need for additional constraints. As in traditional topology optimization, gradient-based optimizers are used with sensitivity information estimated via the adjoint method, solved using finite element analysis. The algorithm is demonstrated on benchmark problems in structural design for the case where the objects are stiff inclusions embedded in a compliant matrix material.",2014
1168,A Hybrid Approach to Polygon Offsetting Using Winding Numbers and Partial Computation of the Voronoi Diagram,"In this paper we present a new, efficient algorithm for computing the “raw offset” curves of 2D polygons with holes. Prior approaches focus on (a) complete computation of the Voronoi Diagram, or (b) pair-wise techniques for generating a raw offset followed by removal of “invalid loops” using a sweepline algorithm. Both have drawbacks in practice. Robust implementation of Voronoi Diagram algorithms has proven complex. Sweeplines take ",2014
1169,Topology Preserving Digitization of Physical Prototypes Using Deformable Subdivision Models,"Physical prototyping is an important stage of product design where designers have a chance to physically evaluate and alter digitally created surfaces. In these scenarios, designers generate a digital model, manufacture and alter the prototype as needed, and redigitize the prototype through scanning. Despite the variety of reverse engineering tools, redigitizing the prototypes into forms amenable to further digital editing remains a challenge. This is because current digitization methods cannot take advantage of the key elements of the original digital model such as the wireframe topology and surface flows. This paper presents a new reverse engineering method that augments conventional digitization with the knowledge of the original digital model’s curve topology to enhance iterative shape design activities. Our algorithm takes as input a curve network topology forming a subdivision control cage and a 3D scan of the physically modified prototype. To facilitate the digital capture of the physical modifications, our algorithm performs a series of registration, correspondence and deformation calculations to compute the new configuration of the initial control cage. The key advantage of the proposed technique is the preservation of the edge flows and initial topology while transferring surface modifications from prototypes. Our studies show that the proposed technique can be particularly useful for bridging the gap between physical and digital modeling in the early stages of product design.",2014
1170,Challenges in Designing and Manufacturing Fully Optimized Functional Gradient Material Objects,"Functionally Gradient Materials (FGM) smoothly transition from one material to another within a single object, allowing engineers to customize the physical response of different regions of the object by modifying the material composition at each region. New FGM research makes design, manufacturing, and use of FGM objects a promising alternative to homogeneous objects or composites with one direction of gradation. Heterogeneous anisotropic artifacts can be manufactured with specific 3D printing processes and potentially bring significant increases in functionalities. However, many challenges exist while designing and manufacturing these objects. This paper explores these challenges and suggests needed research. In particular the ability to model FGM objects, setup and run optimization algorithms, create manufacturing process plans, and control the manufacturing process all need more research and better software tools. In addition, researchers must rigorously test optimally designed FGM objects in order to validate the FGM object properties and the FGM design process before adoption of FGM objects by industry is likely to occur.",2014
1171,Optimization of Tooth Root Profile of Spur Gears for Maximum Load-Carrying Capacity,"Increasing the strength of the gear tooth is a recurrent demand from industry. The authors report a novel approach to the design of tooth-root profile of spur gears using cubic splines, with the aim of investigating the effect of tooth-root geometry on stress concentration in order to increase the gear tooth strength by optimizing the root profile. An iterative co-simulation procedure, consisting of tooth-root profile shape synthesis via nonlinear programming and finite element analysis software tools is conducted, for the purpose of forming the tooth-root geometry design with the minimum stress concentration. The proposed design was verified to be capable of reducing the stress concentration by 21% over its conventional circular-filleted counterpart. Hence, the results showcase an innovative and sound methodology for the design of the tooth-root profile to increase gear load-carrying capacity.",2014
1172,Automatic Detection and Extraction of Tolerance Stacks in Mechanical Assemblies,"Tolerances are specified by a designer to allow reasonable freedom by a manufacturer for imperfections and inherent variability without compromising performance. It takes knowledge and experience to create a good tolerance scheme. It is a tedious process driven by the type of parts, their features and controls needed for each one of them. In this paper, we investigate the extent to which GD&T schema development can be automated. Automated tolerance schema generation, requires identifying critical tolerance loops. The tolerance loop is a loop of dimensions between faces of features governing assembly conditions. For this purpose, the first major step is to identify mating features called assembly features. Also, in order to create the tolerance chains we need Local Constraints (assembly feature relationships), Global constraints (part feature relationships) and directions of control. In addition, we have to identify feature patterns since they have associated tolerances according to Dimensioning and Tolerancing Standard ASME Y14.5M. Directions in which these loops lie are also needed; we call them Direction of Control (DoC). Then we can create the GD&T schema, allocate tolerance values, and prepare it for tolerance evaluation. In this paper, we present an approach to automatically identify the dimensional loops based on assembly requirements. Assignment of tolerance values will be covered in future works as it is based on design function.",2014
1173,A Formal Model of Human Interactions for Service Ecosystem Design,"In this digital era, the natures of services are becoming increasingly complex and diverse due to the convergences between the existing human-centered services and other supportive device services, or interactions between heterogeneous services. Expecting this trend is to be accelerated more, new scientific and engineered approaches to the service design are needed more than ever. In this context, a service designed conceptually and abstractly has significant limitations that keep the customers’ satisfaction from advancing above a certain level. Hence, in an initial service design phase, a service delivery process can be expressed quantitatively through a systematic analysis of its natures and the goals. In this paper, a human-centered complex service system is newly defined as service ecosystem. This study proposes a method for designing service processes as a Discrete Event System (DES) in formal ways by utilizing the concepts of affordance, preference, and a Affordance-based Finite State Automata (FSA) modeling methodology. The proposed design method suggests a model framework that focuses on service actions that reflects related properties of customers, employees, and environment entities and their interactions quantitatively. The formally expressed relations between properties of service entities such as customer, employee, and service environment provide guidelines for service designers in a more scientific way than traditional ones. In addition, it is expected that it will enable to add computational to the human-centered service system design and control and develop effective reusable controllers for complex service deliveries as well.",2014
1174,Identifying Glenoid Geometries Through Radial Basis Functions for Implant Design,"Total Shoulder Arthroplasty is performed on patients to restore range of motion of the shoulder and decrease pain caused by osteoarthritis at the glenohumeral joint. The glenohumeral joint is a slightly unstable ball and socket joint, where muscles hold the humerus in contact with the glenoid, located on the scapula. Improper sizing or alignment of the implant can cause the surgery to fail to restore mobility to the shoulder or only restore mobility for a limited time. Additionally, placement of the glenoid implant on the scapula is complicated by the limited view available during surgery and the deformation of the glenoid caused by osteoarthritis. Implant designs must take into account the large amount of variability present in both intact and osteoarthritic joints. The purpose of this research is to provide a morphable glenoid representation for the scapula to assist with preoperative planning and implant design. CT scans of healthy and osteoarthritic glenoids were provided by Hershey Medical Center for this study. Principal component analysis and radial basis functions are used to represent a range of potential glenoid geometries, both with and without osteoarthritis. This parametric model can be used to guide the design and sizing of implants. This approach should be extensible to the modeling of other bony surfaces, which can improve both implant design and surgical procedure.",2014
1175,Advancing Design Through the Creation and Visualization of Virtual Population Representing U.S. Civilians,"Visualization of the U.S. civilian population provides perspective on the variability of body size and shape, enabling designers and engineers to better understand the needs of their target users. This paper presents a virtual population of digital human models, representative of the U.S. civilian population, and the methods used to create it. It is based on the observed variability in stature, BMI, and waist circumference in the data from the 2007–2010 CDC NHANES survey. The NHANES data were used in conjunction with regression models to develop the required model parameters. The models were then presented such that the variability and distribution of variability in anthropometry can be easily seen.",2014
1176,A Simultaneous Computing Framework for Metamodel-Based Design Optimization,"At the aim of reducing the computational time of engineering design optimization problems using metamodeling technologies, we developed a flexible distributed framework independent of any third-part parallel computing software to implement simultaneous sampling during metamodel-based design optimization procedures. In this paper, the idea and implementation of hardware configuration, software structure, the main functional modular and interfaces of this framework are represented in detail. The proposed framework is capable of integrating black-box functions and legacy software for analyzing and common MBDO methods for space exploring. In addition, a message-based communication infrastructure based on TCP/IP protocol is developed for distributed data exchange. The Client/Server architecture and computing budget allocation algorithm considering software dependency enable samples to be effectively allocated to the distributed computing nodes for simultaneous execution, which gives rise to decreasing the elapsed time and improving MBDO’s efficiency. Through testing on several numerical benchmark problems, the favorable results demonstrate that the proposal framework can evidently save the computational time, and is practical for engineering MBDO problems.",2014
1177,Problem Formulations for Simulation-Based Design Optimization Using Statistical Surrogates and Direct Search,"Typical challenges of simulation-based design optimization include unavailable gradients and unreliable approximations thereof, expensive function evaluations, numerical noise, multiple local optima and the failure of the analysis to return a value to the optimizer. The remedy for all these issues is to use surrogate models in lieu of the computational models or simulations and derivative-free optimization algorithms. In this work, we use the ",2014
1178,An Approach Towards Generating Surrogate Models by Using RBFN With a Priori Bias,"In this paper, an approach to generate surrogate models constructed by radial basis function networks (RBFN) with a priori bias is presented. RBFN as a weighted combination of radial basis functions only, might become singular and no interpolation is found. The standard approach to avoid this is to add a polynomial bias, where the bias is defined by imposing orthogonality conditions between the weights of the radial basis functions and the polynomial basis functions. Here, in the proposed a priori approach, the regression coefficients of the polynomial bias are simply calculated by using the normal equation without any need of the extra orthogonality prerequisite. In addition to the simplicity of this approach, the method has also proven to predict the actual functions more accurately compared to the RBFN with a posteriori bias. Several test functions, including Rosenbrock, Branin-Hoo, Goldstein-Price functions and two mathematical functions (one large scale), are used to evaluate the performance of the proposed method by conducting a comparison study and error analysis between the RBFN with a priori and a posteriori known biases. Furthermore, the aforementioned approaches are applied to an engineering design problem, that is modeling of the material properties of a three phase spherical graphite iron (SGI). The corresponding surrogate models are presented and compared.",2014
1179,A Global High Dimensional Metamodeling Approach With the Ability of Using Non-Uniform Sampling,"In engineering design, spending excessive amount of time on physical experiments or expensive simulations makes the design costly and lengthy. This issue exacerbates when the design problem has a large number of inputs, or of high dimension. High Dimensional Model Representation (HDMR) is one powerful method in approximating high dimensional, expensive, black-box (HEB) problems. One existing HDMR implementation, Random Sampling HDMR (RS-HDMR), can build a HDMR model from random sample points with a linear combination of basis functions. The most critical issue in RS-HDMR is that calculating the coefficients for the basis functions includes integrals that are approximated by Monte Carlo summations, which are error prone with limited samples and especially with non-uniform sampling. In this paper, a new approach based on Principal Component Analysis (PCA), called PCA-HDMR, is proposed for finding the coefficients that provide the best linear combination of the bases with minimum error and without using any integral. Benchmark problems are modeled using the method and the results are compared with RS-HDMR results. With both uniform and non-uniform sampling, PCA-HDMR built more accurate models than RS-HDMR for a given set of sample points.",2014
1180,Concurrent Surrogate Model Selection (COSMOS) Based on Predictive Estimation of Model Fidelity,"One of the primary drawbacks plaguing wider acceptance of surrogate models is their low fidelity in general. This issue can be in a large part attributed to the lack of automated model selection techniques, particularly ones that do not make limiting assumptions regarding the choice of model types and kernel types. A novel model selection technique was recently developed to perform optimal model search concurrently at three levels: (i) optimal model type (e.g., RBF), (ii) optimal kernel type (e.g., multiquadric), and (iii) optimal values of hyper-parameters (e.g., shape parameter) that are conventionally kept constant. The error measures to be minimized in this optimal model selection process are determined by the Predictive Estimation of Model Fidelity (PEMF) method, which has been shown to be significantly more accurate than typical cross-validation-based error metrics. In this paper, we make the following important advancements to the PEMF-based model selection framework, now called the Concurrent Surrogate Model Selection or COSMOS framework: (i) The optimization formulation is modified through binary coding to allow surrogates with differing numbers of candidate kernels and kernels with differing numbers of hyper-parameters (which was previously not allowed). (ii) A robustness criterion, based on the variance of errors, is added to the existing criteria for model selection. (iii) A larger candidate pool of 16 surrogate-kernel combinations is considered for selection — possibly making COSMOS one of the most comprehensive surrogate model selection framework (in theory and implementation) currently available. The effectiveness of the COSMOS framework is demonstrated by successfully applying it to four benchmark problems (with 2–30 variables) and an airfoil design problem. The optimal model selection results illustrate how diverse models provide important tradeoffs for different problems.",2014
1181,Global Optimization With Quantum Walk Enhanced Grover Search,"One of the significant breakthroughs in quantum computation is Grover’s algorithm for unsorted database search. Recently, the applications of Grover’s algorithm to solve global optimization problems have been demonstrated, where unknown optimum solutions are found by iteratively improving the threshold value for the selective phase shift operator in Grover rotation. In this paper, a hybrid approach that combines continuous-time quantum walks with Grover search is proposed. By taking advantage of quantum tunneling effect, local barriers are overcome and better threshold values can be found at the early stage of search process. The new algorithm based on the formalism is demonstrated with benchmark examples of global optimization. The results between the new algorithm and the Grover search method are also compared.",2014
1182,Investigation of Numerical Performance of Partitioning and Parallel Processing of Markov Chain (PPMC) for Complex Design Problems,"Divide-and-conquer strategies have been utilized to perform evaluation calculations of complex network systems, such as reliability analysis of a Markov chain. This paper focuses on partitioning of Markov chain for a multi-modular redundant system and the fast calculation using parallel processing. The complexity of Markov chain is first reduced by eliminating the connections with low transition probabilities associated with a threshold parameter. The transition probability matrix is then reordered and partitioned such that a worse-case reliability is evaluated through the calculations in only the diagonal sub-matrices of the transition probability matrix. Since the calculations of the sub-matrices are independent to each other, the numerical efficiency can be greatly improved using parallel computing. The numerical results showed the selection of threshold parameter is a key factor to numerical efficiency. In this paper, the sensitivity of the numerical performance of Partitioning and Parallel-processing of Markov Chain (PPMC) to the threshold parameter has been investigated and discussed.",2014
1183,A System Uncertainty Propagation Approach With Model Uncertainty Quantification in Multidisciplinary Design,"The performance of a multidisciplinary system is inevitably affected by various sources of uncertainties, usually categorized as aleatory (e.g. input variability) or epistemic (e.g. model uncertainty) uncertainty. In the framework of design under uncertainty, all sources of uncertainties should be aggregated to assess the uncertainty of system quantities of interest (QOIs). In a multidisciplinary design system, uncertainty propagation refers to the analysis that quantifies the overall uncertainty of system QOIs resulting from all sources of aleatory and epistemic uncertainty originating in the individual disciplines. However, due to the complexity of multidisciplinary simulation, especially the coupling relationships between individual disciplines, many uncertainty propagation approaches in the existing literature only consider aleatory uncertainty and ignore the impact of epistemic uncertainty. In this paper, we address the issue of efficient uncertainty quantification of system QOIs considering both aleatory and epistemic uncertainties. We propose a spatial-random-process (SRP) based multidisciplinary uncertainty analysis (MUA) method that, subsequent to SRP-based disciplinary model uncertainty quantification, fully utilizes the structure of SRP emulators and leads to compact analytical formulas for assessing statistical moments of uncertain QOIs. The proposed method is applied to a benchmark electronics packaging problem. To demonstrate the effectiveness of the method, the estimated low-order statistical moments of the QOIs are compared to the results from Monte Carlo simulations.",2014
1184,System of Systems Approach to Air Transportation Design Using Nested Problem Formulation and Direct Search Optimization,"Aircraft sizing, route network design, demand estimation and allocation of aircraft to routes are different facets of the air transportation optimization problem that can be viewed as individual “systems,” since they can be conducted independently. In fact, there is a large body of literature that investigates each of these as a stand-alone problem. In this regard, the air transportation design optimization problem can be viewed as an optimal system-of-systems (SoS) design problem. The resulting mixed variable programming problem cannot be solved all-in-one (AiO) because its size and complexity grow exponentially with increasing number of network nodes. In this work, we use a nested multidisciplinary formulation and the Mesh Adaptive Direct Search (MADS) optimization algorithm to solve the optimal SoS design problem. The expansion of a regional Canadian airline’s network to enable national operations is considered as an example.",2014
1185,Iteration Complexity of the Alternating Direction Method of Multipliers for Quasi-Separable Problems in Multi-Disciplinary Design Optimization,"The Alternating Direction Method of Multipliers (ADMM) is a distributed algorithm suitable for quasi-separable problems in Multi-disciplinary Design Optimization. Previous authors have studied the convergence and complexity of the ADMM algorithm by treating it as an instance of the proximal point algorithm. In this paper, those previous results are extended to an alternate form of the ADMM algorithm applied to the quasi-separable problem. Secondly, a dynamic penalty parameter updating heuristic for the ADMM algorithm is introduced and compared against a previously proposed updating heuristic. The proposed updating heuristic was tested on a distributed linear model fitting example and performed favorably against the other heuristic and the fixed penalty parameter scheme.",2014
1186,Dual Residual in Augmented Lagrangian Coordination for Decomposition-Based Optimization,"As system design problems increase in complexity, researchers seek approaches to optimize such problems by coordinating the optimizations of decomposed sub-problems. Many methods for optimization by decomposition have been proposed in the literature among which, the Augmented Lagrangian Coordination (ALC) method has drawn much attention due to its efficiency and flexibility. The ALC method involves a quadratic penalty term, and the initial setting and update strategy of the penalty weight are critical to the performance of the ALC. The weight in the traditional weight update strategy always increases and previous research shows that an inappropriate initial value of the penalty weight may cause the method not to converge to optimal solutions.",2014
1187,Solving Structure for Network-Decomposed Problems Optimized With Augmented Lagrangian Coordination,"The complexity of design and optimization tasks of modern products which cannot be carried out by a single expert or by a single design team motivated the development of the field of decomposition-based optimization. In general, the process of decomposition-based optimization consists of two procedures: (1) Partitioning the original problem into sub-problems according to the design disciplines, components or other aspects; and (2) Coordinating these sub-problems to guarantee that the aggregation of their optimal solutions results in a feasible and optimal solution for the original problem. Much current work focuses on alternative approaches for these two procedures.",2014
1188,Cooperative Design Optimization (CDO) for Multidisciplinary Systems,"Design engineers and decision-makers across various fields are constantly working to make optimal design decisions for multidisciplinary engineering systems in an effort to improve performance and reduce costs. The multiple disciplines that decision-makers are forced to consider can range from different physical components of a system, to competing physical phenomena influencing a component (e.g. flow forces and structural strength), to completely separate models of interest to a system (e.g. engineering performance and lifecycle cost). The common element that all these decision-making scenarios share is the presence of couplings between the considered disciplines (or subsystems). How the values for these coupling parameters are determined within a decision-making or optimization framework is the subject of countless research efforts. At present the multidisciplinary design optimization (MDO) community has settled on a few proven techniques such Collaborative Optimization (CO) and Analytic Target Cascading (ATC). However, current MDO techniques have issues that limit their effectiveness in solving various MDO problems. Many of these strategies require close coordination between subsystem optimization solvers, require significant effort by decision-makers to pose their problems in a suitable format, and/or can have large computational efficiency problems due to the fact that they involve solving nested optimization problems. In an effort to alleviate some of these issues and make MDO easier to implement and more computationally efficient, a new sequential MDO algorithm called Cooperative Design Optimization (CDO) is proposed. The CDO approach functions through a series of subsystem optimizations using a successively smaller cooperation space. The cooperation space is analogous to the design space of a traditional optimization problem, but includes only the coupling parameters that are a factor in multiple subsystems. A single iteration of the approach can be thought of as a negotiation between all of the subsystems regarding the boundaries of the cooperation space. To facilitate this cooperation, each subsystem optimization problem is restructured as a multi-objective optimization problem so that a Pareto set of optimal solutions, or agreeable design alternatives, are produced. As a result, after all subsystem optimizations are accomplished, each subsystem’s obtained Pareto optimal solution sets are compared with respect to the coupling parameters and new bounds in the cooperation space are determined for the next iteration. This process allows each subsystem to be optimized completely independently within the boundaries of the agreed upon cooperation space while coordination is achieved through an analysis of obtained optimal solutions for each subsystem. Iterations, or negotiations, are repeated until an acceptable solution or set of solutions is obtained for all included subsystems through this cooperative process.",2014
1189,A Dynamic Service-Oriented Distributed Computing Framework for Evaluation of Computationally Expensive Black-Box Analyses,"A practical, flexible, versatile, and heterogeneous distributed computing framework is presented that simplifies the creation of small-scale local distributed computing networks for the execution of computationally expensive black-box analyses. The framework is called the Dynamic Service-oriented Optimization Computing Framework (DSOCF), and is designed to parallelize black-box computation to speed up optimization runs. It is developed in Java and leverages the Apache River project, which is a dynamic Service-Oriented Architecture (SOA). A roulette-based real-time load balancing algorithm is implemented that supports multiple users and balances against task priorities, which is superior to the rigid pre-set wall clock limits commonly seen in grid computing. The framework accounts for constraints on resources and incorporates a credit-based system to ensure fair usage and access to computing resources. Experimental testing results are shown to demonstrate the effectiveness of the framework.",2014
1190,Application of Analytical Target Cascading for Engine Calibration Optimization Problem,"This paper presents the development of an Analytical Target Cascading (ATC) Multidisciplinary Design Optimization (MDO) framework for a steady-state engine calibration optimization problem. The implementation novelty of this research is the use of the ATC framework to formulate the complex multi-objective engine calibration problem, delivering a considerable enhancement compared to the conventional 2-stage calibration optimization approach [1]. A case study of a steady-state calibration optimization of a Gasoline Direct Injection (GDI) engine was used for the calibration problem analysis as ATC. The case study results provided useful insight on the efficiency of the ATC approach in delivering superior calibration solutions, in terms of “global” system level objectives (e.g. improved fuel economy and reduced particulate emissions), while meeting “local” subsystem level requirements (such as combustion stability and exhaust gas temperature constraints). The ATC structure facilitated the articulation of engineering preference for smooth calibration maps via the ATC linking variables, with the potential to deliver important time saving for the overall calibration development process.",2014
1191,Evaluation of System Reconfigurability Based on Usable Excess,"The challenge of designing complex engineered systems with long service lives can be daunting. As customer needs change over time, such systems must evolve to meet these needs. This paper presents a method for evaluating the reconfigurability of systems to meet future needs. Specifically we show that excess capability is a key factor in evaluating the reconfigurability of a system to a particular need, and that the overall system reconfigurability is a function of the system’s reconfigurability to all future needs combined. There are many examples of complex engineered systems; for example, aircraft, ships, communication systems, spacecraft and automated assembly lines. These systems cost millions of dollars to design and millions to replicate. They often need to stay in service for a long time. However, this is often limited by an inability to adapt to meet future needs. Using an automated assembly line as an example, we show that system reconfigurability can be modeled as a function of usable excess capability.",2014
1192,Using Interfaces to Drive Module Definition: Investigating the Impact of a New Design Dependency Measure,"Structural representations for interfaces between modules and components in a product vary widely in the literature. After reviewing several structural approaches to interface definition, a new weighted design dependency measure is described. The new representation takes into account both six different types of interfaces as well as their relative strength and frequency within a product architecture. The resulting design dependency measure provides a means for designers to quantify the change resistance in a product. In this paper, we investigate the use of this new design dependency measure to drive module identification. Specifically, we compare the resulting modules obtained by optimizing Design Structure Matrices (DSMs) using standard 0-1 representations of the interfaces to those obtained using the new design dependency measure. The results indicate that the weighted design dependency measure leads to more a logical definition of modules that maximizes within module dependencies and minimizes interactions between modules.",2014
1193,Solving the Reconfigurable Design Problem for Multiability With Application to Robotic Systems,"Systems that can be reconfigured are valuable in situations where a single artifact must perform several different functions well, and are especially important in cases where system demands are not known a priori. Design of reconfigurable systems present unique challenges compared to fixed system design. Increasing reconfigurable capability improves system utility, but also increases system complexity and cost. In this article a new design strategy is presented for the design of reconfigurable systems for multiability. This study is limited to systems where all system functions are known a priori, and only continuous means of reconfiguration are considered. Designing such a system requires determination of (1) what system features should be reconfigurable, and (2) what should the range of reconfigurability of these features be. The new design strategy is illustrated using a reconfigurable delta robot, which is a parallel manipulator that can be adapted to perform a variety of manufacturing operations. In this case study the tradeoff between end effector stiffness and speed is considered over two separate manipulation tasks.",2014
1194,Designing Scalable Product Families for Black-Box Functions,"Product family design optimization is a cost-efficient concept for achieving the best tradeoff between commonalization and diversification of products. When design functions are computationally intensive and thus viewed as black-boxes, the product family design becomes more challenging. In this study a two-stage platform configuration and product family design optimization method with generalized commonality is proposed for scale-based families involving black-box functions. The platform configuration is unknown and multiple sub-platforms are allowed. In this study, the main parameters used towards the family design include a non-conventional sensitivity analysis, the detachability property of each variable, and the variation of individual optimal values for each design variable. Metamodeling techniques are employed to provide both the non-conventional sensitivity and correlation intensities information, which leads to significant savings in the number of function calls. Efficiency of this method is tested through designing a scalable family of universal electric motors. Compared to a number of previously developed methods, the proposed method yields a design solution with acceptable performance loss after commonalization, and better value for the aggregated preference objective function while satisfying all the performance constraints.",2014
1195,Decision Topology Assessment in Engineering Design Under Uncertainty,"The implications of decision analysis (DA) on engineering design are well known. Recently, the authors proposed decision topologies (DT) as a visual method for making design decisions and proved that they are consistent with normative decision analysis. This paper addresses the practical issue of assessing DTs for a decision maker (DM) using their responses, particularly under uncertainty. This is critical to encoding decision maker preferences so that further analysis and mathematical optimization can be performed using the correct set of preferences. We show how multiattribute DTs can be directly assessed from DM responses. Four methods are shown to evolutionarily assess DTs among which one that requires the DM to rank alternatives and another where a utility function is first assessed. It is also shown that preferences under uncertainty can be easily incorporated. In addition, we show that topologies can be constructed using single attribute topologies similarly to multi-linear functions in utility analysis. This incremental construction simplifies the process of topology construction. The reverse problem of inferring single attribute DTs is also presented. The proposed assessment methods are used on a design decision making problem of a welded beam.",2014
1196,Redundancy Allocation for Reliability Design of Engineering Systems With Failure Interactions,"In reliability design, allocating redundancy through various optimization methods is one of the important ways to improve the system reliability. Generally, in these redundancy allocation problems, it is assumed that failures of components are independent. However, under this assumption failure rates can be underestimated since failure interactions can significantly affect the performance of systems. This paper first proposed an analytical model describing the failure rates with failure interactions. Then a Modified Analytic Hierarchy Process (MAHP) is proposed to solve the redundancy allocation problems for systems with failure interactions. This method decomposes the system into several blocks and deals with those down-sized blocks before diving deep into the most appropriate component for redundancy allocation. Being simple and flexible, MAHP provides an intuitive way to design a complex system and complex explicit objective functions for the entire system is not required in the proposed approach. More importantly, with the help of the proposed analytical failure interaction model, MAHP can capture the effect of failure interactions. Results from case studies clearly demonstrate the applicability of the analytical model for failure interactions and MAHP for reliability design.",2014
1197,Improve System Reliability and Robustness Using Axiomatic Design and Fault Tree Analysis,"The reliability and robustness are critical aspects for engineering systems. In a complex system, usually multiple functions need to be achieved. In such cases, there exist optimal mappings between functions and physical components. In this paper, a new approach, Axiomatic Fault Tree Analysis (AFTA), is proposed to improve the system reliability and robustness simultaneously. In AFTA, system robustness is improved by using Axiomatic Design (AD) to decouple the system to an extent where a group of functions have a corresponding relationship with only one physical component. Moreover, Fault Tree Analysis and Axiomatic Design have been integrated in AFTA by using two new indices indicating the importance degrees of functions and physical components in a reliability sense, which provides a guideline for reliability improvement. A case study of a complex system is used to illustrate the procedure and applicability of AFTA. Theoretical calculations are used to verify AFTA for non-repairable systems, and multi-agent based simulations are for repairable systems. Both the theoretical and simulated results show that the system reliability has been improved significantly without large additional cost.",2014
1198,Efficient Global Optimization Reliability Analysis (EGORA) for Time-Dependent Limit-State Functions,"If a limit-state function involves time, the associated reliability is defined within a period of time. The extreme value of the limit-state function is needed to calculate the time-dependent reliability, and the extreme value is usually highly nonlinear with respect to random input variables and may follow a multimodal distribution. For this reason, a surrogate model of the extreme response along with Monte Carlo simulation is usually employed. The objective of this work is to develop a new method, called the Efficient Global Optimization Reliability Analysis (EGORA), to efficiently build the surrogate model. EGORA is based on the Efficient Global Optimization (EGO) method. Different from the current method that generates training points for random variables and time independently, EGORA draws training points for the two types of input variables simultaneously and therefore accounts for their interaction effects. The other improvement is that EGORA only focuses on high accuracy at or near the limit state. With the two improvements, the new method can effectively reduce the number of training points. Once the surrogate model of the extreme response is available, Monte Carlo simulation is applied to calculate the time-dependent reliability. Good accuracy and efficiency of EGORA are demonstrated by three examples.",2014
1199,Incorporating Flexibility in the Design of Repairable Systems: Design of Microgrids,"The authors have recently proposed a ‘decision-based’ framework to design and maintain repairable systems. In their approach, a multiobjective optimization problem is solved to identify the best design using multiple short and long-term statistical performance metrics. The design solution considers the initial design, the system maintenance throughout the planning horizon, and the protocol to operate the system. Analysis and optimization of complex systems such as a microgrid is however, computationally intensive. The problem is exacerbated if we must incorporate flexibility in terms of allowing the microgrid architecture and its running protocol to change with time. To reduce the computational effort, this paper proposes an approach that “learns” the working characteristics of the microgrid and quantifies the stochastic processes of the total load and total supply using autoregressive time-series. This allows us to extrapolate the microgrid operation in time and eliminate therefore, the need to perform a full system simulation for the entire long-term planning horizon. The approach can be applied to any repairable system. We show that building in flexibility in the design of repairable systems is computationally feasible and leads to better designs.",2014
1200,A Simulation Method to Estimate the Time-Varying Failure Rate of Dynamic Systems,"The failure rate of dynamic systems with random parameters is time-varying even for linear systems excited by a stationary random input. In this paper, we propose a simulation-based method to estimate this time-varying failure rate. The input and output stochastic processes are discretized using a small time step to calculate the trajectories of the output stochastic process accurately through simulation. The planning horizon (time of interest) is then partitioned into a series of longer correlated time intervals and the Saddlepoint approximation (SPA) is employed to estimate the distribution of maximum response and thus obtain the probability of failure in each time interval. Using the same simulated trajectories with SPA, a time-dependent copula is built to provide the correlation between the response in each time interval and the response up to that time interval. The time-varying failure rate is finally estimated at each discrete time, using the probability of failure in each time interval and the correlation information from the estimated copula. The effectiveness of the proposed method is illustrated with a vehicle vibration example.",2014
1201,An Improved Stochastic Upscaling Method for Multiscale Engineering Systems,A stochastic multiscale modeling technique is proposed to construct coarse scale representation of a fine scale model for use in engineering design problems. The complexity of the fine scale heterogeneity under uncertainty is replaced with the homogenized coarse scale parameters by seeking agreement between the responses at both scales. Generalized polynomial chaos expansion is implemented to reduce the dimensionality of propagating uncertainty through scales and the computational costs of the upscaling method. It is integrated into a hybrid optimization procedure with the genetic algorithm and sequential quadratic programming. Two structural engineering problems that involve uncertainties in elastic material properties and geometric properties at fine scales are presented to demonstrate the applicability and merit of the proposed technique.,2014
1202,A Confidence-Based Adaptive Sampling Approach for Dynamic Reliability Analysis,"Dynamic reliability is defined as the probability that an engineered system successfully performs the predefined functionality over a certain period of time considering time-variant operation condition and component deterioration. In practice, it is still a major challenge to conduce dynamic reliability analysis due to the prohibitively high computational costs. In this study, a confidence-based meta-modeling approach is proposed for efficient sensitivity-free dynamic reliability analysis, referred to as double-loop adaptive sampling (DLAS). In DLAS a Gaussian process (GP) model is constructed to approximate extreme system responses over time, so that Monte Carlo simulation (MCS) can be employed directly to estimate dynamic reliability. A qualitative confidence measure is proposed to evaluate the accuracy of dynamic reliability estimation while using the MCS approach based on developed GP models. To improve the confidence, a double-loop adaptive sampling scheme is developed to efficiently update the GP model in a sequential manner, by considering system input variables and time concurrently in double sampling loops. The model updating process can be terminated once the user defined confidence target is satisfied. The DLAS approach does not require computationally expensive sensitivity analysis, thus substantially improves the efficiency of dynamic reliability assessment. Two case studies are used to demonstrate the effectiveness of DLAS for dynamic reliability analysis.",2014
1203,An Integrated Performance Measure Approach for System Reliability Assessment,"This paper presents an integrated performance measure approach (iPMA) for system reliability assessment considering multiple dependent failure modes. An integrated performance function is developed to envelope all component level failure events, thereby enables system reliability approximation by considering only one integrated system limit state. The developed integrated performance function possesses two critical properties. First, it represents exact joint failure surface defined by multiple component failure events, thus no error will be induced due to the integrated limit state function in system reliability computation. Second, smoothness of the integrated performance on system failure surface can be guaranteed, therefore advanced response surface techniques can be conveniently employed for response approximation. With the developed integrated performance function, the maximum confidence enhancement based sequential sampling method is adopted as an efficient component reliability analysis tool for system reliability approximation. To furthermore improve the computational efficiency, a new constraint filtering technique is developed to adaptively identify active limit states during the iterative sampling process without inducing any extra computational cost. One case study is used to demonstrate the effectiveness of system reliability assessment using the developed iPMA methodology.",2014
1204,Inverse Reliability Analysis for Approximated Second-Order Reliability Method Using Hessian Update,"According to order of approximation, there are two types of analytical reliability analysis methods; first-order reliability method and second-order reliability method. Even though FORM gives acceptable accuracy and good efficiency for mildly nonlinear performance functions, SORM is required in order to accurately estimate the probability of failure of highly nonlinear functions due to its large curvature. Despite its necessity, SORM is not commonly used because the calculation of the Hessian is required. To resolve the heavy computational cost in SORM due to the Hessian calculation, a quasi-Newton approach to approximate the Hessian is introduced in this study instead of calculating the Hessian directly. The proposed SORM with the approximated Hessian requires computations only used in FORM leading to very efficient and accurate reliability analysis. The proposed SORM also utilizes the generalized chi-squared distribution in order to achieve better accuracy. Furthermore, an SORM-based inverse reliability method is proposed in this study as well. A reliability index corresponding to the target probability of failure is updated using the proposed SORM. Two approaches in terms of finding more accurate most probable point using the updated reliability index are proposed and compared with existing methods through numerical study. The numerical study results show that the proposed SORM achieves efficiency of FORM and accuracy of SORM.",2014
1205,Confidence-Based Method for Reliability-Based Design Optimization,"An accurate input probabilistic model is necessary to obtain a trustworthy result in the reliability analysis and the reliability-based design optimization (RBDO). However, the accurate input probabilistic model is not always available. Very often only insufficient input data are available in practical engineering problems. When only the limited input data are provided, uncertainty is induced in the input probabilistic model and this uncertainty propagates to the reliability output which is defined as the probability of failure. Then, the confidence level of the reliability output will decrease. To resolve this problem, the reliability output is considered to have a probability distribution in this paper. The probability of the reliability output is obtained as a combination of consecutive conditional probabilities of input distribution type and parameters using Bayesian approach. The conditional probabilities that are obtained under certain assumptions and Monte Carlo simulation (MCS) method is used to calculate the probability of the reliability output. Using the probability of the reliability output as constraint, a confidence-based RBDO (C-RBDO) problem is formulated. In the new probabilistic constraint of the C-RBDO formulation, two threshold values of the target reliability output and the target confidence level are used. For effective C-RBDO process, the design sensitivity of the new probabilistic constraint is derived. The C-RBDO is performed for a mathematical problem with different numbers of input data and the result shows that C-RBDO optimum designs incorporate appropriate conservativeness according to the given input data.",2014
1206,Time-Dependent Reliability Analysis Using the Total Probability Theorem,"A new reliability analysis method is proposed for time-dependent problems with limit-state functions of input random variables, input random processes and explicit in time using the total probability theorem and the concept of composite limit state. The input random processes are assumed Gaussian. They are expressed in terms of standard normal variables using a spectral decomposition method. The total probability theorem is employed to calculate the time-dependent probability of failure using a time-dependent conditional probability which is computed accurately and efficiently in the standard normal space using FORM and a composite limit state of linear instantaneous limit states. If the dimensionality of the total probability theorem integral (equal to the number of input random variables) is small, we can easily calculate it using Gauss quadrature numerical integration. Otherwise, simple Monte Carlo simulation or adaptive importance sampling is used based on a pre-built Kriging metamodel of the conditional probability. An example from the literature on the design of a hydrokinetic turbine blade under time-dependent river flow load demonstrates all developments.",2014
1207,Topology Optimization With Unknown-but-Bounded Load Uncertainty,"In this paper, convex modeling based topology optimization with load uncertainty is presented. The load uncertainty is described using the non-probabilistic based unknown-but-bounded convex model, and the strain energy based topology optimization problem under uncertain loads is formulated. Unlike the conventional deterministic topology optimization problem, the maximum possible strain energy under uncertain loads is selected as the new objective in order to achieve a safe solution. Instead of obtaining approximated solutions as used before, an exact solution procedure is presented. The problem is first formulated as a single level optimization problem, and then rewritten as a two-level optimization problem. The upper level optimization problem is solved as a deterministic topology optimization with the load which generated from the worst structure response in the lower level problem. The lower level optimization problem is to identify this worst structure response, and it is found equivalent to an inhomogeneous eigenvalue problem. Three different cases are discussed for accurately evaluating the global optima of the lower level optimization problem, while the corresponding sensitivities are derived individually. With the function value and sensitivity information ready, the upper level optimization problem can be solved through existing gradient based optimization algorithms. The effectiveness of the proposed convex modeling based topology optimization is demonstrated through different numerical examples.",2014
1208,Applying Robust Design Optimization to Large Systems,"While Robust Optimization has been utilized for a variety of design problems, application of Robust Design to the control of large-scale systems presents unique challenges to assure rapid convergence of the solution. Specifically, the need to account for uncertainty in the optimization loop can lead to a prohibitively expensive optimization using existing methods when using robust optimization for control. In this work, a robust optimization framework suitable for operational control of large scale systems is presented. To enable this framework, robust optimization uses a utility function for the objective, dimension reduction in the uncertainty space, and a new algorithm for evaluating probabilistic constraints. The proposed solution accepts the basis in utility theory, where the goal is to maximize expected utility. This allows analytic gradient and Hessian calculations to be derived to reduce the number of iterations required. Dimension reduction reduces uncertain functions to low dimensional parametric uncertainty while the new algorithm for evaluating probabilistic constraints is specifically formulated to reuse information previously generated to estimate the robust objective. These processes reduce the computational expense to enable robust optimization to be used for operational control of a large-scale system. The framework is applied to a multiple-dam hydropower revenue optimization problem, then the solution is compared with the solution given by a non-probabilistic safety factor approach. The solution given by the framework is shown to dominate the other solution by improving upon the expected objective as well as the joint failure probability.",2014
1209,An Efficient Reliability Analysis Method for Structures With Epistemic Uncertainty Using Evidence Theory,"Evidence theory has a strong ability to deal with the epistemic uncertainty, based on which the uncertain parameters existing in many complex engineering problems with limited information can be conveniently treated. However, the heavy computational cost caused by its discrete property severely influences the practicability of evidence theory, which has become a main difficulty in structural reliability analysis using evidence theory. This paper aims to develop an efficient method to evaluate the reliability for structures with evidence variables, and hence improves the applicability of evidence theory for engineering problems. A non-probabilistic reliability index approach is introduced to obtain a design point on the limit-state surface. An assistant area is then constructed through the obtained design point, based on which a small number of focal elements can be picked out for extreme analysis instead of using all the elements. The vertex method is used for extreme analysis to obtain the minimum and maximum values of the limit-state function over a focal element. A reliability interval composed of the belief measure and the plausibility measure is finally obtained for the structure. Two numerical examples are investigated to demonstrate the effectiveness of the proposed method.",2014
