id,title,abstract,year
1557,Finite Element Modeling and Analysis of Orthotropic Butt Welds,"Butt welds with orthotropic behavior are widely applied in mechanical and structural designs. Since welds cannot always be perfect in practice, it is important to understand the weld’s stress behavior under different imperfect geometries. In this paper research has been performed to investigate the relationship between stress intensity factors and change of geometry of orthotropic butt welds. Finite element methods were applied to simulate weld geometries. The simulation was performed using ANSYS software assuming two beams are welded together with a discontinuity at the bottom of the weld. The combined beams and the butt weld are then considered to be one piece of glued structure. The discontinuity in the structure is used to model a crack and lack of weld penetration. By changing three important factors of the weld geometry under uniform axial static loads, the trend of stress intensity factor behavior versus change of geometry has been investigated. Both single and double sided butt welds were considered in this paper. The results of this investigation will be a helpful tool for design engineers in deciding the best weld geometry in applications.",2010
1558,Theoretical and Experimental Characterization of the 34.6 2D Lattice Material,"In this paper, the effective mechanical properties of the 34 .6 lattice material are characterized theoretically and experimentally. The characterized properties include the stiffness and the strength of the material. A detailed description of the design procedure of the test specimens is presented. Three quasi-static tests are performed, namely, uniaxial tension, uniaxial compression and pure shear. The comparison of the experimental data to the theoretical results shows that the former are in good agreement with the latter. The maximum error obtained of 13% is acceptable according to the literature on experimental studies of cellular solids.",2010
1559,A Viscoelastic Based Mechanism for Improving Spring-In Angle Predictions in Compression Molded Thermoplastic Matrix Composites,"In this paper, dimensional distortion during the compression molding of thermoplastic matrix composites, typically described as spring-in or spring forward, is investigated through a finite element model. Spring-in is the reduction of the enclosed angle of two surfaces on the final component shape with respect to the original mold shape. Spring-in of thermoplastic matrix composites has typically been attributed to the difference in the thermal expansion of in-plane and through thickness directions of the composite. However, using this mechanism alone during modeling has not shown complete agreement with the experimental data. A new meso-level mechanism based on the viscoelasticity effect of the thermoplastic matrix is proposed. With this mechanism, the predicted spring-in angle can be in good agreement with experiments.",2010
1560,The Effect of Carbon Nanotubes on the Natural Frequencies of Microcantilever Beams,"In this study, the natural frequencies and mode shapes of carbon nanotube (CNT) reinforced polymer composite microcantilever beams are investigated by means of a micromechanical model and the three-dimensional finite element analysis. Microcantilever beams are made of Poly vinyl chloride (PVC) and reinforced with multi-wall carbon nanotubes (MWCNTs). MWCNTs can be distributed along the length/width/thickness of the nanocomposite beam. To validate the accuracy and effectiveness of the model, a direct comparison of results is made with an analytical solution for a test case. Next, various material types of the nanocomposite microcantilever beam are introduced and the effect of different distribution patterns and the weight-percents (wt%) of MWCNTs on the first six natural frequencies and mode shapes is found.",2010
1561,Experimental Damage Characterization of Hexagonal Honeycombs Subjected to In-Plane Shear Loading,"Experimental study on the damage of hexagonal honeycombs under in–plane shear loading does not appear to be available in the literature. In this paper, shear damage behaviors of five different hexagonal mesostructures are investigated with rapid prototyped polycarbonate (PC) honeycomb coupon samples and proper design of a fixture for shear loading. Effective shear stress-strain curves of PC honeycomb coupons are generated for each shear test and the corresponding local cell wall failure is investigated. Two different failure modes of PC honeycombs were observed primarily depending on the cell wall thickness: The PC honeycombs having a lower cell wall thickness induce the plastic post buckling, resulting in preventing propagation of initial cracks through the cell wall end up with higher plastic load bearing. On the other hand, the failure mode of the honeycombs having a high cell wall thickness is the cell wall fracture by crack propagation through wall without severe buckling.",2010
1562,Design of Chiral Honeycomb Meso-Structures for High Shear Flexure,Chiral honeycombs are auxetic cellular structures that exhibits negative Poison’s ratio. Chiral honeycombs are structures arranged in an array of cylinders connected by ligaments. Four different configurations of these geometries with 4- and 6- ligaments attached are investigated for its use in shear layer of non-pneumatic wheel. The objective of the study is to find an ideal geometry for the shear layer while meeting its requirements of shear properties (about 6.5 MPa effective shear modulus and 0.15 maximum effective shear strain) with polycarbonate as base material. Finite Element (FE) based numerical tests are carried out and optimum chiral meso-structures are found for the target shear properties. Parametric studies on geometries are also conducted to find the effect of geometries on the target properties. The effect of cell wall thickness is studied and the optimum thickness is suggested to meet the target requirements. Effect of direction of shear loading has been studied on each different configuration in order to minimize the effect of direction of loading.,2010
1563,A Study of the Effect of Geometry Changes on the Structural Stiffness of a Composite D-Spar,"The paper examines the impact of varying two geometric cross-section parameters of an advance composite D-spar on its structural stiffness. For a given blade topology, the orientation of the D-spar web with respect to the beam axis and the distance of the D-spar web from the leading edge of the blade have been selected here as the variables of study, as they govern the elastic properties of the composite cross-section. A code has been developed to calculate the matrix terms of the Euler-Bernoulli cross-sectional stiffness utilizing the closed form expressions of the structural properties formulated by assuming both Thin-Walled composite Beam theory (TWB) and Classical Laminate Theory. The code has been validated through the Variational Asymptotic Beam Sectional analysis (VABS) for the cross-sectional stiffness matrix. Two cases have been studied for a quasi-isotropic laminate D-spar. The first is for a symmetric airfoil, whereas the second is for an unsymmetrical airfoil. The variation of the stiffness parameters for the quasi-isotropic D-spar including the coupling parameters has been visualized into parametric maps. The paper also examines the impact that these geometric variables have on the stiffness-to-mass ratio to show that along with the ply orientations they play a major role in the aeroelastic tailoring and structural optimization of a composite blade.",2010
1564,Low-Velocity Impact Behavior of Aluminum-Fiber/Epoxy Laminates: A Comprehensive Experimental Study,"Considering many potential applications of fiber reinforced metal laminates (FMLs) in sensitive structures, it is necessary to understand their mechanical behavior under impact loads. In this study, low velocity impact tests based on ASTM D7136 have been conducted on FMLs made of 1050 aluminum sheets and various types of fiber reinforced polymer (FRP) layers; namely E-Glass, Kevlar 49, and carbon T300 plain woven in the epoxy resin. Projectile energy, fiber type and the number of successive impacts are selected among important parameters that can affect the performance of FMLs. In particular, the effects of these parameters on the absorbed energy, contact force, front and rear face damage areas, central deflection and permanent deformation of FMLs have been investigated. For determining the damage area and central deflection of the specimens, an image processing method is adapted.",2010
1565,Safety of Spur Gear Design Under Non-Ideal Conditions With Uncertainty,"The current practice of gear design is based on the Lewis bending and Hertzian contact models. The former provides the maximum stress on the gear base, while the latter calculates the contact pressure at the contact point between the gear and pinion. Both calculations are obtained at the reference configuration with ideal conditions; i.e., no tolerances and clearances. The first purpose of this paper is to compare these two analytical models with the numerical results, in particular, using finite element analysis. It turns out that the estimations from the two analytical equations are closely matched with those of the numerical analysis. The numerical analysis also yields the variation of contact pressures and bending stresses according to the change in the relative position between gear and pinion. It has been shown that both the maximum bending stress and contact pressure occur at non-reference configurations, which should be considered in the calculation of a safety factor. In reality, the pinion-gear assembly is under the tolerance of each part and clearance between the parts. The second purpose of this report is to estimate the effect of these uncertain parameters on the maximum bending stress and contact pressure. For the case of the selected gear-pinion assembly, it turns out that due to a 0.57% increase of clearance, the maximum bending stress is increased by 4.4%. Due to a 0.57% increase of clearance, the maximum contact pressure is increased by 17.9%.",2010
1566,Subfunctions as Parts of Functions: Some Formal Problems,"In this paper a proof is presented that shows that the relation between technical functions and their subfunctions in functional descriptions of products can formally not be taken as a relation of parthood. Technical functions of two specific classes are modelled as well as their composition. In this modelling functions are taken as transformations of tokens of flows of energy, material and signals, which makes them proper instances of functions on many engineering accounts of functions. Then it is proved that the relations between the considered functions and their subfunctions do not in general meet the basic postulates of mereology, the theory of parthood relations. The ramification of this proof is that in engineering ontologies the relation between subfunctions and functions should not be described as a formal parthood relation.",2010
1567,Development and Application of a Patent-Based Design Around Process,"This research proposes a patent-based design process by systematically integrating patent information, the rules of patent infringement judgment, strategies of designing around patents, and innovation design methodologies. The purpose of the process is to systematically generate new design concepts that are local variations of one of the concerned patents but does not infringe with existing patents. The basic idea is to consider patent infringement before engineering design concepts are actually generated. In this process, first the designer conducts standard patent analysis to identify the related patents to be designed around. Each patent is then symbolized by a “design matrix” converted from the technology/function matrix of the patent. A design-around algorithm is developed to generate a new design matrix that does not infringe with design matrices of existing patents. Then the new design matrix is transformed back into a real engineering design using the “contradiction matrix” in TRIZ. A computerized design-around tool based on the innovative patent-based design process is also developed.",2010
1568,A Port-Based Agent Approach to Guiding Concept Generation for Customizing Modular Varieties,"As the description of design requirements at the earlier design stage is inaccurate and vague, it is difficult to figure out functional structure of a product and make sense product configuration. Therefore, it plays an important role to formally represent the process of design for product development in the conceptual design stage. Furthermore, port, as the location of intended interaction, is crucial to capture component concept and realize conceptual design for multi-solution generation. Agent is considered as an effective approach to collaboratively implementing design problem solving and reasoning. Combining both port and agent may be employed to generate new concepts of the product in order to customize product scheme varieties. In this paper, the product module attributes are firstly described. The objective is to implement modeling of design process for obtaining system new concepts to guide multi-solution generation. Secondly, an effective approach to decomposing design process is presented to describe the process of structure generations and product decomposition by formal representation. According to properties of modularity for product development and component connections, we can calculate the number of component connections and density of components. In addition, product module division and coupling degree analysis are conducted, and coupling degrees are calculated by considering the correspondence ratio and the cluster independence. A port-based knowledge building process is described for functional modeling. A port-agent collaborative design framework is given and describes different agent functions to help designers to obtain new design schemes. Finally, a case study is presented to describe the modeling process of conceptual design.",2010
1569,Requirement Change Propagation Prediction Approach: Results From an Industry Case Study,"This paper presents an industry case study investigating change propagation due to requirement changes. This paper makes use of a change propagation prediction tool, ΔDSM, to identify if the propagated changes could have been identified and predicted. The study used an automation firm’s client project as the study subject. The project entailed 160 requirements, changing over the span of 15 month. Engineering change notifications were developed for each change and documented under the firm’s data management system. This study makes use of the change notifications to identify if any of the change were as a result of a previous change. The findings of this paper indicated the changes that occurred could have been predicted as the ΔDSM was able to predict affected requirements. This was identified by finding subsequent requirements in the engineering change notification documentation that the ΔDSM indicated might change.",2010
1570,A Graph Theory Based Method for Functional Decoupling of a Design With Complex Interaction Structure,"The primary objective in design is to achieve the target value of the design’s functional requirement. In design with multiple functional requirements, one way a design fails is the inability to converge to the multiple target values in spite of iterative adjustment of the design parameters. This is symptom of a design that fails to perform in the presence of functional coupling. Functional coupling occurs when two or more functional requirements are affected by a common set of design parameters. It is particularly difficult to identify and break when it involves inter-relation loops created among large number of functional requirements, typical of a large complex system. This paper presents a structured method based on the graph theory to effectively identify and eliminate functional couplings in a design. Use of the graph theory in this context is natural by the fact that inter-relations among functional requirements and design parameters can be represented by a digraph. Each inter-relation corresponds to an arc of the digraph, and functional coupling is equivalent to a cycle in it. The proposed method consists of: 1) represent interactions among functional requirements and design parameters as a digraph, 2) construct the cycle matrix for the digraph, 3) identify those candidate sets of arcs that, if removed, will destroy all cycles in the digraph, and 4) examine engineering feasibility of the candidate solutions. Once target interactions, i.e. arcs, are determined, the design parameters responsible for those interactions are modified to implement the solution. To demonstrate the effectiveness of the proposed method, we apply it to a large complex system, the car door to body, involving 28 functional requirements and design parameters.",2010
1571,"Mass Customization: A Review of the Paradigm Across Marketing, Engineering and Distribution Domains","Introduced nearly 25 years ago, the paradigm of mass customization (MC) has largely not lived up to its promise. Despite great strides in information technology, engineering design practice, and manufacturing production, the necessary process innovations that can produce products and systems with sufficient customization and economic efficiency have yet to be found in wide application. In this paper, the state-of-the-art in MC is explored in order to answer the question of “why not?” and to highlight areas for specific research in the MC paradigm. To establish perspective for this work, we consider MC to be a product development approach which allows for the production of goods — after a customer places an order — which minimize the tradeoff between the ideal product and the available product by fulfilling the needs and preferences of individuals functionally, emotionally and anthropologically. Results of this research were generated by reviewing 88 papers from various journals that span three domains of interest (marketing, engineering, and distribution) and explore proposed methodologies, specific information inputs and outputs, proposed metrics, and barriers toward the implementation of MC. Qualitatively, we show that the lack of MC in application is due to two factors: 1) a lack of marketing tools capable of capturing individual needs that can be mapped to the technical space; and 2) a lack of information relation mechanisms that connect the domains of marketing, engineering, and distribution. In the end it is our belief that MC is realizable and that eventually it will emerge as a dominant paradigm in the design and delivery of products and systems. However, pursuing the opportunities for research presented in this work will hopefully speed this emergence.",2010
1572,Automated Concept Generation Using Branched Functional Models,"This paper discusses a new concept generation technique that improves upon a previous automated concept generation theory and algorithm developed by Bryant, et al. at the University of Missouri – Rolla. The previous automated concept generation algorithm utilizes the design knowledge present in a repository to produce an array of partial concept solutions. While the previous algorithm is capable of handling branched functional models, it does not efficiently remove all of the infeasible partial solutions to leave only whole concepts in the final results. A matrix-based algorithm is presented in this paper that utilizes the result from the previous concept generation algorithm and solves for complete solutions of branched concepts. The presented algorithm eliminates incomplete and infeasible concepts or components from the results and generates a set of full solutions for further analysis by a designer. The details of the algorithm are described in this paper, and a peanut-sheller example is used to illustrate the effective use of the algorithm for producing branched concept variants.",2010
1573,Development of Game Theoretic Protocols for Multilevel Design,"The effectiveness of the use of game theory in addressing multi-objective design problems has been illustrated. For the most part, researchers have focused on design problems at single level. In this paper, we illustrate the efficacy of using game theoretic protocols to model the relationship between multidisciplinary engineering teams and facilitate decision making at multiple levels. We will illustrate the protocols in the context of an underwater vehicle with three levels that span material and geometric modeling associated with microstructure mediated design of the material and vehicle.",2010
1574,Exploring Automated Concept Generator Output Through Principal Component Analysis,"During conceptual design it is desirable to produce many potential solutions. Recently, computational tools have emerged to help designers more fully explore possible solutions. These automated concept generators use knowledge from existing products and the desired functionality of the new design to suggest solutions. While research has shown these tools can increase the variety of solutions developed, they often provide unmanageably large sets of poorly differentiated results. This work proceeds from the hypothesis that automated concept generator output includes many permutations of a relatively few principal solution variants. A method to discover these underlying solution types from the initial concept generator output is proposed. The proposed method employs principal component analysis for variable reduction followed by cluster analysis for classification. The method is applied to the automatically generated solutions of three sample design problems. Preliminary evidence of the utility and efficiency of the proposed method is presented based upon those sample problems. Finally, a method for extending the proposed technique to much larger solution sets is discussed.",2010
1575,Concept Opportunity Diagrams: A Visual Modeling Method to Find Multifunctional Design Concepts,"A transforming product is a system that has different functionality when physically changed or reconfigured into a different state. This increased functionality allows diverse customer needs to be met in a single product. Transforming devices have become more prevalent in recent years, as customers desire both increased capabilities and reduced complexity to reduce waste in our society. When designing a multifunctional product that transforms from one state to another, it can be difficult to conceptualize a design that does not reduce effectiveness or provide a compromise in either state. Transformational Design Theory has been developed and shows basic principles and facilitators that enable transformation to occur within a product space. An illustrative example is a chair designed to flip over to be used as a table. Flip is one of the 19 facilitators that are found in transformation design. This is also an example of expose/cover, a transformation design principle. Certain principles and facilitators are more prevalent than others in different design domains (such as tools, storage, organisms etc.). If we know the states that exist within the transformer, concept opportunity diagrams can be used to determine the opportunities for transformation within each state. When the diagrams are paired with a constituent relationship chart specific to each domain, new design concepts may be facilitated. This technique creates a cognitive process for designers where they process a series of questions when creating the concept opportunity diagram. The diagram will help them understand the unanticipated additional design space of each state. The Constituent Relationship Chart is a tool that allows them to apply their knowledge of these states to the facilitator hierarchy so that prospective facilitators can directly contribute to originally unforeseen design concepts. This paper presents this twofold process known as the Transformer Diagram Matching Method and shows the results on a fully functioning prototype of an office supply transformer. Although the proposed process is detailed, it allows the designer to find a large number of quality concepts they would not have foreseen otherwise. Our original concept generation processes produced thirty eight ideas, but this process added another thirty two ideas to the design space. The paper indicates specifically how this method can be integrated in with the standard transformational design process as well as suggests strategies for implementation within other design techniques.",2010
1576,Modular Product Configuration: An Automatic Tool for Eliciting Design Knowledge From Parametric CAD Models,"The offer of tailored products is a key factor to satisfy specific customer needs in the current competitive market. Modular products can easily support customization in a short time. Design process, in this case, can be regarded as a configuration task where solution is achieved through the combination of modules in overall product architecture. In this scenario efficient configuration design tools are evermore important. Although many tools have been already proposed in literature, they need further investigation to be applicable in real industrial practice, because of the high efforts required to implement system and the lack of flexibility in products updating. This work describes an approach to overcome drawbacks and to introduce a product independent configuration system which can be useful in designing recurrent product modules. To manage configuration from the designer perspective, the approach is based on Configurable Virtual Prototypes (CVP). In particular, the definition of geometrical models is analyzed providing a tool for eliciting and reusing knowledge introduced by parametric template CAD models. Semantic rules are used to recognize parts parameterization and assembly mating constraints. The approach is exemplified through a case study.",2010
1577,Reconfigurable Products and Their Means of Reconfiguration,"Reconfigurable systems are able to meet the increasingly diverse needs of consumers. A reconfigurable system is able to change its configuration repeatedly and reversibly to match the customer’s needs or the surrounding environment, allowing the system to meet multiple requirements. In this paper, a sample of reconfigurable products was studied to better understand the methods used to achieve different configurations. Four methods of reconfiguration were discovered. This expands work previously done in a parallel field with products that transform where only three methods were identified. In order to support the findings of this paper, the variations were identified and example products were presented that clearly show the need for at least one additional method of reconfiguring. A case study is also provided to illustrate the benefits of incorporating four principles, as apposed to three, into the concept generation phase of new reconfigurable product development.",2010
1578,"Optimal Adaptable Design Considering Changes of Requirements, Configurations and Parameters in the Whole Product Life-Cycle","Adaptable design is a new design approach to create an adaptable product to replace multiple products for satisfying the different requirements in the product life-cycle. In this research, a method to identify the optimal product considering changes of requirements, configurations and parameters in the whole product life-cycle is introduced. The requirements, configurations and parameters of the adaptable product are modeled as functions of the life-cycle time parameter. The adaptable product is changed to different configurations and parameters to satisfy the different requirements in different life-cycle time periods. The evaluation measures, which are achieved from configurations and parameters, are also changed in different life-cycle time periods. The optimal product, modeled by its configurations and parameters, considering the whole product life-cycle is identified through optimization. A case study is provided to demonstrate how the introduced method can be employed for solving engineering problems.",2010
1579,State Transition in Reconfigurable Systems,"Reconfigurable systems that maintain a high level of performance under changing operational conditions and requirements are an ongoing research challenge. Many existing systems are able to work under narrow operational conditions only. They should perform the configuration transitions from the current state to a desired one smoothly. The question is: how to find the optimal state transition trajectory that configures the system from an initial state to a desired state? We present a method to determine steps (list of actions to be taken) of state transitions for reconfigurable systems. This method makes use of graph search algorithms that solve shortest path problems and that are commonly used in routing: Dijkstra’s algorithm and the A* algorithm. The method is applied to the design of a reconfigurable printer, which has to change its configuration to achieve maximum performance (e.g. high quality of print) when operational conditions are changing (e.g. speed of printing).",2010
1580,An Engineering Design Strategy for Reconfigurable Products That Support Poverty Alleviation,"Reconfigurable products can adapt to new and changing customer needs. One potential, high-impact, area for product reconfiguration is in the design of income-generating products for poverty alleviation. Non-reconfigurable income-generating products such as manual irrigation pumps have helped millions of people sustainably escape poverty. However, millions of other impoverished people are unwilling to invest in these relatively costly products because of the high perceived and actual financial risk involved. As a result, these individuals do not benefit from such technologies. Alternatively, when income-generating products are designed to be reconfigurable, the window of affordability can be expanded to attract more individuals, while simultaneously making the product adaptable to the changing customer needs that accompany an increased income. The method provided in this paper significantly reduces the risks associated with purchasing income-generating products while simultaneously allowing the initial purchase to serve as a foundation for future increases in income. The method presented builds on principles of multiobjective optimization and Pareto optimality, by allowing the product to move from one location on the Pareto frontier to another through the addition of modules and reconfiguration. Elements of product family design are applied as each instantiation of the reconfigurable product is considered in the overall design optimization of the product. The design of a modular irrigation pump for developing nations demonstrates the methodology.",2010
1581,A Framework for Choice Modeling in Usage Context-Based Design,"Usage Context-Based Design (UCBD) is an area of growing interest within the design community. A framework and a step-by-step procedure for implementing consumer choice modeling in UCBD are presented in this work. To implement the proposed approach, methods for common usage identification, data collection, linking performance with usage context, and choice model estimation are developed. For data collection, a method of try-it-out choice experiments is presented. This method is necessary to account for the different choices respondents make conditional on the given usage context, which allows us to examine the influence of product design, customer profile, usage context attributes, and their interactions, on the choice process. Methods of data analysis are used to understand the collected choice data, as well as to understand clusters of similar customers and similar usage contexts. The choice modeling framework, which considers the influence of usage context on both the product performance, choice set and the consumer preferences, is presented as the key element of a quantitative usage context-based design process. In this framework, product performance is modeled as a function of both the product design and the usage context. Additionally, usage context enters into an individual customer’s utility function directly to capture its influence on product preferences. The entire process is illustrated with a case study of the design of a jigsaw.",2010
1582,Product Family Deployment Through Optimal Resource Allocation Under Market System,"A series of products, i.e. a product family is deployed for effectively and flexibly meeting with a variety of customer’s needs under a given product platform. Since such a deployment consumes various engineering resources and simultaneously brings profits gradually over the time sequence, when and how respective modules are designed and respective products are launched to the market must be rationally planed. Further, as a nature of product families, module commonalization accelerates the deployment but infuses some overheads on features and production cost. This paper investigates such a product family deployment problem under the optimal design viewpoint. After some general discussions, a mathematical model of dynamic design decisions is conditionally developed by integrating a combinatorial optimization technique for decision of module selection on commonalization and a market system model with discrete choice analysis and for describing the compromise among sequence of product rollout, arrangement of product lineup, required engineering resource, expected profit, etc. Then, the compromise among those factors is illustrated through the case study on a simplified deployment problem of circuit boards for digital television sets. Finally, an optimal planning approach for product family deployment and accompanied resource allocation is envisioned based on the developed model and findings from the case studies.",2010
1583,Strategic Product Design Decisions for Uncertain Market Systems Using an Agent Based Approach,"Market players, such as competing manufacturing firms and retail channels, can significantly influence the demand and profit of a new product. Existing methods in design for market systems use game theoretic models that can maximize a focal manufacturing firm’s profit with respect to product design and price variables given the Nash equilibrium of the market system. However, in the design for uncertain market systems, there is seldom equilibrium with players having fixed strategies in a given time period. In this paper, we propose an agent based approach for design for market systems that accounts for learning behaviors of the market players under uncertainty. By learning behaviors we mean that market players gradually, over a time period, learn to play with better strategies based on action-reaction behaviors of other players. We model a market system with agents representing competing manufacturers and retailers who possess learning capabilities and are able to automatically react and make decisions on the product design and pricing. The proposed approach provides strategic design and pricing decisions for a focal manufacturer in response to anticipated reactions from market players in the short and long term horizons. Our example results show that the proposed agent based approach can produce competitive strategies for a focal firm over a time period when market players react only by setting prices compared to a game theoretic approach. Furthermore, it can yield profitable product design decisions and competitive strategies when competing firms react by changing design attributes in the short term — a case for which no previous method in design for market systems has been reported.",2010
1584,A Method for Supporting Service Design Based on Multiple Domain Knowledge,"Recently, it has become increasingly important to provide customers with highly creative services to attain differentiation from competing firms. Fulfilling customer requirements with such creative service contents is an effective way to differentiate a firm’s services from those of its competitors. However, there have been few studies that directly support the creation of new service contents, e.g., service design support using information-processing technology for knowledge by the computer. It is valid for service providers to acquire creative service design solutions that fulfill customer requirements. In this paper, we propose a support method to fulfill customer requirements for a target service by using the functions of another type of service. This method supports the acquisition of new service design solutions on the basis of the similarity with the customer requirements or functions of different services. The proposed method is verified by applying it to an existing service case.",2010
1585,Parallel Biogeography-Based Optimization With GPU Acceleration for Nonlinear Optimization,"This paper presents a massively parallel Biogeography-based Optimization – Pattern Search (BBO-PS) algorithm with graphics hardware acceleration on bound constrained optimization problems. The objective of this study was to determine the effectiveness of using Graphics Processing Units (GPU) as a hardware platform for BBO-PS. GPU, the common graphics hardware found in modern personal computers (PC), can be used for data-parallel computing in a desktop setting. In this research, the BBO was adapted in the data-parallel GPU computing platform featuring ‘Single Instruction – Multiple Thread’ (SIMT). The global optimal search of the BBO was enhanced by the classical local Pattern Search (PS) method. The hybrid BBO-PS method was implemented in the GPU environment, and compared to a similar implementation in the common computing environment with a Central Processing Unit (CPU). Computational results indicated that GPU-accelerated SIMT-BBO-PS method was orders of magnitude faster than the corresponding CPU implementation. The main contribution of this paper was the parallelization analysis and performance analysis of the hybrid BBO-PS with GPU acceleration. The research result was significant in that it demonstrated a very promising direction for high speed optimization with desktop parallel computing on a personal computer (PC).",2010
1586,Constraint Importance Mode Pursuing Sampling for Continuous Global Optimization,"Many engineering design problems deal with global optimization of constrained black-box problems which is usually computation-intensive. Ref. [1] proposed a Mode-Pursuing Sampling (MPS) method for global optimization based on a sampling technique which systematically generates more sample points in the neighborhood of the function mode while statistically covering the entire problem domain. In this paper, we propose a novel and more efficient sampling technique which greatly enhances the performance of the MPS method, especially in the presence of ",2010
1587,"Design Preference Elicitation, Derivative-Free Optimization and Support Vector Machine Search","In design preference elicitation, we seek to find individuals’ design preferences, usually through an interactive process that would need only a very small number of interactions. Such a process is akin to an optimization algorithm that operates with point values of an unknown function and converges in a small number of iterations. In this paper, we assume the existence of individual preference functions and show that the elicitation task can be translated into a derivative-free optimization (DFO) problem. Different from commonly-studied DFO formulations, we restrict the outputs to binary classes discriminating sample points with higher function values from those with lower values, to capture people’s natural way of expressing preferences through comparisons. To this end, we propose a heuristic search algorithm using support vector machines (SVM) that can locate near-optimal solutions with a limited number of iterations and a small sampling size. Early experiments with test functions show reliable performance when the function is not noisy. Further, SVM search appears promising in design preference elicitation when the dimensionality of the design variable domain is relatively high.",2010
1588,"Protocol-Based Multi-Agent Systems: Examining the Effect of Diversity, Dynamism, and Cooperation in Heuristic Optimization Approaches","Many heuristic optimization approaches have been developed to combat the ever-increasing complexity of engineering problems. In general, these approaches can be classified based on the diversity of the search strategies used, the amount of change to those search strategies during the optimization process, and the level of cooperation between the strategies. A review of the literature indicates that approaches which are simultaneously very diverse, highly dynamic, and cooperative are rare but have immense potential for finding high quality final solutions. In this work, a taxonomy of heuristic optimization approaches is introduced and used to motivate a new approach, entitled Protocol-based Multi-Agent Systems. This approach is found to produce final solutions of much higher quality when its implementation includes the use of multiple search protocols, the adaptation of those protocols during the optimization, and the cooperation between the protocols than when these characteristics are absent.",2010
1589,A Hybrid Biomimetic Genetic Algorithm Using a Local Fuzzy Simplex Search,"This paper presents a hybrid genetic algorithm that expands upon the previously successful approach of twinkling genetic algorithm (TGA) by incorporating a highly efficient local fuzzy-simplex search within the algorithm. The TGA was in principle a bio-mimetic algorithm that introduced a controlled deviation from a typical GA method, by not requiring that every genevariable of an offspring be the result of a crossover. Instead, twinkling allowed the genetic information of the randomly chosen gene locations to be directly passed on from one parent, which was shown to increase the likelihood of survival of a successful gene value within the offspring, rather than requiring it to be blended. The twinkling genetic algorithms proved highly effective at locating exact global optimum with a competitive rate of convergence for a wide variety of benchmark problems. In this work, it is proposed to couple the TGA with a fuzzy simplex local search to increase the rate of convergence of the algorithm. The proposed algorithm is tested using common mathematical and engineering design benchmark problems. Comparison of the results of this algorithm with earlier algorithms is presented.",2010
1590,Computation of the Usage Contexts Coverage of a Jigsaw With CSP Techniques,"In the context of the Usage Context Based Design (UCBD) of a product-service, a taxonomy of variables is suggested to setup the link between the design parameters of a product-service and the part of a set of expected usages that may be covered. This paper implements a physics-based model to provide a performance prediction for each usage context that also depends on the user skill. The physics describing the behavior and consequently the performances of a jigsaw are established. Simulating numerically the usage coverage is non trivial for two reasons: the presence of circular references in physical relations and the need to efficiently propagate value sets or domains instead of accurate values. For these two reasons, we modeled the usage coverage issue as a Constraint Satisfaction Problem and we result in the expected service performances and a value of a covered usage indicator.",2010
1591,Visual Analysis of User Accommodation,"This study presents a novel, quantitative tool for design decision-making for products designed for human variability. Accommodation, which describes the ability of a user to interact with a device or environment in a preferred way, is a key product performance metric. Methods that offer a better understanding of accommodation of broad user populations would allow for the design of products that are more cost-effective, safer, and/or lead to greater levels of customer satisfaction. Target user populations are often characterized by measures of anthropometry, or body dimensions. A methodology is proposed that uses a visual analysis method for understanding and exploring accommodation across the variability in anthropometry of a target user population. This is achieved by assessing binary accommodation of individuals using a “virtual fit” method and examining trends in binary accommodation across the range of anthropometric variability, referred to as the “anthropometry space”. Various factors influencing accommodation, such as user preference independent of anthropometry and the quality of a design, are also discussed and are an important contribution of the work. Two demonstration studies are presented that illustrate the methodology and provide opportunity for discussion of its impact. The first study investigates the simple univariate problem of dimensionally optimizing the seat height and range of adjustability of an exercise cycle. The second study investigates the more complex problem of optimally configuring the driver package of a commercial truck.",2010
1592,Considering Secular and Demographic Trends in Designing for Present and Future Populations,"In products designed for human variability, the anthropometry (body measurements) of the target user population constitutes a primary source of variability that must be considered in the optimization of the spatial dimensions of the product. Accommodation, which describes the ability of a user to interact with a device or environment in their preferred manner, is a key measure of its performance. Other studies have considered various methods for accounting for the variability in anthropometry in a target user population to calculate estimated accommodation, but few have explicitly considered the effects of secular trends and demographic changes over time. This paper considers these changes in the context of a case study involving truck drivers and cab geometry. The truck driver populations are used to illustrate changes in body size and shape over a 30-year period and show how they affect user acceptability of designs. Changes in the gender split of the driver population are also considered, and are shown to have a significant effect on accommodation. The work demonstrates that secular trends and demographic changes over time significantly affect accommodation, but a well designed product will be more robust to these changes.",2010
1593,Augmented Reality Visualization of Automobile Air Conditioning Using Optical Tracking Tools,"Virtual prototyping allows us to reduce the expensive production of real prototypes to a minimum and shorten vehicle development phases. Augmented Reality (AR) visualization is a demonstrative and intuitive tool in order to overlay physical prototypes with virtual content and thus comprehend complex relationships quickly. Further, AR technologies can intensify the collaboration between specialists with different expert knowledge and support common decision making during reviews meetings. However, the existing work processes, software tools and predefined regulations do not permit the use of AR tools in all automotive areas. In this work, a prototypical AR application, based on optical tracking tools, was set up in a real automotive environment and evaluated in terms of its applicability for CFD simulation data in the passenger compartment. The examination provides valuable information about environmental conditions, requirements from end users as well as the integration in existing work processes. The results are a basis for future improvements in order to offer a seamless and automated workflow in an early state of the development process and for maintenance.",2010
1594,Reverse Engineering for Spotting of Sheet Metal Forming Parts,"The paper elucidates how to connect forming process simulation with innovative measurement- and analysis equipment thereby taking into account the machine influences. Reverse Engineering use 3D-Scanning data of sheet metal forming dies. Following this paradigm, the models simulation relies on are refined, and spotting of forming dies is subjected to a scientific analysis. That means, that with Reverse Engineering, “extended process engineering” is verified at the real spotting procedure, the comparison of simulation- and measuring results is used to evaluate how close the investigated models are to reality, extending the optimisation algorithms used for springback compensation to die spotting, the modification of the die topology will be carried out automatically thanks to new software functions.",2010
1595,2-D Path Planning for Direct Laser Deposition Process,"The zigzag and offset path have been the two most popular path patterns for tool movement in machining process. Different from the traditional machining processes, the quality of parts produced by the metal deposition process is much more dependent upon the choice of deposition paths. Due to the nature of the metal deposition processes, various tool path patterns not only change the efficiency but also affect the deposition height, a critical quality for metal deposition process. This paper presents the research conducted on calculating zigzag pattern to improve efficiency by minimizing the idle path. The deposition height is highly dependent on the laser scanning speed. The paper also discussed the deposition offset pattern calculation to reduce the height variation by adjusting the tool-path to achieve a constant scanning speed. The results show the improvement on both efficiency and height.",2010
1596,A Multi-Axis Slicing Method for Direct Laser Deposition Process,"With multi-axis capability, direct laser deposition process can produce a metal part without the usage of support structures. In order to fully utilize such a capability, the paper discusses a slicing method for multi-axis metal deposition process. Using the geometry information of adjacent layers, the slicing direction and layer thickness can be changed as needed. A hierarchy structure is designed to manage the topological information which is used to determine the slicing sequence. Its usage is studied to build overhang type structure. With such a character, some overhang features such as holes, can be deposited directly to save the required machining operation and material cost, which improves the efficiency of the metal deposition process. Combined with direct 3D layer deposition technique, the multi-axis slicing method is implemented.",2010
1597,Basic Study of Autologous-Bone-Replaceable Artificial Bone Fabrication With Porosity Distribution Using Electrolysis,"Recently, the use of bioresorbable materials (e.g., β-tricalcium phosphate (β-TCP)) has enabled the development of autologous-bone-replaceable artificial bones that are degraded and resorbed, i.e., replaced with autologous bone, when placed inside the human body for a sufficiently long duration. Although such autologous-bone replaceability requires high porosity of the artificial bone to promote the ingression of blood vessels and cells, the high porosity reduces the mechanical strength, which leads to disadvantages such as possible fracture after bone substitution surgery. One solution to this problem is to optimally arrange low-porosity portions for mechanical strength and high-porosity portions for autologous-bone replaceability in solid artificial bones. Commercially available artificial bones typically have fixed shapes such as a rectangular parallelepiped or cylinder. The use of recent solid freeform fabrication technologies, however, has enabled solid artificial bones with various shapes to be customized for individual medical cases. In this paper, the authors propose a solid freeform fabrication method for autologous-bone-replaceable artificial bones with a porosity distribution. A β-TCP porous artificial bone can be fabricated by placing a slurry consisting of β-TCP powder, water, a peptization reagent and a frother in a mold, drying it to form a solid shape and then sintering it. This β-TCP slurry contains ammonium polyacrylate as the peptization reagent, which is an electrolyte, and ammonia, hydrogen and oxygen gases are produced from its electrolysis. The authors conceived the idea of controlling the foaming of the β-TCP slurry by electrolysis, and of designing and implementing a fabrication system consisting of a fine nozzle with a microscrew for extruding β-TCP slurry as a filament and electrodes for controlling the electrolysis of the slurry. Using this system, we can fabricate a solid shape by drawing two-dimensional sections with the slurry filament and stacking each section, and at the same time vary the porosity by controlling the electric current applied for the electrolysis of the slurry. Using the experimental system, three β-TCP porous samples (approximately 18mm × 18mm × 9mm) of high (71.8%), medium (59.5%) and low (54.6%) porosity are successfully fabricated by applying electric currents of 20mA, 10mA and 0mA, respectively. Then a β-TCP porous sample (approximately 40mm × 10mm × 10mm) with a gradient porosity distribution (from 72.3% to 56.1%) is successfully fabricated by varying the electric current from 0mA to 20mA in a continuous fabrication process. From these results, the authors confirm the efficacy and potential of the proposed approach.",2010
1598,Additive Manufacturing Based on Multiple Calibrated Projectors and Its Mask Image Planning,Additive manufacturing (AM) processes based on mask image projection such as ,2010
1599,The Automated Planning for Fixture Location Based on Process Requirement,"This research carries out a systematic investigation on the automation of fixture location planning. A novel classification and representation method of constrained degrees of freedom (DOF) is proposed. Constrained DOFs are classified into four types: linear translation DOF, linear rotation DOF, planar translation DOF and planar rotation DOF, and they are represented in the form of vector. On the basis of the classification and representation, constrained DOFs are automatically obtained from process requirement. By analyzing datum constraint DOF capability and datum location ability, the operation rules of constrained DOF: union, decomposition and coordinate transformation, are established. The approach to the evaluation of datum location ability and the judgment of datum conflict is explored, and the solution of datum conflict and datum location problem is presented. Eventually, the rules of location point layout are formalized.",2010
1600,Robustness Analysis of Airfoil Performance,"We demonstrate a technique to evaluate the aerodynamic robustness of a given blade profile which it is exposed to stochastic geometrical variation. The technique is based on random fields, with geometrical deviations continuously defined over the entire structure, with a prescribed statistical distribution function and a given correlation between these deviations. Control points are defined on the blade surface to model the blade geometry disturbances. At each control point a stochastic deviation is defined, which acts in the normal direction of the blade. By modeling disturbances in the normal direction instead of in the separate Cartesian directions, we automatically reduce the number of stochastic variables by a factor two. The perturbation variables are transformed via Karhunen-Loève eigenvalue decomposition, giving stochastically independent variables. The robustness is finally estimated by a Monte Carlo simulation, where computational fluid dynamic simulations are performed to evaluate the resulting change in blade performance for given geometrical perturbations.",2010
1601,Symmetry Plane Detection for 3D CAD Volumes,"Symmetry properties of components have many applications during a product development process, including shape transformations for modification purposes, Finite Element Analysis (FEA), model retrieval, etc. This paper presents an algorithm to generate 3D model symmetry planes using the B-Rep model of CAD volumes. In the framework of CAD software, 3D models are described as B-Rep volume models. Design processes of volume models strongly rely on extrusion and revolution primitives from sketches containing essentially straight line segments and circular arcs. Hence, the boundary surfaces considered are planes, cylinders, cones, tori and spheres. The object boundary is effectively processed as an infinite set of points. Global symmetry properties of faces are derived to initiate the global symmetry planes of the object. To this end, the intersection curves between two adjacent faces are used to characterize possible global symmetry planes of the object. Then, the algorithm starts analyzing the symmetry properties of couple of faces. Subsequently, the candidate symmetry planes set up contains all the possible global symmetry planes. Finally, the symmetry properties of neighboring faces help determining robustly the global symmetry planes, whether there is a finite number or an infinite number.",2010
1602,Collision-Free Locating of Mobile Cranes in 3D Lifting System,"Locating cranes is critical toward safely lifting in construction industry. In this paper, based on overlapping work envelopes, we present optimized solutions for collision-free locating of mobile cranes. Our solution first determines the feasible location areas of the cranes, which are discretized with regular grids. We then incorporate a collision detection method, through which the locations with potential collision are eliminated. With the objective of minimizing the weight sum about safety, a smart search is further performed to identify the most suitable locations. Our solution has been implemented in the intelligent computation module for 3D lifting system developed by our group. Its feasibility and effectiveness has also been demonstrated through a concrete case study.",2010
1603,A Fast Grid Deformation Algorithm for Aerodynamic Shape Optimization,"A mesh movement algorithm suitable for aerodynamic design optimization problems is presented. It involves B-spline surface construction, projection and evaluation on B-spline faces for the surface mesh movement, as well as inverse-distance and 2D/3D TFI interpolations for the volume mesh deformation. The algorithm is fast and exhibits an excellent parallel efficiency. It is used to deform the surface and volume mesh of an ONERA-M6 wing undergoing several planform changes. The quality of the deformed mesh is preserved as long as the difference between the initial surface mesh and the B-spline surface model is small. A good agreement reported between the flow simulation results on the deformed mesh and those obtained on initial fixed mesh.",2010
1604,Fundamental Concepts for Collaboratively Obtaining Optimum Product Designs,"Fundamental concepts for obtaining optimum product designs from higher view points are presented. These fundamental concepts are: (1) concepts that aim to achieve designs that are in harmony with natural and human environments; (2) collaborations based on mutuality, where the collaborating individuals are free from inhibiting hierarchies; and (3) methodologies that enable discovery of effective solutions by examining the deepest, most fundamental levels of design problems. Product designs that minimize stress on natural environments and maximize benefit to people are increasingly important, given limited natural resources and an increasing world population. The achievement of such product designs generally requires collaborative scenarios based on mutuality and equality of the participants, and the implementation of characteristics-based hierarchical optimization methodologies. Practical methodologies based on these fundamental concepts are discussed here and examples are provided.",2010
1605,Investigation of Design Tools as Complexity Management Techniques,"Design tools which appear to manage complexity through their inherent behavior do not appear to have been developed specifically for complexity management. This research explores how complexity is managed within the design process through: the generation of complexity within the design process (sources), the techniques which were used to manage complexity (approaches), and the examination of design tools with respect to complexity. Mappings are developed between the sources, the approaches, and the tools with respect to phases of design. The mappings are propagated through these distinct, yet adjacent domains in order to study how the tools might be able to be used to manage complexity sources found in different stages of the design process. As expected, the highest value for each design tool is found in the stage of design in which the tool is traditionally been used. However, there are secondary ratings which suggest that design tools can be used in other stages of the design process to manage specific aspects of complexity.",2010
1606,Bayesian Network Classifiers for Set-Based Collaborative Design,"Complex design problems are typically decomposed into smaller design problems that are solved by domain-specific experts who must then coordinate their solutions into a satisfactory system-wide solution. In set-based collaborative design, collaborating engineers coordinate themselves by communicating multiple design alternatives at each step of the design process. The goal in set-based collaborative design is to spend additional resources exploring multiple options in the early stages of the design process, in exchange for less iteration in the latter stages, when iterative rework tends to be most expensive. Several methods have been proposed for representing sets of designs, including intervals, surrogate models, fuzzy membership functions, and probability distributions. In this paper, we introduce the use of Bayesian networks for capturing sets of promising designs, thereby classifying the design space into satisfactory and unsatisfactory regions. The method is compared to intervals in terms of its capacity to accurately classify satisfactory design regions as a function of the number of available data points. A simplified, multilevel design problem for an unmanned aerial vehicle is presented as the motivating example.",2010
1607,Functional Decomposition of the Clustering Approach for Matrix-Based Structuring,"In engineering design, matrices have been commonly used to capture dependency relationships for structure-related problems (e.g., product architecture, process workflow, and team organization). In this context, structuring is considered a group formation process that clusters the design entities and identifies the interactions among the formed groups. To support matrix-based design structuring, this paper proposes a clustering approach that has three phases in the working procedure. Firstly, the coupling analysis is used to assess the coupling strength of any two entities according to the application context. Secondly, the sorting analysis is used to organize the matrix’s rows and columns by bringing the highly coupled entities close to each other, thus yielding a sorted matrix. Thirdly, the partitioning analysis is applied to form a structured matrix that identifies the groups of entities and their interactions based on some structural criteria (e.g., number of groups, limits on group sizes, etc). The proposed clustering method has been applied to four engineering examples to demonstrate its flexibility and adaptability in tackling different design structuring problems.",2010
1608,Examining Interactions Between Solution Architecture and Designer Mistakes,"When designing complex systems, it is often the case that a design process is subjected to a variety of unexpected inputs, interruptions, and changes. These disturbances can create unintended consequences including changes to the design process architecture, the planned design responsibilities, or the design objectives and requirements. In this paper a specific type of design disturbance, mistakes, is investigated. The impact of mistakes on the convergence time of a distributed multi-subsystem optimization problem is studied for several solution process architectures. A five subsystem case study is used to help understand the ability of certain architectures to handle the impact of the mistakes. These observations have led to the hypothesis that selecting distributed design architectures that minimize the number of iterations to propagate mistakes can significantly reduce their impact. It is also observed that design architectures that converge quickly tend to have these same error damping properties. Considering these observations when selecting distributed design architectures can passively reduce the impact of mistakes.",2010
1609,Support Vector Regression Modeling for Design and Analysis of Mechanical Snubbing,"This paper discusses the application of Support Vector Regression (SVR) for modeling the non-linear and hysteretic behavior exhibited by mechanical snubbing systems. Though the discussion in this paper is limited to the application of SVR to snubbing in elastomeric isolators, the approach is generic and can be applied to other dynamic systems or to systems exhibiting hysteretic behavior. A theoretical model that represents the coupled dynamics of an isolation system with the corresponding snubbing system for a single degree-of-freedom system is proposed. The theoretical model is experimentally validated and is subsequently used to build a metamodel using SVR. The results of the metamodel are compared to the theoretical model for a simulation example and are found to be comparable, thereby reducing the computational time for the design and analysis of the snubbing system by orders of magnitude. The SVR based metamodel can, therefore, be used to substitute the computationally intense theoretical model for performing design iterations and design optimization of the snubbing system, significantly reducing model complexity as well as computational time during the design cycle.",2010
1610,A Simulation-Based Robust Concept Exploration Method,"In early stages of the engineering design process it is necessary to explore the design space to find a feasible range or point that satisfies the design requirements. When robustness of the system is among the requirements, the Robust Concept Exploration Method (RCEM) can be used. In RCEM a metamodel such as a global response surface of the entire design space is used. Based on this surrogate model the robustness of the system is evaluated. In nonlinear or multimodal design spaces a very detailed metamodel such as a very high order response surface might be required to reflect accurately the characteristics of the model. For large design spaces this is computationally very expensive. In this paper, using the Probabilistic Collocation Method (PCM) for generating local response models at the points of interest, a Simulation-Based RCEM is proposed as a very efficient and flexible robust concept exploration method. We believe that using the PCM with other design exploration methods would be equally effective.",2010
1611,Making the Most Out of Surrogate Models: Tricks of the Trade,"Design analysis and optimization based on high-fidelity computer experiments is commonly expensive. Surrogate modeling is often the tool of choice for reducing the computational burden. However, even after years of intensive research, surrogate modeling still involves a struggle to achieve maximum accuracy within limited resources. This work summarizes advanced and yet simple statistical tools that help. We focus on four techniques with increasing popularity in the design automation community: (i) screening and variable reduction in both the input and the output spaces, (ii) simultaneous use of multiple surrogates, (iii) sequential sampling and optimization, and (iv) conservative estimators.",2010
1612,Turning Black-Box Into White Functions,"Modeling of h igh dimensional e xpensive b lack-box (HEB) functions is challenging. A recently developed method, radial basis function-based high dimensional model representation (RBF-HDMR), has been found promising. This work extends RBF-HDMR to enhance its modeling capability beyond the current second order form and “uncover” black-box functions so that not only a more accurate metamodel is obtained, but also key information of the function can be gained and thus the black-box function can be turned “white.” The key information that can be gained includes 1) functional form, 2) (non)linearity with respect to each variable, 3) variable correlations. The resultant model can be used for applications such as sensitivity analysis, visualization, and optimization. The RBF-HDMR exploration is based on identifying the existence of certain variable correlations through derived theorems. The adaptive process of exploration and modeling reveals the black-box functions till all significant variable correlations are found. The black-box functional form is then represented by a structure matrix that can manifest all orders of correlated behavior of variables. The proposed approach is tested with theoretical and practical examples. The test result demonstrates the effectiveness and efficiency of the proposed approach.",2010
1613,"Improving Multi-Response Metamodels With Upper/Lower Bound Information Using Multi-Stage, Non-Stationary Covariance Functions","Metamodels have been proposed in the literature to reduce the time and resources devoted to design space exploration, to learn about design trade-offs, and to find the best solution to the design problem in the context of simulation-based design and optimization. In previous work in engineering design based on multiple performance criteria, we have proposed the use of Multi-response Bayesian Surrogate Models (MR-BSM) to model several response variables simultaneously, instead of modeling them independently. By doing so, it is expected that the correlation among the response variables can be used to achieve better models with smaller data sets. In this work, we extend the capabilities of MR-BSM by developing a multistage formulation with non-stationary covariance functions. This formulation for multi-response metamodeling in successive stages of experimental design, data acquisition and model fitting, enables the integration of different sources of information about system responses, with different levels of accuracy, into a single, global model of the system. The feasibility of the proposed formulation is demonstrated with an example in which two test functions are jointly approximated in two stages. In addition, we demonstrate the potential of the methodology to take advantage of a priori information, expressed as upper and lower bounds on the responses, to improve the accuracy of the metamodels. Results show that the use of bound information can result in order-of-magnitude improvements in metamodel accuracy.",2010
1614,A MINLP Model for Global Optimization of Plug-In Hybrid Vehicle Design and Allocation to Minimize Life Cycle Greenhouse Gas Emissions,"Plug-in hybrid electric vehicles (PHEVs) have potential to reduce greenhouse gas (GHG) emissions in the U.S. light-duty vehicle fleet. GHG emissions from PHEVs and other vehicles depend on both vehicle design and driver behavior. We pose a twice-differentiable, factorable mixed-integer nonlinear programming model utilizing vehicle physics simulation, battery degradation data, and U.S. driving data to determine optimal vehicle design and allocation for minimizing lifecycle greenhouse gas (GHG) emissions. The resulting nonconvex optimization problem is solved using a convexification-based branch-and-reduce algorithm, which achieves global solutions. In contrast, a randomized multistart approach with local search algorithms finds global solutions in 59% of trials for the two-vehicle case and 18% of trials for the three-vehicle case. Results indicate that minimum GHG emissions is achieved with a mix of PHEVs sized for around 35 miles of electric travel. Larger battery packs allow longer travel on electric power, but additional battery production and weight result in higher GHG emissions, unless significant grid decarbonization is achieved. PHEVs offer a nearly 50% reduction in life cycle GHG emissions relative to equivalent conventional vehicles and about 5% improvement over ordinary hybrid electric vehicles. Optimal allocation of different vehicles to different drivers turns out to be of second order importance for minimizing net life cycle GHGs.",2010
1615,Hybrid Power/Energy Generation System Design Through Multistage Design Optimization Problem With Complementarity Constraints,"The optimal design of hybrid power generation systems (HPGS) can significantly improve the economical and technical performance of power supply. However, the discrete-time simulation with logical disjunctions involved in HPGS design usually leads to a nonsmooth optimization model, to which well established techniques for smooth nonlinear optimization could not be directly applied. This paper proposes a multistage design optimization problem with complementarity constraints approach for HPGS design, which introduces a complementarity formulation of the nonsmooth logical disjunction, as well as a multistage decomposition framework, to ensure a fast local solution. A numerical study of a stand-alone hybrid photovoltaic (PV)/wind power generation system is presented to demonstrate the effectiveness of the proposed approach.",2010
1616,Design and Analysis of Hybrid Guideway Heating System for Morgantown Personal Rapid Transit,"The Morgantown Personal Rapid Transit (M-PRT) system is a comfortable conveyance for travel in Morgantown, WV. One of its operating concerns is the increasing cost of heat to the guideway during winter. As the vehicles cannot run safely during snow, the system includes a guideway heating system to melt the ice from the guideway. To reduce the use of expensive natural gas, an interest has been expressed to define a hybrid heating system using an alternate fuel supply. Solid Oxide Fuel Cell (SOFC) was incorporated in the hybrid heating system. This hybrid heating system was designed, and then a detailed analysis was performed to ascertain the performance parameters like heat produced, thermal efficiency, cost of the system and the emissions involved. This high temperature fuel cell releases large amounts of usable heat in the form of exhaust gases. The exhaust gases are deprived of any undesired emissions that pollute the atmosphere. A USDOE EPSCoR WV State Implementation Award conducted by Advance Power Electricity Research Center (APERC) at West Virginia University provided support for conducting this research.",2010
1617,Design Optimization of Reverse Osmosis Water Desalination Systems via Genetic Algorithms,"This paper explores the application of genetic algorithms (GA) for optimal design of reverse osmosis (RO) water desalination systems. While RO desalination is among the most cost and energy efficient methods for water desalination, optimal design of such systems is rarely an easy task. In these systems, salty water is made to flow at high pressure through vessels that contain semi-permeable membrane modules. The membranes can allow water to flow through, but prohibit the passage of salt ions. When the pressure is sufficiently high, water molecules will flow through the membranes leaving the salt ions behind and are collected in a fresh water stream. Typical system design variables for RO systems include the number and layout of the vessels and membrane modules, as well as the operating pressure and flow rate. This paper explores models for single and two-stage RO pressure vessel configurations. The number and layout of the vessels and membrane modules are regarded as discrete variables, while the operating pressures and flow rate are regarded as continuous variables. GA is applied to optimize the models for minimum overall cost of unit produced fresh water. Case studies are considered for four different water salinity concentration levels. In each of the studies, three different types of crossover are explored in the GA. While all the studied crossover types yielded satisfactory results, the crossover types that attempt to exploit design variable continuity performed slightly better, even for the discrete variables of this problem.",2010
1618,Studies on the Design of Reverse Osmosis Water Desalination Systems for Cost and Energy Efficiency,"This paper explores optimal design of reverse osmosis (RO) systems for water desalination. In these systems, salty water flows at high pressure through vessels containing semi-permeable membrane modules. The membranes can allow water to flow through, but prohibit the passage of salt ions. When the pressure is sufficiently high, water molecules will flow through the membranes leaving the salt ions behind, and are collected in a fresh water stream. Typical system design variables include the number and layout of the vessels and membrane modules, as well as the operating pressure and flow rate. This paper presents models for single and two-stage pressure vessel configurations. The models are used to explore the various design scenarios in order to minimize the cost and energy required per unit volume of produced fresh water. Multi-objective genetic algorithm (GA) is used to generate the Pareto-optimal design scenarios for the systems. Case studies are considered for four different water salinity concentration levels. Results of the studies indicate that even though the energy required to drive the RO system is a major contributor to the cost of fresh water production, there exists a tradeoff between minimum energy and minimum cost. An additional parametric study on the unit cost of energy is performed in order to explore future trends. The parametric study demonstrates how an increase in the unit cost of energy may shift the minimum cost designs to shift to more energy-efficient design scenarios.",2010
1619,An Extended Pattern Search Approach to Wind Farm Layout Optimization,"An extended pattern search approach is presented for optimizing the placement of wind turbines on a wind farm. The algorithm will develop a two-dimensional layout for a given number of turbines, employing an objective function that minimizes costs while maximizing the total power production of the farm. The farm cost is developed using an established simplified model that is a function of the number of turbines. The power development of the farm is estimated using an established simplified wake model, which accounts for the aerodynamic effects of turbine blades on downstream wind speed, to which the power output is directly proportional. The interaction of the turbulent wakes developed by turbines in close proximity largely determines the power capability of the farm. As pattern search algorithms are deterministic, multiple extensions are presented to aid escaping local optima by infusing stochastic characteristics into the algorithm. This stochasticity improves the algorithm’s performance, yielding better results than purely deterministic search methods. Three test cases are presented: a) constant, unidirectional wind, b) constant, multidirectional wind, and c) varying, multidirectional wind. Resulting layouts developed by this extended pattern search algorithm develop more power than previously explored algorithms with the same evaluation models and objective functions. In addition, the algorithm’s layouts motivate a heuristic that yields the best layouts found to date.",2010
1620,Design of an Extended-Range Electric Vehicle for the EcoCAR Challenge,"The EcoCAR Challenge team at The Ohio State University has designed a range-extending electric vehicle capable of 40 miles all-electric range via a 22 kWh lithium-ion battery pack, with range extension and limited parallel operation supplied by a 1.8 L dedicated E85 engine. This vehicle is designed to drastically reduce fuel consumption, with an estimated fuel economy of 89 miles per gallon gasoline equivalent (mpgge), while meeting Tier II Bin 5 emissions standards. This paper documents the team’s control system development effort, starting with the vehicle architecture selection and specifying the powertrain configuration, explaining a detailed control system development process, summarizing the selected control hardware architecture at vehicle and component level, describing supervisory control algorithm design and implementation for fuel economy optimization and performance improvement, and concluding with the use of MIL and HIL techniques for system development and validation.",2010
1621,Plug-In Hybrid Electric Vehicle Battery Selection for Optimum Economic and Environmental Benefits Using Pareto Set Points and PSAT™,"Plug-in hybrid electric vehicles (PHEVs) have the potential to reduce green house gases emissions and provide a promising alternative to conventional internal combustion engine vehicles. However, PHEVs have not been widely adopted in comparison to the conventional vehicles due to their high costs and short charging intervals. Since PHEVs rely on large storage batteries relative to the conventional vehicles, the characteristics and design issues associated with PHEV batteries play an important role in the potential adoption of PHEVs. Consumer acceptance and adoption of PHEVs mainly depends on fuel economy, operating cost, operation green house gas (GHG) emissions, power and performance, and safety among other characteristics. We compare the operational performance of PHEV20 (PHEV version sized for 20 miles of all electric range) based on fuel economy, operating cost, and greenhouse gas (GHG) emissions through Pareto set point identification approach for 15 different types of batteries, including lithium-ion, nickel metal hydride (NiMH), nickel zinc (NiZn), and lead acid batteries. It is found that two from 15 batteries dominate the rest. Among the two, a NiMH (type ess_nimh_90_72_ovonic) gives the highest fuel economy, and a lithium-ion (type ess_li_7_303) yields the lowest operating cost and GHG emissions. From comparing nine batteries that are either on or close to the Pareto frontier, one can see that lithium-ion and NiMH batteries offer better fuel economy than lead-acid batteries. Though lithium-ion batteries bear clear advantage on operating costs and GHG emissions, NiMH and lead-acid batteries show similar performances from these two aspects.",2010
1622,"An Investigation of Sustainability, Preference, and Profitability in Design Optimization","Customer preferences for sustainable products are dependent upon the context in which the customer makes a purchase decision. This paper investigates a case study in which fifty-five percent of survey customers say they prefer recycled paper towels, but do not purchase them. These customers represent a profit opportunity for a firm. This paper explores the impact of investing capital in activating pro-environmental preferences on a firm’s profitability and greenhouse gas (GHG) emissions through a multi-objective optimization study. A product optimization is designed to include models of carbon dioxide emissions, manufacturing costs, customer preference, and technical performance. Because the optimization includes a tradeoff between recycled paper and performance, a model of customer preferences, and a market of competing products, the maximum GHG reduction occurs at less than 100% recycled paper. Also, the tradeoff between GHG reductions and profit is not dictated by the configuration of the product, but instead by its price. These results demonstrate the importance of including customer preferences with engineering performance in design optimization. Investment in the activation of pro-environmental preferences is high at all points on the Pareto optimal frontier, suggesting that further engineering design research into the activation of pro-environmental product preferences is warranted.",2010
1623,Response Surface Based Cost Model for Onshore Wind Farms Using Extended Radial Basis Functions,"This paper develops a cost model for onshore wind farms in the U.S.. This model is then used to analyze the influence of different designs and economic parameters on the cost of a wind farm. A response surface based cost model is developed using Extended Radial Basis Functions (E-RBF). The E-RBF approach, a combination of radial and non-radial basis functions, can provide the designer with significant flexibility and freedom in the metamodeling process. The E-RBF based cost model is composed of three parts that can estimate (i) the installation cost, (ii) the annual Operation and Maintenance (O&M) cost, and (iii) the total annual cost of a wind farm. The input parameters for the E-RBF based cost model include the rotor diameter of a wind turbine, the number of wind turbines in a wind farm, the construction labor cost, the management labor cost and the technician labor cost. The accuracy of the model is favorably explored through comparison with pertinent real world data. It is found that the cost of a wind farm is appreciably sensitive to the rotor diameter and the number of wind turbines for a given desirable total power output.",2010
1624,Optimal Camera Path Planning for the Inspection of Printed Circuit Boards Using a Two Stepped Optimization Approach,"Automated Optical Inspection (AOI) systems are rapidly replacing slow and tedious manual inspections of Printed Circuit Boards (PCBs). In an AOI system, a minicamera traverses the PCB in a pre-defined travel path, snapping shots of all the PCB components or nodes, at pre-defined locations. The images are then processed and information about the different nodes is extracted and compared against ideal standards stored in the AOI system. This way, a flawed board is detected. Minimizing both the number of images required to scan all the PCB nodes, and the path through which the camera must travel to achieve this, will minimize the image acquisition time and the traveling time, and thus the overall time of inspection. This consequently both reduces costs and increases production rate. This work breaks down this problem into two sub-problems: The first is a clustering problem; the second a travelling salesman sequencing problem. In the clustering problem, it is required to divide all the nodes of a PCB into the minimum number of clusters. The cluster size is constrained by the given dimensions of the camera’s scope or Field of Vision (FOV). These dimensions determine the dimension of the inspection windows. It is thus required to find the minimum number of inspection windows that will scan all the nodes of a PCB, and their locations. Genetic algorithms are applied in a two-step approach with special operators suited for the problem. A continuous Genetic Algorithm (GA) is applied to find the optimum inspection window locations that cover one node and as many other nodes as possible. A discrete GA is then applied to eliminate redundant inspection windows leaving the minimum number of windows that cover all nodes throughout the PCB. In the second sub-problem, an Ant Colony Optimization (ACO) method is used to find the optimum path between the selected inspection windows. The method proposed in this paper is compared against relevant published work, and it is shown to yield better results.",2010
1625,Constraint Management of Reduced Representation Variables in Decomposition-Based Design Optimization,"In decomposition-based design optimization strategies, such as Analytical Target Cascading (ATC), it is sometimes necessary to use reduced dimensionality representations to approximate functions of large dimensionality whose values need to be exchanged among subproblems. The reduced representation variables may not be physically meaningful, and it can become challenging to constrain them properly and define the model validity region. For example, in coordination strategies like ATC, representing vector-valued coupling variables with improperly constrained reduced representation variables can lead to poor performance or convergence failure. This paper examines two approaches for constraining effectively the model validity region of reduced representation variables based on proper orthogonal decomposition: a penalty value-based heuristic and a support vector domain description. An ATC application on electric vehicle design helps to illustrate the concepts discussed.",2010
1626,Combined Plant and Controller Design Using Decomposition-Based Design Optimization and the Minimum Principle,"An often cited motivation for using decomposition-based optimization methods to solve engineering system design problems is the ability to apply discipline-specific optimization techniques. For example, structural optimization methods have been employed within a more general system design optimization framework. We propose an extension of this principle to a new domain: control design. The simultaneous design of a physical system and its controller is addressed here using a decomposition-based approach. An optimization subproblem is defined for both the physical system (i.e., plant) design and the control system design. The plant subproblem is solved using a general optimization algorithm, while the controls subproblem is solved using a new approach based on optimal control theory. The optimal control solution, which is derived using the the Minimum Principle of Pontryagin (PMP), accounts for coupling between plant and controller design by managing additional variables and penalty terms required for system coordination. Augmented Lagrangian Coordination is used to solve the system design problem, and is demonstrated using a circuit design problem.",2010
1627,Optimizing the Unrestricted Placement of Turbines of Differing Rotor Diameters in a Wind Farm for Maximum Power Generation,"This paper presents a new method (the Unrestricted Wind Farm Layout Optimization (UWFLO)) of arranging turbines in a wind farm to achieve maximum farm efficiency. The powers generated by individual turbines in a wind farm are dependent on each other, due to velocity deficits created by the wake effect. A standard analytical wake model has been used to account for the mutual influences of the turbines in a wind farm. A variable induction factor, dependent on the approaching wind velocity, estimates the velocity deficit across each turbine. Optimization is performed using a constrained Particle Swarm Optimization (PSO) algorithm. The model is validated against experimental data from a wind tunnel experiment on a scaled down wind farm. Reasonable agreement between the model and experimental results is obtained. A preliminary wind farm cost analysis is also performed to explore the effect of using turbines with different rotor diameters on the total power generation. The use of differing rotor diameters is observed to play an important role in improving the overall efficiency of a wind farm.",2010
1628,A Cutting Plane Method for Analytical Target Cascading With Augmented Lagrangian Coordination,"Various decomposition and coordination methodologies for solving large-scale system design problems have been developed and studied during the past few decades. However, there is generally no guarantee that they will converge to the expected optimum design under general assumptions. Those with proven convergence often have restricted hypotheses or a prohibitive cost related to the required computational effort. Therefore there is still a need for improved, mathematically grounded, decomposition and coordination techniques that will achieve convergence while remaining robust, flexible and easy to implement. In recent years, classical Lagrangian and augmented Lagrangian methods have received renewed interest when applied to decomposed design problems. Some methods are implemented using a subgradient optimization algorithm whose performance is highly dependent on the type of dual update of the iterative process. This paper reports on the implementation of a cutting plane approach in conjunction with Lagrangian coordination and the comparison of its performance with other subgradient update methods. The method is demonstrated on design problems that are decomposable according to the analytic target cascading (ATC) scheme.",2010
1629,Reducible Uncertain Interval Design (RUID) by Kriging Meta-Model Assisted Multi-Objective Optimization,"Optimization under uncertainty can be a difficult and computationally expensive problem driven by the need to consider the degrading effects of system variations. Sources of uncertainty that may be reducible in some fashion present a particular challenge because designers must determine how much uncertainty to accept in the final design. Many of the existing approaches for design under input uncertainty require potentially unavailable or unknown information about the uncertainty in a system’s input parameters; such as probability distributions, nominal values or uncertain intervals. These requirements may force designers into arbitrary or even erroneous assumptions about a system’s input uncertainty when attempting to estimate nominal values and/or uncertain intervals for example. These types of assumptions can be especially degrading during the early stages in a design process when limited system information is available. In an effort to address these challenges a new design approach is presented that can produce optimal solutions in the form of upper and lower bounds (which specify uncertain intervals) for all input parameters to a system that possess reducible uncertainty. These solutions provide minimal variation in system objectives for a maximum allowed level of input uncertainty in a multi-objective sense and furthermore guarantee as close to deterministic Pareto optimal performance as possible with respect to the uncertain parameters. The function calls required by this approach are dramatically reduced through the use of a kriging meta-model assisted multi-objective optimization technique performed in two stages. The capabilities of the approach are demonstrated through three example problems of varying complexity.",2010
1630,Multi-Objective Optimization in Industrial Robotic Cell Design,"It has become a common practice to conduct simulation-based design of industrial robotic cells, where Mechatronic system model of an industrial robot is used to accurately predict robot performance characteristics like cycle time, critical component lifetime, and energy efficiency. However, current robot programming systems do not usually provide functionality for finding the optimal design of robotic cells. Robot cell designers therefore still face significant challenge to manually search in design space for achieving optimal robot cell design in consideration of productivity measured by the cycle time, lifetime, and energy efficiency. In addition, robot cell designers experience even more challenge to consider the trade-offs between cycle time and lifetime as well as cycle time and energy efficiency. In this work, utilization of multi-objective optimization to optimal design of the work cell of an industrial robot is investigated. Solution space and Pareto front are obtained and used to demonstrate the trade-offs between cycle-time and critical component lifetime as well as cycle-time and energy efficiency of an industrial robot. Two types of multi-objective optimization have been investigated and benchmarked using optimal design problem of robotic work cells: 1) single-objective optimization constructed using Weighted Compromise Programming (WCP) of multiple objectives and 2) Pareto front optimization using multi-objective generic algorithm (MOGA-II). Of the industrial robotics significance, a combined design optimization problem is investigated, where design space consisting of design variables defining robot task placement and robot drive-train are simultaneously searched. Optimization efficiency and interesting trade-offs have been explored and successful results demonstrated.",2010
1631,An Improved Kriging Assisted Multi-Objective Genetic Algorithm,"Although Genetic Algorithms (GAs) and Multi-Objective Genetic Algorithms (MOGAs) have been widely used in engineering design optimization, the important challenge still faced by researchers in using these methods is their high computational cost due to the population-based nature of these methods. For these problems it is important to devise MOGAs that can significantly reduce the number of simulation calls compared to a conventional MOGA. We present an improved kriging assisted MOGA, called Circled Kriging MOGA (CK-MOGA), in which kriging metamodels are embedded within the computation procedure of a traditional MOGA. In the proposed approach, the decision as to whether the original simulation or its kriging metamodel should be used for evaluating an individual is based on a new objective switch criterion and an adaptive metamodeling technique. The effect of the possible estimated error from the metamodel is mitigated by applying the new switch criterion. Three numerical and engineering examples with different degrees of difficulty are used to illustrate applicability of the proposed approach. The results show that, on the average, CK-MOGA outperforms both a conventional MOGA and our developed Kriging MOGA in terms of the number of simulation calls.",2010
1632,Investigating the Significance of “One-to-Many” Mappings in Multiobjective Optimization,"Significant research has focused on multiobjective design optimization and negotiating trade-offs between conflicting objectives. Many times, this research has referred to the possibility of attaining similar performance from multiple, unique design combinations. While such occurrences may allow for greater design freedom, their significance has yet to be quantified for trade-off decisions made in the design space (DS). In this paper, we computationally explore which regions of the performance space (PS) exhibit “one-to-many” mappings back to the DS, and examine the behavior and validity of the corresponding region associated with this mapping. Regions of interest in the PS and DS are identified and generated using indifference thresholds to effectively “discretize” both spaces. The properties analyzed in this work are a mapped region’s location in the PS and DS and the total hypervolume of the mappings. Our proposed approach is demonstrated on two different multiobjective engineering problems. The results indicate that one-to-many mappings occur in engineering design problems, and that while these mappings can result in significant design space freedom, they often result in notable performance sacrifice.",2010
1633,Ball Bearing Fatigue and Wear Life Optimization Using Elastohydrodynamic and Genetic Algorithm,"The present work attempts to improve the performance of rolling element bearings through the increase of fatigue life and the reduction of bearing wear. The formulation is based on Elastohydrodynamic to maximize the realistically evaluated minimum film thickness without significant increase in viscous friction torque. Design vectors are reduced in the present study relative to previous studies as some variables are considered as dependent variables. Other design vectors are not considered as variables in the study due to machining accuracy. This paper presents a more viable method to solve the multi-objective optimization problem using genetic algorithms (GAs) to reduce the chances of getting trapped in local maximum or minimum. Using a utility function, optimal Pareto points are obtained by changing the weight coefficients. Specific weights can be used depending on the designer decision whether to increase fatigue and wear life, or decrease the power consumption.",2010
1634,Development of a Product Family Analysis Toolkit for Systematic Benchmarking,"As global markets saturate and competition intensifies, many manufacturers are focusing on benchmarking families of products alongside individual products to gain valuable insight and strategic advantage over their competitors. Unfortunately, the advantages of benchmarking families of products are often undermined by the limited capability of current benchmarking tools to assist in this process. While methods have been proposed for product family analysis and benchmarking, a major problem is the way in which the component details are collected, and few of the methods have been integrated together. Benchmarking and product family analysis is also time-consuming and subject to human variability since the process is typically done manually without the aid of software. To address these problems, we introduce the Product Family Analysis Toolkit (PFA Toolkit), which combines several popular benchmarking tools to streamline and standardize the process of product family benchmarking. We describe the toolkit’s features and capabilities and then discuss its functionality and usability. The advantages of automating the product family benchmarking process are discussed along with future work.",2010
1635,User Interface Design for Interactive Product Family Analysis and Variants Derivation,"Product family design (PFD) is one of the commonly adopted strategies of product realization in mass customization paradigm. Among the current product family modeling approaches, ontology based modeling has been identified as a promising approach. Previously, we have studied the feasibility of using a semantically annotated multi-facet product family ontology in performing product analysis and variant derivation in the PFD domain. However, the visualization aspects of the ontology are important to assist product designers and engineers to gain insights and benefit from the ever-increasing information from the ontology, e.g. dimension, assembly or configuration wise. From the previous literature, we observe that there are limited usage of visualization and interaction in PFD for tasks such as product analysis and variant derivation. The current hierarchy based representations are limited in displaying ontological relationships and tasks such as commonality analysis seldom make use of visualization to foster better understanding of component similarity. In this study, we report our efforts in assisting product family analysis and variant derivation through visualization and user interface (UI) which enables interactive PFD. Design considerations for our visualization and user interaction design are discussed. By using a multi-touch UI, we discuss on how our UI is able to enable users to better perform product analysis and variant derivation based on the aforementioned ontology in an interactive, intuitive and intelligent manner. We finally conclude this paper with some indications for future works.",2010
1636,Validating the Generational Variety Index (GVI) Through Product Family Optimization: A Preliminary Study,"Effective product platforms must strike an optimal balance between commonality and variety. Increasing commonality can reduce costs by improving economies of scale while increasing variety can improve market performance, or in our robot family example, satisfy various robot missions. Two metrics that have been developed to help resolve this tradeoff are the Generational Variety Index (GVI) and the Product Family Penalty Function (PFPF). GVI provides a metric to measure the amount of product redesign that is required for subsequent product offerings, whereas PFPF measures the dissimilarity or lack of commonality between design (input) parameters during product family optimization. GVI is examined because it is the most widely used metric applicable during conceptual development to determine platform components. PFPF is used to validate GVI because of its ease of implement for parametric variety, as used in this case. This paper describes a product family trade study that has been performed using GVI for a robot product family and compares the results to those obtained by optimizing the same family using PFPF. This work provides a first attempt to validate the output of GVI by using a complementary set of results obtained from optimization. The results of this study indicate that while there are sometimes similarities between the results of GVI and optimization using PFPF, there is not necessarily a direct correlation between these two metrics. Moreover, the platform recommended by GVI is not necessarily the most performance-optimized platform, but it can help improve commonality. In the same regard, PFPF may miss certain opportunities for commonality. The benefits of integrating the two approaches are also discussed.",2010
1637,Universal Product Family Design Valuation in an Uncertain Market Environment,"Strategic adaptability is essential in capitalizing on future investment opportunities and responding properly to market trends in an uncertain environment. Customized products or services are an important source of revenue for many companies, particularly those working with in a mass customization environment where customer satisfaction is of paramount important. In this paper, we extend methods from mass customization and product family design to create specific methods for universal product family design. The objective of this research is to propose a valuation financial model to facilitate universal design strategies that will maximize the expected profit under uncertain constrains. Real options analysis is applied to estimate the valuation of options related to introducing new modules as a platform in a universal product family. We use customers’ preferences based on performance utilities for universal design to reflect demand and demographic trends. To demonstrate implementation of the proposed model, we use a case study involving a family of light-duty trucks. We perform sensitivity analysis to investigate the behavior of the estimated option value against chaining system parameters.",2010
1638,Designing a Product Package Platform,"An essential part of designing a successful product family is establishing a recognizable, familiar, product family identity. It is very often the case that consumers first identify products based on their physical embodiment. The Apple iPod, DeWalt power tools, and KitchenAid appliances are all examples of product families that have successfully branded themselves based on physical principles. While physical branding is often the first trait apparent to designers, there are some products that cannot be differentiated based on physical appearance. This is especially common for consumable products. For example, it is impossible to differentiate between diet Coke, Classic Coke, and Pepsi when each is poured into separate glasses. When differentiation is difficult to achieve from a product’s physical characteristics, the product’s package becomes a vital part of establishing branding and communicating membership to a product family while maintaining individual product identity. In this paper, product packaging is investigated with a focus on the graphic packaging components that identify product families. These components include: color, shape, typography, and imagery. Through the application of tools used in facilities layout planning, graph theory, social network theory, and display design theory an approach to determine an optimal arrangement of graphic components is achieved. This approach is validated using a web based survey that tracks user-package interactions across a range of commonly used cereal boxes.",2010
1639,System RBDO With Correlated Variables Using Probabilistic Re-Analysis and Local Metamodels,"A simulation-based, system reliability-based design optimization (RBDO) method is presented that can handle problems with multiple failure regions and correlated random variables. Copulas are used to represent dependence between random variables. The method uses a Probabilistic Re-Analysis (PRRA) approach in conjunction with a sequential trust-region optimization approach and local metamodels covering each trust region. PRRA calculates very efficiently the system reliability of a design by performing a single Monte Carlo (MC) simulation per trust region. Although PRRA is based on MC simulation, it calculates “smooth” sensitivity derivatives, allowing the use of a gradient-based optimizer. The PRRA method is based on importance sampling. One requirement for providing accurate results is that the support of the sampling PDF must contain the support of the joint PDF of the input random variables. The trust-region optimization approach satisfies this requirement. Local metamodels are constructed sequentially for each trust region taking advantage of the potential overlap of the trust regions. The metamodels are used to determine the value of the indicator function in MC simulation. An example with correlated input random variables demonstrates the accuracy and efficiency of the proposed RBDO method.",2010
1640,A Simulation-Based Random Process Method for Time-Dependent Reliability of Dynamic Systems,"Reliability is an important engineering requirement for consistently delivering acceptable product performance through time. As time progresses, a product may fail due to time-dependent operating conditions and material properties, and component degradation. The reliability degradation with time may significantly increase the lifecycle cost due to potential warranty costs, repairs and loss of market share. In this work, we consider the first-passage reliability, which accounts for the first time failure of non-repairable systems. Methods are available that provide an upper bound to the true reliability, but they may overestimate the true value considerably. This paper proposes a methodology to calculate the cumulative probability of failure (probability of first passage or upcrossing) of a dynamic system with random properties, driven by an ergodic input random process. Time series modeling is used to characterize the input random process based on data from a “short” time period (e.g. seconds) from only one sample function of the random process. Sample functions of the output random process are calculated for the same “short” time because it is usually impractical to perform the calculation for a “long” duration (e.g. hours). The proposed methodology calculates the time-dependent reliability, at a “long” time using an accurate “extrapolation” procedure of the failure rate. A representative example of a quarter car model subjected to a stochastic road excitation demonstrates the improved accuracy of the proposed method compared with available methods.",2010
1641,Multi-Objective Design and Tolerance Allocation for Single- and Multi-Level Systems,In this work we develop a method to perform simultaneous design and tolerance allocation for engineering problems with multiple objectives. Most studies in existing literature focus on either optimal design with constant tolerances or the optimal tolerance allocation for a given design setup. Simultaneously performing both design and tolerance allocation with multiple objectives for hierarchical systems increases problem dimensions and raises additional computational challenges. A design framework is proposed to obtain optimal design alternatives and to rank their performances when variations are present. An optimality influence range is developed to aid design alternatives selections with an influence signal-to-noise ratio that indicates the accordance of objective variations to the Pareto set and an influence area that quantifies the variations of a design. An additional tolerance design scheme is implemented to ensure that design alternatives meet the target tolerance regions. The proposed method is also extended to decomposed multi-level systems by integrating traditional sensitivity analysis for uncertainty propagation with analytical target cascading. This work enables decision-makers to select their best design alternatives on the Pareto set using three measures with different purposes. Examples demonstrate the effectiveness of the method on both single- and multi-level systems.,2010
1642,Modified Reduced Gradient With Realizations Sorting for Hard Equality Constraints in Reliability-Based Design Optimization,"In this work, the presence of equality constraints in reliability-based design optimization (RBDO) problems is studied. Relaxation of soft equality constraints in RBDO and its challenges are briefly discussed while the main focus is on hard equalities that can not be violated even under uncertainty. Direct elimination of hard equalities to reduce problem dimensions is usually suggested; however, for nonlinear or black-box functions, variable elimination requires expensive root-finding processes or inverse functions that are generally unavailable. We extend the reduced gradient methods in deterministic optimization to handle hard equalities in RBDO. The efficiency and accuracy of the first and the second order predictions in reduced gradient methods are compared. Results show the first order prediction being more efficient when realizations of random variables are available. A gradient-weighted sorting with these random samples is proposed to further improve the solution efficiency of the reduced gradient method. Feasible design realizations subject to hard equality constraints are then available to be implemented with the state-of-the-art sampling techniques for RBDO problems. Numerical and engineering examples show the strength and simplicity of the proposed method.",2010
1643,Probabilistic Design Optimization of Frequency Dispersion for Rotating Blades,"In this paper, a probabilistic design optimization method based on finite element method is proposed to calculate the variability of design parameters subject to a specified dispersion of natural frequencies of rotating blades. The element stiffness and mass matrices are derived using a two-stage finite element method and numerical integration. Based on the perturbation technology, the sensitivity of the frequencies, as well as relationship between the frequency dispersion and the coefficient of variability (CV) of the design parameters can be obtained. Such sensitivity information is then used to convert the probabilistic design optimization problem into a deterministic optimization problem. Two case studies are given to illustrate the proposed method. From the results, it is concluded that rotation of blade changes the sensitivity of CV to the design parameters considered, and using the proposed method can transform the probabilistic constraints to deterministic constraints.",2010
1644,Second-Order Reliability Method With First-Order Efficiency,"The widely used First Order Reliability Method (FORM) is efficient, but may not be accurate for nonlinear limit-state functions. The Second Order Reliability Method (SORM) is more accurate but less efficient. To maintain both high accuracy and efficiency, we propose a new second order reliability analysis method with first order efficiency. The method first performs the FORM and identifies the Most Probable Point (MPP). Then the associated limit-state function is decomposed into additive univariate functions at the MPP. Each univariate function is further approximated as a quadratic function, which is created with the gradient information at the MPP and one more point near the MPP. The cumulant generating function of the approximated limit-state function is then available so that saddlepoint approximation can be easily applied for computing the probability of failure. The accuracy of the new method is comparable to that of the SORM, and its efficiency is in the same order of magnitude as the FORM.",2010
1645,A Joint Probability Approach to Multiobjective Optimization Under Uncertainty,"In this paper, we present a new approach to solve optimization problems with multiple objectives under uncertainty. Optimality is considered in terms of the risk that the overall system performance, as defined by all of the multiple objectives exceeding their desired thresholds, remains acceptable. Unlike the existing state-of-the-art, where first-order moments of the system level objectives are used to ensure optimality, we employ a joint probability formulation in our research. The Pareto optimality criterion under uncertainty is defined in terms of joint probability, i.e., probability that all  system objectives are less than the desired thresholds. These thresholds can be viewed as the desired upper/lower bounds on the individual system objectives. The higher the joint probability, the more reliably the thresholds bound the system performance, hence the lower the overall system performance risk. However, a desirable high joint probability may necessitate undesirably high/low thresholds, and hence the tradeoff. In this context, the proposed method provides two decision-making capabilities: (1) Maximum probability design:  given a set of threshold values for system objectives, find the design that yields the maximum joint probability (2) Optimum threshold design:  Given a desired joint probability, find the set of thresholds that yield this probability. In this paper, optimization formulations are presented to solve the above two decision-making problems. A two-bar truss example and the conceptual design of a two-stage-to-orbit launch vehicle are presented to illustrate the proposed methods. The numerical results show that optimizing the mean values of the objectives individually does not necessarily guarantee the desired performance of all objectives jointly under uncertainty, which is of ultimate interest in multiobjective optimization.",2010
1646,Parameter Screening in Dynamic Computer Model Calibration Using Global Sensitivities,"Sensitivity analysis and computer model calibration are generally treated as two separate topics. In sensitivity analysis one quantifies the effect of each input factor on outputs, whereas in calibration one finds the values of input factors that provide the best match to a set of field data. In this paper we show a connection between these two seemingly separate concepts, and illustrate it with an automotive industry application involving a Road Load Acquisition Data (RLDA) computer model. We use global sensitivity analysis for computer models with transient responses to screen out inactive input parameters and make the calibration algorithm numerically more stable. Because the computer model can be computationally intensive, we construct a fast statistical surrogate for the computer model with transient responses. This fast surrogate is used for both sensitivity analysis and RLDA computer model calibration.",2010
1647,Trending Mining for Predictive Product Design,The ,2010
1648,Validating Designs Through Sequential Simulation-Based Optimization,"Computational simulation models support a rapid design process. Given model approximation and operating conditions uncertainty, designers must have confidence that the designs obtained using simulations will perform as expected. This paper presents a methodology for validating designs as they are generated during a simulation-based optimization process. Current practice focuses on validation of simulation models throughout the entire design space. In contrast, the proposed methodology requires validation only at design points generated during optimization. The goal of such validation is confidence in the resulting design rather than in the underlying simulation model. The proposed methodology is illustrated on a simple cantilever beam design subject to vibration.",2010
1649,"Risk Management in Product Design: Current State, Conceptual Model and Future Research","Risk management is an important element of product design. It helps to minimize the project- and product-related risks such as project budget and schedule overrun, or missing product cost and quality targets. Risk management is especially important for complex, international product design projects that involve a high degree of novel technology. This paper reviews the literature on risk management in product design. It examines the newly released international standard ISO 31000 “Risk management — Principles and guidelines” and explores its applicability to product design. The new standard consists of the seven process steps communication and consultation; establishing the context; risk identification; risk analysis; risk evaluation; risk treatment; and monitoring and review. A literature review reveals, among other findings, that the general ISO 31000 process model seems applicable to risk management in product design; the literature addresses different process elements to varying degrees, but none fully according to ISO recommendations; and that the integration of product design risk management with risk management of other disciplines, or between project and portfolio level in product design, is not well developed.",2010
1650,Model-Based Method to Translate System Level Customer Need to Part Specification,"Products are successful because they meet customer needs. However, many customer needs are not expressed in measurable terms. In addition, when such needs are achieved by a complex system made of hardware parts and software, decomposing customer needs to part-level specification is not a trivial task. This paper presents a model-based approach to address such problems. In the case study, the customer need was the noise and vibration level of an unconventional gasoline engine system when running at idle. The hardware component whose performance tolerance needed to be specified was a new type of fuel injectors. These new fuel injectors had higher piece-to-piece performance variations than the conventional fuel injectors. It was unclear whether such variation was acceptable for customer perceived powertrain quality. A virtual powertrain system simulation model was used to analytically evaluate the impact of the fuel injector performance variability. Monte Carlo simulation was carried out to assess the impact of injector variability. The results from the simulation were further refined using engine hardware testing. This study made recommendations for the acceptable level of hardware tolerance, which was different from what the supplier of the injectors had suggested.",2010
1651,Sampling-Based Stochastic Sensitivity Analysis Using Score Functions for RBDO Problems With Correlated Random Variables,"This study presents a methodology for computing stochastic sensitivities with respect to the design variables, which are the mean values of the input correlated random variables. Assuming that an accurate surrogate model is available, the proposed method calculates the component reliability, system reliability, or statistical moments and their sensitivities by applying Monte Carlo simulation (MCS) to the accurate surrogate model. Since the surrogate model is used, the computational cost for the stochastic sensitivity analysis is negligible. The copula is used to model the joint distribution of the correlated input random variables, and the score function is used to derive the stochastic sensitivities of reliability or statistical moments for the correlated random variables. An important merit of the proposed method is that it does not require the gradients of performance functions, which are known to be erroneous when obtained from the surrogate model, or the transformation from X-space to U-space for reliability analysis. Since no transformation is required and the reliability or statistical moment is calculated in X-space, there is no approximation or restriction in calculating the sensitivities of the reliability or statistical moment. Numerical results indicate that the proposed method can estimate the sensitivities of the reliability or statistical moments very accurately, even when the input random variables are correlated.",2010
1652,Reliability-Based Design Optimization With Confidence Level for Non-Gaussian Distributions Using Bootstrap Method,"For reliability-based design optimization (RBDO), generating an input statistical model with confidence level has been recently proposed to offset the inaccurate estimation of the input statistical model with Gaussian distributions. For this, the confidence intervals of mean and standard deviation are calculated using the Gaussian distributions of input random variables. However, if the input random variables are non-Gaussian, the use of the Gaussian distributions of input variables will provide inaccurate confidence intervals, and thus, yield undesirable confidence level of the reliability-based optimum design meeting the target reliability ",2010
1653,Multiscale Variability and Uncertainty Quantification Based on a Generalized Multiscale Markov Model,"Variability is inherent randomness in systems, whereas uncertainty is due to lack of knowledge. In this paper, a generalized multiscale Markov (GMM) model is proposed to quantify variability and uncertainty simultaneously in multiscale system analysis. The GMM model is based on a new imprecise probability theory that has the form of generalized interval, which is a Kaucher or modal extension of classical set-based intervals to represent uncertainties. The properties of the new definitions of independence and Bayesian inference are studied. Based on a new Bayes’ rule with generalized intervals, three cross-scale validation approaches that incorporate variability and uncertainty propagation are also developed.",2010
1654,"Updating Predictive Models: Calibration, Bias Correction and Identifiability","Model updating, which utilizes mathematical means to combine model simulations with physical observations for improving model predictions, has been viewed as an integral part of a model validation process. While calibration is often used to “tune” uncertain model parameters, bias-correction has been used to capture model inadequacy due to a lack of knowledge of the physics of a problem. While both sources of uncertainty co-exist, these two techniques are often implemented separately in model updating. This paper examines existing approaches to model updating and presents a modular Bayesian approach as a comprehensive framework that accounts for many sources of uncertainty in a typical model updating process and provides stochastic predictions for the purpose of design. In addition to the uncertainty in the computer model parameters and the computer model itself, this framework accounts for the experimental uncertainty and the uncertainty due to the lack of data in both computer simulations and physical experiments using the Gaussian process model. Several challenges are apparent in the implementation of the modular Bayesian approach. We argue that distinguishing between uncertain model parameters (calibration) and systematic inadequacies (bias correction) is often quite challenging due to an identifiability issue. We present several explanations and examples of this issue and bring up the needs of future research in distinguishing between the two sources of uncertainty.",2010
1655,A Hybrid Reliability Approach for Reliability-Based Design Optimization,"Reliability-based Design Optimization problems have been solved by two well-known methods: Reliability Index Approach (RIA) and Performance Measure Approach (PMA). RIA generates first-order approximate probabilistic constraints using the measures of reliability indices. For infeasible design points, the traditional RIA method suffers from inaccurate evaluation of the reliability index. To overcome this problem, the Modified Reliability Index Approach (MRIA) has been proposed. The MRIA provides the accurate solution of the reliability index but also inherits some inefficiency characteristics from the Most Probable Failure Point (MPFP) search when nonlinear constraints are involved. In this paper, the benchmark examples have been utilized to examine the efficiency and stability of both PMA and MRIA. In our study, we found that the MRIA is capable of obtaining the correct optimal solutions regardless of the locations of design points but the PMA is much efficient in the inverse reliability analysis. To take advantages of the strengths of both methods, a Hybrid Reliability Approach (HRA) is proposed. The HRA uses a selection factor that can determine which method to use during optimization iterations. Numerical examples from the proposed method are presented and compared with the MRIA and the PMA.",2010
1656,Enabling Integrated Material and Product Design Under Uncertainty Through Stochastic Constitutive Relations,"This paper presents a computational framework that mathematically propagates material microstructure uncertainties to coarser system resolutions for use in multiscale design frameworks. The computational framework uses a homogenized stochastic constitutive relation that links microstructure uncertainty with stochastic material properties. The stochastic constitutive relation formulated in this work serves as the critical link between the material and product domains in integrated material and product design. Ubiquitous fine resolution uncertainty sources influencing prediction of material properties based on their structures are categorized, and stochastic cell averaging is achieved by two advanced uncertainty quantification methods: random process polynomial chaos expansion and statistical copula functions. Both methods confront the mathematical difficulty in randomizing constitutive law parameters by capturing the marked correlation among them often seen in complex materials, thus the results proffer a more accurate probabilistic estimation of constitutive material behavior. The method put forth in this research, though quite general, is applied to a plastic, high strength steel alloy for demonstration.",2010
1657,Tolerance Allocation of Assemblies Using Fuzzy Comprehensive Evaluation and Decision Support Processes,"Advancements in manufacturing technology significantly impact the design process. The ability to manufacture assembly components with specific tolerances has increased the need for tolerance allocation. This research proposes a framework that overcomes the drawbacks of the traditional tolerance control methods, and reduces subjectivity by using fuzzy set theory and decision support processes. The combination of fuzzy comprehensive evaluation and conjoint analysis facilitate the reduction of subjectivity in the tolerance control process. The application of the framework is demonstrated with two practical engineering problems. Tolerances are allocated for a clutch assembly and an O-ring seal in an accumulator.",2010
1658,Effective Random Field Characterization Considering Statistical Dependence for Probability Analysis and Design,"The Proper Orthogonal Decomposition (POD) method has been employed to extract the important signatures of the random field presented in an engineering product or process. Our preliminary study found that coefficients of the signatures are statistically uncorrelated but may be dependent. In general, the statistical dependence of the coefficients is ignored in the random field characterization for probability analysis and design. This paper thus proposes an effective approach to characterize the random field for probability analysis and design while accounting for the statistical dependence among the coefficients. The proposed approach is composed of two technical contributions. The first contribution is to develop a generic approximation scheme of random field as a function of the most important field signatures while preserving prescribed approximation accuracy. The coefficients of the signatures can be modeled as random field variables and their statistical properties are identified using the Chi-Square goodness-of-fit test. Second, the Rosenblatt transformation is employed to transform the statistically dependent random field variables into statistically independent random field variables. There exist so many transformation sequences when the number of random field variables becomes large. It was found that an improper selection of a transformation sequence may introduce high nonlinearity into system responses, which causes inaccuracy in probability analysis and design. Hence, a novel procedure is proposed for determining an optimal transformation sequence that introduces the least degree of nonlinearity to the system response after the Rosenblatt transformation. The proposed random field characterization can be integrated with one of the advanced probability analysis methods, such as the Eigenvector Dimension Reduction (EDR) method, Polynomial Chaos Expansion (PCE) method, etc. Three structural examples including a Micro-Electro-Mechanical Systems (MEMS) bistable mechanism are used to demonstrate the effectiveness of the proposed approach. The results show that the statistical dependence in random field characterization cannot be neglected for probability analysis and design. Moreover, it is shown that the proposed random field approach is very accurate and efficient.",2010
1659,Validation of Computational Fluid Structure Interaction Models for Shape Optimization Under Blast Impact,A first order structural optimization problem is examined to evaluate the effects of structural geometry on blast energy transfer in a fully coupled fluid structure interaction problem. The fidelity of the fluid structure interaction simulation is shown to yield significant insights into the blast mitigation problem not captured in similar empirically based blast models. An emphasis is placed on the accuracy of simulating such fluid structure interactions and its implications on designing continuum level structures. Higher order design methodologies and algorithms are discussed for the application of such fully coupled simulations on vehicle level optimization problems.,2010
1660,A Modified Continuous Reactive Tabu Search for Damage Detection in Beams,"The use of vibration-based techniques in damage identification has recently received considerable attention in many engineering disciplines. While various damage indicators have been proposed in the literature, those relying only on changes in the natural frequencies are quite appealing since these quantities can conveniently be acquired. The identification of damage involves an optimization step where response of a continuously updated finite elements model (FEM) is compared with the response of the experimental measurements and error between both responses is minimized. In this paper it is shown that such error function is highly multi-modal and that the same response can be obtained by more than one damage scenario. In order to find these optima a hybrid optimization approach is developed which utilizes two components; namely. Modified Continuous Reactive Tabu Search (MCRTS) and Real Coded Genetic Algorithms. MCRTS, the primary component, is a meta-heuristic capable of finding several optima in a multi-modal search space, which suites the nature of the problem at hand. GAs, the secondary component, although a global optimizer, is used as a local optimizer that is fired in promising regions of the search space as identified by the major component (MCRTS). It is used in favor of direct search methods to account for the presence of minor local optima. In order to test the algorithm, several beams are manufactured and crack damages are induced using wire-cutting, and the natural frequencies are tested experimentally. Such beams have two locations that can give the same response. The developed algorithm managed to find the two sought-for optima consistently in several runs. This proves the merit of this approach as being capable of handling the problem at hand.",2010
1661,An Efficient Tradeoff Approach for Topology Optimization With Manufacturing Constraints,"In this paper an efficient approach for generating tradeoff curves when performing topology optimization with manufacturing constraints is presented. By minimizing a new stiffness-volume ratio, or in-fact a new compliance-volume product, the tradeoff curve is generated by changing a new design parameter. The volume appearing in the objective is raised to the power of this new design parameter. In such manner different conceptual designs can be generated. By adopting a nested approach, the problem is easily solved by a simple numerical scheme. This is a nice feature of the approach which makes the numerical performance most efficient and robust. This feature makes it also easy to include manufacturing constraints by simply updating the move limits such that these constraints are satisfied. The design parametrization is done by the SIMP-model and patterns of checker-boards are prevented by adopting Sigmund’s filter. The efficiency of the approach is demonstrated by presenting tradeoff curves for both 2D- and 3D-problems.",2010
1662,Optimum Design of an Anchoring System for Percutaneous Mitral Valve Repair,"In a novel procedure for percutaneous mitral valve repair, inter-related hook-shaped anchors are inserted around the annulus to replace the surgeon’s suturing in open-heart ring annuloplasty. To properly attach to the tissue, the anchors should withstand large deformation applied during the delivery process and recover their original shape when released into the heart tissue. To this end, stress concentration is avoided along the anchors, which are fabricated of a super-elastic material, by means of shape optimization. Shape optimization consists in finding the smoothest anchor mid-curve possible, which minimizes the von Mises stresses applied during the delivery process. An optimization algorithm aimed at minimizing the weighted rms value of the curvature is introduced. A geometrically optimum shape is obtained by equally weighting the curvature values. Further reduction in the stress values is possible by weighting the curvature values along the anchor in an iterative procedure that yields a structurally optimum anchor. The weights at each iteration are defined proportional to the stress distribution along the anchor obtained in the previous iteration.",2010
1663,Tracing Pareto-Optimal Frontiers in Topology Optimization,In ,2010
1664,Topology Optimization of Piezoelectric Energy Harvesting Devices Subjected to Stochastic Excitation,"Converting ambient vibration energy into electrical energy using piezoelectric energy harvester has attracted much interest in the past decades. In this paper, topology optimization is applied to design the optimal layout of the piezoelectric energy harvesting devices. The objective function is defined as to maximize the energy harvesting performance over a range of ambient vibration frequencies. Pseudo excitation method (PEM) is applied to analyze structural stationary random responses. Sensitivity analysis is derived by the adjoint method. Numerical examples are presented to demonstrate the validity of the proposed approach.",2010
1665,Fastener Pattern Optimization of an Eccentrically Loaded Multi-Fastener Connection,"This paper presents the use of a genetic algorithm in conjunction with geometric nonlinear finite element analysis to optimize the fastener pattern and lug location in an eccentrically loaded multi-fastener connection. No frictional resistance to shear was included in the model, as the connection transmitted shear loads into four dowel fasteners through bearing-type contact without fastener preload. With the goal of reducing the maximum von Mises stress in the connection to improve fatigue life, the location of the lug hole and four fastener holes were optimized to achieve 55% less maximum stress than a similar optimization using the traditional instantaneous center of rotation method. Since the maximum stress concentration was located at the edge of a fastener hole where fatigue cracks could be a concern, reduction of this quantity lowers the probability of crack growth for both bearing-type and slip-resistant connections. It was also found that the location of the maximum von Mises stress concentration jumped from the fastener region to the lug as the applied force angle was decreased below 45 degrees, thus the fastener pattern could not be optimized for lower angles.",2010
1666,The Mathematical Model of a Procedure for Percutaneous Annuloplasty,"Existing mathematical models of the mitral valve allow the simulation of ring open-heart annuloplasty procedures intended to reduce the lumen of the valve. Using these models, only a posteriori effects can be predicted. With the advent of novel percutaneous annuloplasty approaches, there is a need to describe a priori effects; in particular, this paper focuses on a technique which consists of sequentially installing interconnected anchors around the mitral annulus, whose lumen is reduced by the tightening of the tethered wire. We develop here a static mathematical model of the mitral annulus that takes into account the mechanical response of its tissue and the surrounding muscular tissue. A number of roughly coplanar points corresponding to anchor positions, at about equal distantes, are identified on the annulus. Each of these points is then attached to a linearly elastic spring of a given stiffness, The spring-end is connected to a fixed pinned support, the other end supporting the wire, that forms a loop. With this model we estimate the anchor-points position vectors after lumen reduction and the wire tension that is needed to reduce the perimeter of the polygon defined by the anchor points to a given value, which, for each patient, is related to the desired lumen. This formulation leads to the minimization of the potential energy of the mechanical system over the position vectors of the anchor points after tightening, which are the design variables. These are found by solving the first-order normality conditions of the equality-constrained optimization problem. Preliminary experimental data obtained on cadaveric swine hearts validate the model: it can be used to predict, for a given perimeter size reduction, the wire tension as well as the anchor position after repair.",2010
1667,Using Level Set Method in Order to Design Structures Against Buckling,"The level set approach has been used as a powerful tool in designing structures with a proper safety margin against stability and buckling issues. In this article a closed form equation for critical buckling load of any arbitrary topology has been proposed and employed in Level Set formulation in order to maximize it. Results show that the Level Set Method is straight forward and easy to implement, with fewer limitations overall in the topology optimization of engineering structures.",2010
1668,An Optimization Model for Parabolic-Trough Solar Power Generation Systems,"This paper explores the design optimization of parabolic-trough solar power generation systems. In these systems, solar radiation is focused onto receiver tubes in which a thermal carrier fluid is circulated. The collected thermal energy is then used to generate steam that powers a steam turbine to drive an electric generator. An optimization model is constructed that aims to minimize the cost of electric energy produced. In this model, the optimization is concerned with decision variables that affect: i) the solar field and ii) thermal storage. The steam turbine and generator are not part of the optimization model, as they are assumed to use the same off-shelf components that are used in fossil-fuel based power plants. It is understood that decisions concerning the solar field both affect and are affected by the design of the solar collector assemblies (SCAs), which are the support structures that hold the focusing mirrors. Design of the SCAs is a structural optimization problem that aims to minimize the cost of the structure while satisfying dimensional and loading constraints. Genetic algorithm (GA) is used for the optimization of the parabolic trough system model. For every candidate design examined by GA for the solar field and thermal storage, the most suitable structural design of the SCAs is obtained from solving the sub-problem of structural optimization. This “nested” optimization model is made possible by pre-analyzing a large range of SCA designs and recording them as a lookup database. The developed optimization model of the parabolic trough systems allows for parametric studies on how certain incentives, government policies and key technological developments may affect the system design decisions.",2010
