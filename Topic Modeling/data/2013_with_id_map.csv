PaperID,id,title,abstract,year
110,1210,An Explicit Level-Set Approach for Structural Topology Optimization,"Level-set approaches are a family of domain classification techniques that rely on defining a scalar level-set function (LSF), then carrying out the classification based on the value of the function relative to one or more thresholds. Most continuum topology optimization formulations are at heart, a classification problem of the design domain into structural materials and void. As such, level-set approaches are gaining acceptance and popularity in structural topology optimization. In conventional level set approaches, finding an optimum LSF involves solution of a Hamilton-Jacobi system of partial differential equations with a large number of degrees of freedom, which in turn, cannot be accomplished without gradients information of the objective being optimized. A new approach is proposed in this paper where design variables are defined as the explicit values of the LSF at knot points, then a Kriging model is used to interpolate the LSF values within the rest of the domain so that classification into material or void can be performed. Perceived advantages of the explicit level-set (ELS) approach include alleviating the need for gradients of objectives and constraints, while maintaining a reasonable number of design variables that is independent from the mesh size. A hybrid genetic algorithm (GA) is then used for solving the optimization problem(s). An example problem of a short cantilever is studied under various settings of the ELS parameters in order to infer the best practice recommendations for tuning the approach. Capabilities of the approach are then further demonstrated by exploring its performance on several test problems.",2013
111,1211,Mixed Discrete and Continuous Variable Optimization Based on Constraint Aggregation and Relative Sensitivity,"This work presents a new approach for solving nonlinear mixed discrete-continuous variable problems with constraints. The proposed method falls under the category of direct search methods for discrete variables. Different from the traditional direct search methods that determine the search direction based on decreasing objective function within the feasible space, a relative sensitivity that jointly considers change in objective and constraint functions is introduced in this work to help determining the search direction. For feasible discrete points, the coordinate direction with the maximum relative sensitivity is taken as the search direction, so that the objective function value decreases the fastest with minimum increase in constraint values. For infeasible points, the search direction is determined by the minimum relative sensitivity, so that the points can be dragged into the feasible region with constraints decreasing the fastest and minimum increase of the objective. In addition, in order to reduce the number of constraints and calculate the relative sensitivity, a constraint aggregation technique with Kreisselmeier-Steinhauser function is applied to transform all constraints into an equivalent differentiable inequality constraint. The efficacy and accuracy of the proposed approach is demonstrated with different types of test problems and application to a design problem. The proposed method has advantages in solving nonlinear mixed discrete-continuous variable problems with constraints compared to other existing methods.",2013
112,1212,Optimization of the Cutting Conditions for High Speed Drilling of Woven Composites,"The present work proposes a new algorithm for the optimization of cutting parameters in the high speed drilling of woven composites. The cutting parameters under consideration are the feed rate and the spindle speed. Three performance parameters are to be minimized. These are the exit delamination, the surface roughness and the thrust force. These performance parameters are observed experimentally. One of the challenges that face the experimental testing of these parameters is the high cost of the drilling tools and specimen materials. Therefore, the minimization of the number of experimental tests is a necessary requirement. The algorithm presented hybridizes Kriging as a meta-modeling technique with evolutionary multi-objective optimization to optimize the cutting parameters while intelligently selecting the new set of cutting parameters in each iteration. After starting with a factorial design of the search space, and after testing the performance criteria at these points, the algorithm fits a multi-dimensional surface using Kriging. This step is followed by an evolutionary search on the fitted model. The search spreads a population of search points in the direction of better performance criteria as well as in the direction of un-sampled space. The previous two steps are conducted iteratively for a pre-defined number of iterations. In the final iteration, the population of search points is clustered to yield a small number of new points at which the new experiments will be conducted. The whole process is iterated until the maximum number of allowable experiments is achieved. The algorithm is tested using an existing set of previously published experimental data that are dense enough to predict the actual response surface of the performance criteria. Results showed that the algorithm smartly moved into the direction of higher performance criteria with a low number of experimental trials.",2013
113,1213,Optimization of the Shock Mitigation Layer in the Space Frame Joints of an Armored Vehicle,"Armored vehicles have to survive multiple threats such as projectile or land mines. The shocks induced by these threats can harm vehicle occupants or damage sensitive electronic components. Therefore, a goal of modern armored vehicle design is to reduce transmitted shocks to critical components. In this paper, finite element (FE) models of an armored vehicle prototype having the internal space frame structure with the aforementioned features are developed. One model comprises of only solid elements, while another model is created with purely beam elements. The beam elements model is used for optimization studies whose objective is to reduce the shocks within the vehicle, due to mine blast while maintaining its overall structural integrity. The thickness of the rubberized shock mitigation layer at the joints of the space frame is varied during the optimization process. The optimization problem is solved using the Successive Heuristic Quadratic Approximation (SHQA) algorithm, which combines successive quadratic approximation with an adaptive random search while varying the bounds of the search space. The entire optimization process is carried out within the MATLAB environment. The results show that a significant reduction in the shock can be achieved using this approach.",2013
114,1214,Non-Probabilistic Based Topology Optimization Under External Load Uncertainty With Eigenvalue-Superposition of Convex Models,"In this paper the Eigenvalue-Superposition of Convex Models (ESCM) based topology optimization method for solving topology optimization problems under external load uncertainties is presented. The load uncertainties are formulated using the non-probabilistic based unknown-but-bounded convex model. The sensitivities are derived and the problem is solved using gradient based algorithm. The proposed ESCM based method yields the material distribution which would optimize the worst structure response under the uncertain loads. Comparing to the deterministic based topology optimization formulation the ESCM based method provided more reasonable solutions when load uncertainties were involved. The simplicity, efficiency and versatility of the proposed ESCM based topology optimization method can be considered as a supplement to the sophisticated reliability based topology optimization methods.",2013
115,1215,A Spatial Grammar for the Computational Design Synthesis of Vise Jaws,"For the machining and assembly of mechanical parts, their secure fixation in a defined position is crucial. To achieve this task, flexible fixture devices (FFDs) are the industry standard for small and medium batch-sizes. Unlike dedicated fixtures, FFDs allow for the fixation of different part shapes, increasing their applicability and economic efficiency. Aiming to create a low-cost and autonomous FFD, a reconfigurable vise with adaptable jaws was developed. The jaws can be machined to a variety of shapes to securely hold prismatic and cylindrical parts. In this paper, a spatial grammar approach for the computational design synthesis of these customizable jaws is presented. Different sets of rules for the generation of 3D solid models of vise jaws based on the model of the workpiece to be held are developed and realized in a CAD environment. The approach is verified by generating jaw designs for example parts.",2013
116,1216,A Knowledge-Based Approach for the Automated Design of Robotic/Human Manufacturing Workcells in Hazardous Environments,"The design of manufacturing systems in hazardous environments is complex, requiring interdisciplinary knowledge to determine which components and operators (human or robotic) are feasible. When conceptualizing designs, some options may be overlooked or unknowingly infeasible due to the design engineers’ lack of knowledge in a particular field or ineffective communication of requirements between disciplines. Computational design tools can help alleviate many of the problems encountered in this design task. We create a knowledge-based system (KBS) utilizing CLIPS to automate the synthesis of conceptual manufacturing system designs in radioactive environments. The KBS takes a high-level functional description of a process and uses FBS modeling to generate multiple designs with generic components retrieved from a database and low-level manufacturing task sequences. Using this approach, many options are explored and operator task compatibility is directly addressed. The KBS is applied to the design of glovebox processing systems at Los Alamos National Laboratory (LANL).",2013
117,1217,A New Approach for Designing a Gearbox for a New Kind of Independently Controllable Transmissions,"This paper deals with a new approach for the development of a gearbox of a new kind of variable transmissions, the independently controllable transmission (ICT). It provides a continuous output speed which is independent of the input speed. In the beginning the requirements of a gearbox are mentioned. Design guidelines are the starting point for the design process of the ICT gearbox. Some of these guidelines will be applied on the gearbox of the ICT to model rules which were needed for the design process. The process of the product design of the gearbox can be improved more efficiently by using multidisciplinary design optimization with evolutionary algorithms and topology optimization. This paper describes an approach for the development of this gearbox and the preparation of the optimization models based on a process chain, which is a guide for the next steps of creating the design process in detail. The first step is a parametric CAD model which consists of different parts and provides the complete design space of the gearbox.",2013
118,1218,Topology Optimization of Fluid Channels Using Generative Graph Grammars,"This paper presents a new technique for topology optimization of fluid channels using generative design methods. The proposed method uses the generative abilities of graph grammars with simulation and analysis power of conventional CFD methods. The graph grammar interpreter GraphSynth is used to carry out graph transformations, which define different topologies for a given multi-inlet multi-outlet problem. The generated graphs are first transformed into meaningful 3D shapes. These solutions are then analyzed by a CFD solver to find the optimum. The effectiveness of the proposed method is checked by solving a variety of available test problems and comparing them with those found in the literature. Furthermore by solving complex problems the robustness and effectiveness of the method is tested.",2013
119,1219,Particle Swarm Optimization With Crossover and Mutation Operators Using the Diversity Criteria,"Particle Swarm Optimization is a population based globalized search algorithm that mimics the behavior of swarms. It belongs to the larger class of evolutionary algorithms as widely used stochastic technique in the global optimization field. Since the PSO is population based, it requires no auxiliary information, such as the gradient of the problem.",2013
120,1220,A New Method for Design Decisions Using Decision Topologies,"This paper shows how reliability block diagrams can be used as a decision making tool. The premise behind the idea is that classical decision analysis while very powerful, does not provide tractability in assessing utility functions and their use in making decisions. Our recent work has shown that visual representation of systems using a reliability block diagram can be used to describe a decision situation. In decision making, we called these block diagrams decision topologies. We show that decision topologies can be used to make many engineering decisions and can replace decision analysis for most decisions. The paper proves that at the limit, using decision topologies is entirely consistent with decision analysis for both single attribute and multiattribute cases. The main advantages of the proposed method are that (1) it provides a visual representation of a decision situation, (2) it can easily model tradeoffs, (3) it allows binary attributes, (4) it can be used when limited information is available, and (5) it can be used in a low-fidelity sense to quickly make a decision. The paper details the theoretical basis of the proposed method and highlights its benefits. An example is used to demonstrate how decision topologies can be used in practice.",2013
121,1221,Towards Understanding the Role of Interaction Effects in Visual Conjoint Analysis,"We investigate consumer preference interactions in visual choice-based conjoint analysis, where the conjoint attributes are parameters that define shapes shown to the respondent as images. Interaction effects are present when preference for the level of one attribute is dependent on the level of another attribute. When interaction effects are negligible, a main-effects fractional factorial experimental design can be used to reduce data requirements and survey cost. This is particularly important when the presence of many parameters or levels makes full factorial designs intractable. However, if interaction effects are relevant, a main-effects design creates biased estimates and potentially misleading conclusions. Most conjoint studies assume interaction effects are negligible; however, interactions may play a larger role for shape parameters than for other types of attributes. We conduct preliminary tests on this assumption in three visual conjoint studies. The results suggest that interactions can be either negligible or dominant in visual conjoint, depending on both consumer preferences and shape parameterization. When interactions are anticipated, it is possible in some cases to re-parameterize the shape such that interactions in the new space are negligible. Generally, we suggest that randomized designs are better than fractional factorial designs at avoiding bias due to the presence of interactions and/or the organization of profiles into choice sets.",2013
122,1222,Expanding the Bottom-Up Design Approach Through Integrating Design Attitudes Into Set-Based Design,"In distributed design systems, designers are related to each other through couplings, however they have limited control over the design variables. Any inconsistency in the design system can result in design conflicts through these couplings. Modeling designer attitudes can help to understand inconsistencies and manage conflicts in design processes. We expand the bottom-up design approach through agent-based modeling techniques to another level where designers can make decisions directly on their wellbeing values that represent how their desires are satisfied. Set-based design and constraint programming techniques are used to explore the imprecision of the design activities. Monte Carlo simulations are performed to evaluate the performance of our approach. The results show that the number of design conflicts and their harshness can be lowered when the design process is defined with our approach.",2013
123,1223,"Preference Construction, Sequential Decision Making, and Trade Space Exploration","This paper develops and explores the interface between two related concepts in design decision making. First, design decision making is a ",2013
124,1224,Investigating the Relationship Between Product Design Complexity and FDA for Medical Device Development,"Product complexity has been studied as an important factor to decrease the cost and time of the development process. With this purpose, prior research has included the development of design complexity metrics as a method to assess and decrease complexity. Recent studies have also focused on the comparison of complexity metrics for the particular case of medical devices development (MDD). However, the major issue relevant to MDD has not been addressed; the relationship between FDA regulations and the device complexity is not clarified. Therefore, to increase MDD safety and decrease the time to market, we must understand the regulatory decision process and rules. In this paper, we investigate the relation between different complexity metrics and FDA’s decision time using a sample of 100 hip replacement devices. Bayesian network learning is used to explore in detail local relationships between different variables, both complexity measures and product variables. This relationship was found significant for the first two clusters of the analysis. However, for a third cluster it is speculated that FDA decision time does not depend solely upon the degree of medical device complexity. Company or organization relevant variables could be playing a greater role than just complexity. Additional questions are drawn based on the results that must be investigated.",2013
125,1225,Multi-Objective Robust Optimization Design of a Fixed-Speed Horizontal Axis Wind Turbine,"The produced power and the thrust force exerted on the wind turbine are two conflicting objectives in the design of a floating horizontal axis wind turbine. Meanwhile, the variations in design variables and design environment parameters are unavoidable. The variations include the small variations in the design variables due to manufacturing errors, and the large variations in the wind speed. Therefore, two robustness indices are introduced in this paper. The first one characterizes the robustness of multi-objective optimization problems against small variations in the design variables and the design environment parameters. The second robustness index characterizes the robustness of multi-objective optimization problems against large variations in the design environment parameters. The robustness of the solutions based on the two robustness indices is treated as a vector defined in the robustness function space. As a result, the designer can compare the robustness of all Pareto optimal solutions and make a decision. Finally, the multi-objective robust optimization design of a fixed-speed horizontal axis wind turbine illustrates the proposed methodology.",2013
126,1226,Multi-Stage Optimization of Wind Farms With Limiting Factors,"Larger onshore wind farms are often installed in phases, with discrete smaller sub-farms being installed and becoming operational in succession until the farm as a whole is completed. An extended pattern search (EPS) algorithm that selects both local turbine position and geometry is presented that enables the installation of a complete farm in discrete stages, exploring optimality of both incremental sub-farm solutions and the completed project as a whole. The objective evaluation is the maximization of profit over the life of the farm, and the EPS uses modeling of cost based on an extensive cost analysis by the National Renewable Energy Laboratory (NREL). The EPS uses established wake modeling to calculate the power development of the farm, and allows for the consideration of multiple or overlapping wakes.",2013
127,1227,Wave Energy Extraction Maximization in Irregular Ocean Waves Using Pseudospectral Methods,"Energy extraction from ocean waves and conversion to electrical energy is a promising form of renewable energy, yet achieving economic viability of wave energy converters (WECs) has proven challenging. In this article, the design of a heaving cylinder WEC will be explored. The optimal plant (i.e. draft and radius) design space with respect to the design’s optimal control (i.e. power take-off trajectory) for maximum energy production is characterized. Irregular waves based on the Bretschneider wave spectrum are considered. The optimization problem was solved using a pseudospectral method, a direct optimal control approach that can incorporate practical design constraints, such as power flow, actuation force, and slamming. The results provide early-stage guidelines for WEC design. Results show the resonance frequency required for optimal energy production with a regular wave is quite different than the resonance frequency found for irregular waves; specifically, it is much higher.",2013
128,1228,Design of Thin Film Solar Cell Material Structures for Reliability and Performance Robustness,"An exponential growth of photovoltaic (PV) technologies in the past decade has paved a path to a sustainable solar-powered world. The development of alternative PV technologies with low-cost and high-stability materials has attracted a growing amount of attention. One of these alternatives is the use of second generation thin film PV technologies. However, even in the presence of their bandgap properties, a major issue faced by most thin film solar cells is the low output efficiency due to manufacturing variability and uncertain operating conditions. Thus, to ensure the reliability and performance robustness of the thin film PV technologies, the design of the solar cell is studied. To represent the thin film PV technologies, a copper gallium (di)selenide (CIGS) solar cell model is developed and optimized with Reliability-based Robust Design Optimization (RBRDO) method. This model takes into account the variability of the structure and the material properties of the CIGS solar cells, and assumes an ideal-weather operating condition. This study presents a general methodology to optimize the design of the CIGS PV technologies and could be used to facilitate the development and assessment of new PV technologies with more robust performance in efficiency and stability.",2013
129,1229,Design Optimization of a Solar-Powered Reverse Osmosis Desalination System for Small Communities,"Fresh water availability is essential for the economic development in small communities in remote areas. In desert climate, where naturally occurring fresh water is scarce, seawater or brackish water from wells is often more abundant. Since water desalination approaches are energy intensive, a strong motivation exists for the design of cost-effective desalination systems that utilize the abundant renewable energy resource; solar energy. This paper presents an optimization model of a solar-powered reverse osmosis (RO) desalination system. RO systems rely on pumping salty water at high pressure through semi-permeable membrane modules. Under sufficient pressure, water molecules will flow through the membranes, leaving salt ions behind, and are collected in a fresh water stream. Since RO system are primarily powered via electricity, the system model incorporates photovoltaic (PV) panels, and battery storage for smoothing out fluctuations in the PV power output, as well as allowing system operation for a number of hours after sunset. Design variables include sizing of the PV solar collectors, battery storage capacity, as well as the sizing of the RO system membrane module and power elements. The objective is to minimize the cost of unit volume produced fresh water, subject to constraints on production capacity. A genetic algorithm is used to generate and compare optimal designs for two different locations near the Red Sea and Sinai.",2013
130,1230,Assessing Long-Term Wind Conditions by Combining Different Measure-Correlate-Predict Algorithms,"This paper significantly advanced the hybrid measure-correlate-predict (MCP) methodology, enabling it to account for the variations of both wind speed and direction. The advanced hybrid MCP method used the recorded data of multiple reference stations to estimate the long-term wind condition at the target wind plant site with greater accuracy than possible with data from a single reference station. The wind data was divided into different sectors according to the wind direction, and the MCP strategy was implemented for each wind sector separately. The applicability of the proposed hybrid strategy was investigated using four different MCP methods: (i) linear regression; (ii) variance ratio; (iii) artificial neural networks; and (iv) support vector regression. To implement the advanced hybrid MCP methodology, we used the hourly averaged wind data recorded at six stations in North Dakota between the years 2008 and 2010. The station Pillsbury was selected as the target plant site. The recorded data at the other five stations (Dazey, Galesbury, Hillsboro, Mayville, and Prosper) was used as reference station data. The best hybrid MCP strategy from different MCP algorithms and reference stations was investigated and selected from the 1,024 combinations. The accuracy of the hybrid MCP method was found to be highly sensitive to the combination of individual MCP algorithms and reference stations used. It was also observed that the best combination of MCP algorithms was strongly influenced by the length of the correlation period.",2013
131,1231,Designing a System of Plug-In Hybrid Electric Vehicle Charging Stations,"In this paper we present a two-step approach for the design of a system of Plug-in Hybrid Electric Vehicle (PHEV) charging stations. Our approach consists of a simulation model and a mathematical model. The simulation model formulates the charging station’s ability to meet charging demand by using discrete event simulation. The mathematical model formulates the design decisions made when designing the charging stations, i.e. locations and configurations of charging stations, using the compromise Decision Support Problem (cDSP). Waiting time, service time, number of slots (chargers) and demand are key inputs for the simulation model. Output of the simulation model, which is the service level of the charging stations, is used as an input for the mathematical model. By compromising between maximizing the service level, maximizing the demand coverage, minimizing the installation cost for slots and minimizing distance between charging stations and demand nodes, design decisions are taken in the mathematical model. Our focus in this paper is on the method which is widely applicable. However the approach is presented and evaluated for a data set from Dallas County, Texas.",2013
132,1232,Optimum Solar-Powered HDH Desalination System for Semi-Isolated Communities,"For semi-isolated communities, fresh water may be scarce; however, brackish water or seawater can be easily accessed. This provides a drive to develop optimum-cost desalination system for such communities. This paper presents the optimization of a water-heated solar-powered humidification-dehumidification (HDH) desalination system with variable saline water flow rate. The design variables include the sizing of solar collector, storage tank and its internal heat exchanger, humidifier and dehumidifier. A program was developed to predict performance based on selected weather data file and optimize the system for minimum unit cost of produced fresh water. System cost is predicted via different first-order estimators. A tailored optimization technique is used and compared to a genetic algorithm procedure in the design optimization for local climate and market. A case study develops an optimum desalination plant for the Red Sea near the city of Hurgada and compared to previously developed system.",2013
133,1233,Sensitivity of Array-Like and Optimized Wind Farm Output to Key Factors and Choice of Wake Models,"The creation of wakes, with unique turbulence characteristics, downstream of turbines significantly increases the complexity of the boundary layer flow within a wind farm. In conventional wind farm design, analytical wake models are generally used to compute the wake-induced power losses, with different wake models yielding significantly different estimates. In this context, the wake behavior, and subsequently the farm power generation, can be expressed as functions of a series of key factors. A quantitative understanding of the relative impact of each of these factors is paramount to the development of more reliable power generation models; such an understanding is however missing in the current state of the art in wind farm design. In this paper, we quantitatively explore how the farm power generation, estimated using four different analytical wake models, is influenced by the following key factors: (i) incoming wind speed, (ii) land configuration, and (iii) ambient turbulence. The sensitivity of the maximum farm output potential to the input factors, when using different wake models, is also analyzed. The extended Fourier Amplitude Sensitivity Test (eFAST) method is used to perform the sensitivity analysis. The power generation model and the optimization strategy is adopted from the Unrestricted Wind Farm Layout Optimization (UWFLO) framework. In the case of an array-like turbine arrangement, both the first-order and the total-order sensitivity analysis indices of the power output with respect to the incoming wind speed were found to reach a value of 99%, irrespective of the choice of wake models. However, in the case of maximum power output, significant variation (around 30%) in the indices was observed across different wake models, especially when the incoming wind speed is close to the rated speed of the turbines.",2013
134,1234,"Effects of Uncertain Land Availability, Wind Shear, and Cost on Wind Farm Layout","The robust optimization presented in this paper is formulated to assist in early-stage wind farm development. It can help wind farm developers predict project viability and can help landowners predict where turbines will be placed on their land. A wind farm layout is optimized under multiple sources of uncertainty. Landowner participation is represented with a novel uncertain model of willingness-to-accept monetary compensation. An uncertain wind shear parameter and economies-of-scale cost reduction parameter are also included. Probability Theory, Latin Hypercube Sampling, and Compromise Programming are used to form the robust design problem and minimize the two objectives: the normalized mean and standard deviation of Cost-of-Energy. The results suggest that some landowners that will only accept high levels of compensation are worth pursuing, while others are not.",2013
135,1235,An Activity-Based Costing Method to Support Market-Driven Top-Down Product Family Design,"As more and more companies offer product families rather than individual products, the competitive advantage of product platforming is shrinking. In order to compete companies need to link marketing and engineering so that designers are able to make decisions about critical trade-offs between cost and performance. The current methods for market-driven platform designs use traditional product costing where indirect costs are assigned to individual products based on relative production quantities. Because of increasing product diversity and decreasing direct labor costs, the ratio of indirect costs to total cost of products is increasing. A method for use during the design stage of top-down product family design is needed to assign indirect costs to individual products based on the product’s consumption of indirect resources. An activity-based costing method for top-down product family design is presented here. This method allows the designer to model indirect costs as a function of engineering attributes, creating a framework for top-down product platform optimization that provides a more accurate estimation of cost than traditional product costing methods. An illustrative example shows that an activity-based costing model predicts different profitability from a traditional costing system for a number of different motor designs.",2013
136,1236,Predicting Consumer Choice Set Using Product Association Network and Data Analytics,"Although discrete choice analysis has been shown to be useful for modeling consumer preferences and choice behaviors in the field of engineering design, information of choice set composition is often not available in majority of the collected consumer purchase data. When a large set of choice alternatives exist for a product, such as automotive vehicles, randomly choosing a small set of product alternatives to form a choice set for each individual consumer will result in misleading choice modeling results. In this work, we propose a data-analytics approach to mine existing data of choice sets and predict the choice set for each individual customer in a new choice modeling scenario where the choice set information is lacking. The proposed data-analytics approach integrates product association analysis, network analysis, consumer segmentation, and predictive analytics. Using the J.D. Power vehicle survey as the existing choice set data, we demonstrate that the association network approach is capable of recognizing and expressively summarizing meaningful product relations in choice sets. Our method accounts for consumer heterogeneity using the stochastic generation algorithm where the probability of selecting an alternative into a choice set integrates the information of customer profile clusters and products chosen frequencies. By comparing multiple multinomial logit models using different choice set compositions, we show that the choice model estimates are sensitive to the choice set compositions and our proposed method leads to improved modeling results. Our method also provides insights into market segmentation that can guide engineering design decisions.",2013
137,1237,A Bayesian Approach to Extrinsic Versus Intrinsic Uncertainty in Design for Market Systems,"This article illustrates how variance in the predictive distribution of the profit objective function in a design for market systems model can be decomposed into two components using a simulation based Bayesian approach introduced in the econometrics literature. The first component, intrinsic uncertainty, would be retained in the model even if the model calibration parameter values, such as parameters representing customer preferences, were known with certainty. The second component, extrinsic uncertainty, stems from lack of precision regarding model calibration parameters such as customer preferences. The simulation based approach overcomes a key problem in decomposing uncertainty for the typical design for market systems problem by overcoming the difficulties associated with analytical treatment of non-normal distributions. The variance decomposition approach is demonstrated for the design of a handheld grinder power tool. Following the same Bayesian decision analysis framework the variance simulation method can be applied to other design for market system problems with other objective functions and with additional sources of uncertainty.",2013
138,1238,Strategies for Consumer Control of Complex Product Forms in Generative Design Systems,"In recent years, the number of products that can be tailored to consumers’ needs and desires has increased dramatically; there are many opportunities to individualize the colors, materials or options of products. However, current trends indicate that the future consumer will not be satisfied with mere material and color choices, but will desire control over form as well. While it is technically feasible to allow consumers to partially mass-customize the form of products subject to functional and production constraints through the use of a generative design system, the question of how the control of form should be presented to the user arises. The issue becomes especially important when the product form is based on complex morphologies, which require in-depth knowledge of their parameters to be able to control them fully. In this paper, we discuss this issue and present and test two strategies for controlling complex forms in consumer-oriented generative design systems, one offering the user full control over the design (“total control” strategy), while the other automatically generates designs for the user (“no control” strategy). The implementation of those two control strategies in a generative design system for two categories of products (bookshelf and table) and five types of morphologies are described and tested with a number of design interested participants to estimate their level of satisfaction with the two control strategies. The empirical study shows that the participants enjoyed both the total control and no control strategies. The development of the full control modes for the five morphologies was on the other hand not straightforward, and in general, making the controls meaningful to the consumer can be difficult with complex morphologies. It seems that a consumer-oriented generative design system with two different control strategies, as the ones presented in this article, would offer the most satisfaction.",2013
139,1239,Robust Supply Chain Network Design by Considering Demand-Side Uncertainty and Supply-Side Disruption,"In this paper we propose a method that includes stochastic and robust models for designing the network structure of a three-tiered supply chain involving suppliers, manufacturers and retailers under random demands of markets and by considering disruption probabilities in the procurement facilities and connecting links of the chain. Demands of the markets are as assumed to be random variables with normal distribution and the disruption of the chain is formulated by defining different scenarios. Having substitutable facilities and extra production capacities makes it possible to overcome the negative effects of demand fluctuations and disruption of facilities. First we determine the optimal number, location and capacity of facilities in the first and third tiers and the best flow of material and product throughout the chain in a way to maximize the expected profit from the chain. Then to reduce the difference between the performances of the chain in different scenarios, we propose the ISP-Robust technique based on P-Robust technique to make the model robust. We discuss the differences and advantages of this technique in comparison to the P-Robust technique. We illustrate our method using data from the petrochemical industry.",2013
140,1240,Extracting Consumer Preference From User-Generated Content Sources Using Classification,"The use of online, user-generated content for consumer preference modeling has been a recent topic of interest among the engineering and marketing communities. With the rapid growth of many different types of user-generate content sources, the tasks of reliable opinion extraction and data interpretation are critical challenges. This research investigates one of the largest and most-active content sources, Twitter, and its viability as a content source for preference modeling. Support Vector Machine (SVM) is used for sentiment classification of the messages, and a Twitter query strategy is developed to categorize messages according to product attributes and attribute levels. Over 7,000 messages are collected for a smartphone design case study. The preference modeling results are compared with those from a typical product review study, including over 2,500 product reviews. Overall, the results demonstrate that consumers do express their product opinions through Twitter; thus, this content source could potentially facilitate product design and decision-making via preference modeling.",2013
141,1241,Enhanced Targeted Initial Populations for Multiobjective Product Line Optimization,"Initial populations for genetic algorithms are often created using randomly generated designs in an effort to maximize the genetic diversity in the design space. However, research indicates that the inclusion of solutions generated based on domain knowledge (i.e. non-random solutions) can notably improve the performance of the genetic algorithm with respect to solution performance and/or computational cost for convergence. This performance increase is extremely valuable for computationally expensive problems, such as product line optimization. In prior research, the authors demonstrated these improvements for product line design problems where market share of preference was the performance objective. Initial product line solutions were constructed from products that had the largest product-level utility for individual respondents. However, this simple product identification strategy did not adequately scale to accommodate the richer design problem associated with multiple objectives. This paper extends the creation of targeted initial populations to multiobjective product line design problems by using the objectives of the problem, instead of product level utility, to identify candidate designs. A MP3 player and vehicle feature packaging product line design problems are used to demonstrate this approach and assess the improvement of this modification.",2013
142,1242,Sensitivity of Vehicle Market Share Predictions to Alternative Discrete Choice Model Specifications,"When design decisions are informed by consumer choice models, uncertainty in the choice model and its share predictions creates uncertainty for the designer. We take a first step in investigating the variation in and accuracy of market share predictions by characterizing fit and forecast accuracy of multinomial logit, mixed logit, and nested logit models over a variety of utility function specifications for the US light duty new vehicle market. Using revealed preference data for years 2004–2006, we estimate a multinomial logit model for each combination of a chosen set of utility function covariates found in the literature. We then use each of the models to predict vehicle shares for the 2007 market and examine several metrics to measure fit and predictive accuracy. We find that the best models selected using any of the proposed metrics outperform random guessing yet retain substantial error in fit and prediction for individual vehicle models. For example, with no information (random guessing) 30% of share predictions are within 0.2% absolute share error in a market with an average share of ∼0.4%, whereas for the best models 70% are within 0.2% (for the 2007 vehicle market this translates to an error of ∼33,000 units sold). Share predictions are sensitive to the presence of utility covariates but less sensitive to the form. Models that perform well on one metric tend to perform well on the other metrics as well. In particular, models selected for best fit have comparable forecast error to those with the best forecasts, and residual error in model fit is a major source of forecast error.",2013
143,1243,Determining Vehicle-Level Specifications for a New Car Program Considering Market Environments and Engineering Design Constraints,"Designing a marketable automobile for target markets is an important challenge in the globalized market environment. What makes the vehicle planning even more challenging includes various factors such as engineering feasibility, change of market environment, enforced regulations, and ongoing advancement of technology. In this circumstance, the complexity of automobile design arises from the fact that both the target market’s customer preferences and engineering characteristics should be taken into account in balanced as well as quantitative manners.",2013
144,1244,A Framework for System Design Optimization Based on Maintenance Scheduling With Prognostics and Health Management,"The optimal maintenance scheduling of systems with degrading components is highly coupled with the design of the system and various uncertainties associated with the system, including the operating conditions, the interaction of different degradation profiles of various system components, and the ability to measure and predict degradation using prognostics and health management (PHM) technologies. Due to this complexity, designers need to understand the correlations and feedback between the design variables and lifecycle parameters to make optimal decisions. A framework is proposed for the high level integration of design, component degradation, and maintenance decisions. The framework includes constructing screening models for rapid design evaluation, defining a multi-objective robust optimization problem, and using sensitivity studies to compare trade-offs between different design and maintenance strategies. A case example of power plant condenser is used to illustrate the proposed framework and advise how designers can make informed comparisons between different design concepts and maintenance strategies under highly uncertain lifecycle conditions.",2013
145,1245,Concurrent Design of Functional Reliability and Failure Prognosis for Engineered Resilience,"This paper presents a new system design platform and approaches leading to the development of resilient engineered systems through integrating design of system functions and prognosis of function failures in a unified design framework. Failure prognosis plays an increasingly important role in complex engineered systems since it detects, diagnoses, and predicts the system-wide effects of adverse events, therefore enables a proactive approach to deal with system failures at the life cycle use phase. However, prognosis of system functional failures has been largely neglected in the past at early system design stage, mainly because quantitative analysis of failure prognosis in the early system design stage is far more challenging than these activities themselves that have been mainly carried out at the use phase of a system life cycle. In this paper, a generic mathematical formula of resilience and predictive resilience analysis will be introduced, which offers a unique way to consider lifecycle use phase failure prognosis in the early system design stage and to systematically analyze their costs and benefits, so that it can be integrated with system function designs concurrently to generate better overall system designs. Engineering design case studies will be used to demonstrate the proposed design for resilience methodology.",2013
146,1246,Design of a Robust Classification Fusion Platform for Structural Health Diagnostics,"Efficient health diagnostics provides benefits such as improved safety, improved reliability, and reduced costs for the operation and maintenance of engineered systems. This paper presents a multi-attribute classification fusion approach which leverages the strengths provided by multiple membership classifiers to form a robust classification model for structural health diagnostics. Health diagnosis using the developed approach consists of three primary steps: (i) fusion formulation using a k-fold cross validation model; (ii) diagnostics with multiple multi-attribute classifiers as member algorithms; and (iii) classification fusion through a weighted majority voting with dominance system. State-of-the-art classification techniques from three broad categories (i.e., supervised learning, unsupervised learning, and statistical inference) were employed as the member algorithms. The proposed classification fusion approach is demonstrated with a bearing health diagnostics problem. Case study results indicated that the proposed approach outperforms any stand-alone member algorithm with better diagnostic accuracy and robustness.",2013
147,1247,Computerized Systematic Approach to Fault Tree Analysis Based on Quantity Dimension Indexing,"Fault tree analysis (FTA) is an effective method of ensuring the security and safety of the product by identifying all the possible causes of the problem and fixing them. However, it is not easy for a designer to construct a complete fault tree about various physical phenomena without any misunderstanding or oversight, and some computerized method of managing (i.e., storing, searching and utilizing) knowledge about FTA is needed. To solve the problem, the authors have proposed and studied a method and software tool for knowledge management of FTA based on quantity dimension indexing as a design knowledge management method to avoid ambiguity of literal expression about physical phenomena. In the previous method and software, however, fault values of quantities were limited as just above- and below-normal, and dynamic phenomena such as oscillation could not be described. In this paper, the authors introduce a systematically classified definition of fault values as above/below normal, one-side/both-sides, constant/varying, monotonic/non-monotonic and sudden/gradual, and expand the computerized systematic approach to FTA. Feasibility of the method was examined by applying it to fault tree examples made in a company.",2013
148,1248,A Copula-Based Sampling Method for Data-Driven Prognostics and Health Management,"This paper develops a Copula-based sampling method for data-driven prognostics and health management (PHM). The principal idea is to first build statistical relationship between failure time and the time realizations at specified degradation levels on the basis of off-line training data sets, then identify possible failure times for on-line testing units based on the constructed statistical model and available on-line testing data. Specifically, three technical components are proposed to implement the methodology. First of all, a generic health index system is proposed to represent the health degradation of engineering systems. Next, a Copula-based modeling is proposed to build statistical relationship between failure time and the time realizations at specified degradation levels. Finally, a sampling approach is proposed to estimate the failure time and remaining useful life (RUL) of on-line testing units. Two case studies, including a bearing system in electric cooling fans and a 2008 IEEE PHM challenge problem, are employed to demonstrate the effectiveness of the proposed methodology.",2013
149,1249,A Heat Transfer Model for the Conceptual Design of a Biomass Cookstove for Developing Countries,"The use of biomass cookstoves to meet household energy needs has a profound impact on the life and health of individuals, families, and communities in the developing world. This paper introduces an experimentally validated heat transfer analysis model for use during the conceptual design process of a biomass cookstove to be used in the developing world. This steady-state model of a shielded, natural-draft biomass cookstove fitted with a flat-bottomed pot with pot-shield was developed using published experimental data that included 63 variations of 15 operating, geometrical, and material variables. The model provides the essential information needed to support decision making during the cookstove conceptual design process by predicting heat transfer efficiency as a function of stove geometry, construction material, firepower, and fuel moisture content.",2013
150,1250,Eight Principles Derived From the Engineering Literature for Effective Design for the Developing World,"This paper reviews the findings of several engineering researchers and practitioners on the topic of design for the developing world. We arrange these findings into eight guiding principles aimed at helping those who are searching for effective and sustainable approaches for design for the developing world. The findings reviewed come from the mechanical engineering discipline, as well as from other engineering disciplines. For each principle, we provide references to various studies as a means of supporting the principle. We also provide a detailed example of each principle. Finally, based on our own experience and based on the many papers reviewed, we provide a succinct list of suggestions for using each principle. Ultimately, we believe that the stated principles help overcome the challenges of design for the developing world, which are often dominated by designer unfamiliarity with poverty and foreign culture, as well as by the constraint of extreme affordability.",2013
151,1251,Establishing Consumer Need and Preference for Design of Village Cooking Stoves,"In some villages the use of wood cooking stoves accounts for more than three-quarters of total village energy use. Because of this the design of clean, affordable, and desirable cooking stoves can have a dramatic impact on human health and the local economy. Unfortunately, too often development projects fail. For example, an estimated 30% of water projects in sub-Saharan Africa have failed prematurely in the last 20 years, and only 10% of cooking stove programs started in the 1980s were operational two years after startup. Similar anecdotal evidence suggests a mixed record of success for other energy, infrastructure, health, and sanitation projects in the developing world. In part, these failures occur because of a lack of design questions and design methods to identify consumer need and preference during the problem definition phase of the product design. Because isolated rural villages are generally far from the design engineers’ previous experiences it is even more important to gather in-depth primary data in isolated rural villages. Based on data collected during in-depth field visits to villages in rural West Africa during a village energy study this paper proposes a structured process for collecting the data necessary to design cookstoves that meet local needs, fit within local contexts, and create an aspirational experience that fosters a sustainable solution.",2013
152,1252,Techno-Economic Design of Off-Grid Domestic Lighting Solutions Using HOMER,"Kerosene, candles, and disposable batteries are commonplace in the developing world for rural domestic lighting. These technologies come with negative health and environmental effects that are well documented and often form the basis for engineering design. The immediate and near-term concerns that families experience on a daily basis are also important — economics, quality of light, and quality of service. Families in off-grid rural villages often spend more than half of their energy-related expenditures on domestic lighting. Many technologies have been implemented to provide low-cost and renewable power for lighting, yet these efforts have had a mixed record of success due to persistent financial barriers, issues of consumer acceptance and adoption, and a variety of technical complications. The incidence of these problems can be reduced by completing a techno-economic comparison of alternatives during conceptual design. This paper compares three major categories of off-grid domestic lighting projects: (1) centralized electrification with a micro-grid, (2) battery charging stations, and (3) solar lanterns. The HOMER Energy software is used to compare these options using data gathered from rural villages in Africa. To offer a comparison to existing options available, this paper provides a full financial comparison to a base case — kerosene lanterns — to suggest financing strategies and business models for the options investigated.",2013
153,1253,Structural Complexity Quantification for Engineered Complex Systems and Implications on System Architecture and Design,"The complexity of today’s highly engineered products is rooted in the interwoven architecture defined by its components and their interactions. Such structures can be viewed as the adjacency matrix of the associated dependency network representing the product architecture. To evaluate a complex system or to compare it to other systems, numerical assessment of its structural complexity is essential. In this paper, we develop a quantitative measure for structural complexity and apply the same to real-world engineered systems like gas turbine engines. It is observed that low topological complexity implies centralized architectures and that higher levels of complexity generally indicate highly distributed architectures. We posit that the development cost varies non-linearly with structural complexity. Empirical evidence of such behavior is presented from the literature and preliminary results from simple experiments involving assembly of simple structures further strengthens our hypothesis. We demonstrate that structural complexity and modularity are not necessarily negatively correlated using a simple example. We further discuss distribution of complexity across the system architecture and its strategic implications for system development efforts.",2013
154,1254,Utilizing Emergent Levels to Facilitate Complex Systems Design: Demonstrated in a Synthetic Biology Domain,"Designing complex systems often requires consideration of many components interacting across vast scales of space and time, thus producing highly challenging design spaces to search. In particular, nano-based technologies may require considerations of how nanoscale (10",2013
155,1255,Evolutionary Design of Cellular Self-Organizing Systems,"In this paper, a genetic algorithm (GA) is used to discover interaction rules for a cellular self-organizing (CSO) system. The CSO system is a group of autonomous, independent agents that perform tasks through self-organization without any central controller. The agents have a local neighborhood of sensing and react only to other agents within this neighborhood. Their interaction rules are a simple set of direction vectors based on a flocking model. The five local interaction rules are assigned relative weights, and the agents self-organize to display some emergent behavior at the system level. The engineering challenge is to identify which sets of local rules will cause certain desired global behaviors. The global required behaviors of the system, such as flocking or exploration, are translated into a fitness function that can be evaluated at the end of a multi-agent based simulation run. The GA works by tuning the relative weights of the local interaction rules so that the desired global behavior emerges, judged by the fitness function. The GA approach is shown to be successful in tuning the weights of these interaction rules on simulated CSO systems, and, in some cases, the GA actually evolved qualitatively different local interaction “strategies” that displayed equivalent emergent capabilities.",2013
156,1256,Probabilistic Design of Smart Sensing Functions for Structural Health Monitoring and Prognosis,"Significant technological advances in sensing and communication promote the use of large sensor networks to monitor structural systems, identify damages, and quantify damage levels. Prognostics and health management (PHM) technique has been developed and applied for a variety of safety-critical engineering structures, given the critical needs of the structure health state awareness. The PHM performance highly relies on real-time sensory signals which convey the structural health relevant information. Designing an optimal structural sensor network (SN) with high detectability is thus of great importance to the PHM performance. This paper proposes a generic SN design framework using a detectability measure while accounting for uncertainties in material properties and geometric tolerances. Detectability is defined to quantify the performance of a given SN. Then, detectability analysis will be developed based on structural simulations and health state classification. Finally, the generic SN design framework can be formulated as a mixed integer nonlinear programming (MINLP) using the detectability measure and genetic algorithms (GAs) will be employed to solve the SN design optimization problem. A power transformer study will be used to demonstrate the feasibility of the proposed generic SN design methodology.",2013
157,1257,Resilient Design of Complex Engineered Systems,"This paper presents a complex network and graph spectral approach to calculate the resiliency of complex engineered systems. Resiliency is a key driver in how systems are developed to operate in an unexpected operating environment, and how systems change and respond to the environments in which they operate. This paper deduces resiliency properties of complex engineered systems based on graph spectra calculated from their adjacency matrix representations, which describes the physical connections between components in a complex engineered systems. In conjunction with the adjacency matrix, the degree and Laplacian matrices also have eigenvalue and eigenspectrum properties that can be used to calculate the resiliency of the complex engineered system. One such property of the Laplacian matrix is the algebraic connectivity. The algebraic connectivity is defined as the second smallest eigenvalue of the Laplacian matrix and is proven to be directly related to the resiliency of a complex network. Our motivation in the present work is to calculate the algebraic connectivity and other graph spectra properties to predict the resiliency of the system under design.",2013
158,1258,Descriptor-Based Methodology for Designing Heterogeneous Microstructural Materials System,"In designing a microstructural materials system, there are several key questions associated with design representation, design evaluation, and design synthesis: how to quantitatively represent the design space of a heterogeneous microstructure system using a small set of design variables, how to efficiently reconstruct statistically equivalent microstructures for design evaluation, and how to quickly search for the optimal microstructure design to achieve the desired material properties. This paper proposes a new descriptor-based methodology for designing microstructural materials systems. A descriptor-based characterization method is proposed to provide a quantitative representation of material morphology using a small set of microstructure descriptors covering features of material composition, dispersion status, and phase geometry at different levels of representation. A descriptor-based multi-phase microstructure reconstruction algorithm is developed which allows efficient stochastic reconstruction of microstructures for Finite Element Analysis (FEA) of material behavior. The choice of descriptors for polymer nanocomposites is verified by establishing a mapping between the finite set of descriptors and the infinite dimensional correlation function. Finally, the descriptor-based representation allows the use of parametric optimization approach to search the optimal microstructure design that meets the target material properties. To improve the search efficiency, this paper employs state-of-the-art computational design methods such as Design of Experiment (DOE), metamodeling, statistical sensitivity analysis, and multi-objective optimization. The proposed methodology is demonstrated using the design of a polymer nanocomposites system.",2013
159,1259,Design Methods for Integrated Design of Blast Resistance Panels and Materials,"Integrated Materials and Products Design (IMPD) differs in the way that materials as well as product layout are designed or optimized in a concurrent manner to meet design requirements. IMPD allows the specific performance required in a product to be achieved by tailoring materials and product, since system performance will not be limited by a pre-chosen material employed in conventional, material-selection-based design. In this study, Blast Resistance Panels (BRPs) with square honeycomb core are designed based on this new design approach to further enhance the performance of BRPs.",2013
160,1260,An Energy-Based Design Approach for a Meso-Structure With High Shear Flexure,"This paper presents a design approach for developing meso-structure for high shear flexure in considering distribution of strain energy for a unit cell. Currently, flexible components are often designed with elastomers to take advantage of their unique properties of low shear modulus and high elongation. However, elastomers exhibit high loss modulus at a high frequency when they are subjected to cyclic loading. As a design requirement to find an alternative material in one of the sub systems in the extraterrestrial rover, materials with high elongation but low energy loss is investigated using a meso-structure design approach. In this paper, an approach to design a meso-structure exhibiting shear flexure is developed by conducting comparative studies of shear flexure on three equivalent configurations: auxetic, honeycomb, and sinusoidal. Based on this comparative study, a new hypothesis is proposed that specific strain energy distribution pattern in these meso-structure has a direct impact on the high shear flexure performance. This proposition is verified by developing a new meso-structure, termed ‘S’-Type, which is compared with auxetic and sinusoidal auxetic meso-structures on their shear flexure ability. It is shown from this comparative analysis that the ‘S’-Type meso-structure exhibits higher shear flexure than the other two meso-structures at 5, 10, 20, and 40 MPa of effective shear moduli. Hence, based on this result, a four-step design approach is proposed to design future meso-structures with high shear flexure.",2013
161,1261,A Comparison of Design Approaches to Meso-Structure Development,"The design protocols of meso-structures to satisfy given properties have been an interesting area in material science. Having various mechanical properties by modifying the topology of unit cells is a major issue in developing new meso-structures. This paper reviews different design methodologies to design meso-structures. For each method, an algorithm is presented and compared. Computational methods including topology optimization, parametric optimization, and synthesis methods are among the most popular methods for design meso-structures. Ultimately, it is found that there is a gap in systematic design methods for developing new meso-structure architectures as the current methods are limited to parametric sizing and selection. This gap will be addressed in future research.",2013
162,1262,The Boundary Smoothing in Discrete Topology Optimization of Structures,"In discrete topology optimization, material state is either solid or void and there is no topology uncertainty caused by intermediate material state. A common problem of the current discrete topology optimization is that boundaries are unsmooth. Unsmooth boundaries are caused by corners in topology solutions. Although the outer corner cutting and inner corner filling strategy can mitigate corners, it cannot eliminate them. 90-degree corners are usually mitigated to 135-degree corners under the corner handling strategy. The existence of corners in topology solutions is because of the subdivision model. If regular triangles are used to subdivide design domains, corners are inevitable in topology solutions. To eradicate corner from any topology solution, a subdivision model is introduced in this paper for the discrete topology optimization of structures. The design domain is discretized into quadrilateral design cells and every quadrilateral design cell is further subdivided into triangular analysis cells that have a curved hypotenuse. With the presented subdivision model, all boundaries and connections are smooth in any topology solution. The proposed subdivision approach is demonstrated by two discrete topology optimization examples of structures.",2013
163,1263,Multiscale Topology Optimization of Structures and Periodic Cellular Materials,"The introduction of cellular materials models in topology optimization allows designers to achieving significant weight reductions in structural applications. However, higher material savings and increased performance can be achieved if the material and the structure topologies are concurrently designed. The objective of this paper is to incorporate and establish a design methodology to obtaining optimal macro-scale structures and the corresponding optimal meso-scale periodic material designs in continuum design domains. The proposed approach makes use of homogenization theory to establish communication bridges between both material and structural scales. The periodicity constraint makes such cellular materials manufacturable. Penalization methods are used to obtaining binary solutions in both scales. This proposed methodology is demonstrated in the design of compliant mechanisms and structures of minimum compliance. The results demonstrate potential benefits when this multi-scale design algorithm when applied to the design of ultra-lightweight structures.",2013
164,1264,Hierarchical Design of Composite Materials With Negative Stiffness Inclusions Using a Bayesian Network Classifier,"Recent research in the field of composite materials has shown that it is theoretically possible to produce composite materials with macroscopic mechanical stiffness and loss properties that surpass those of conventional composites. This research explores the possibility of designing and fabricating these composite materials by embedding small volume fractions of negative stiffness inclusions in a continuous host material. Achieving high stiffness and loss from these materials by design, however, is a nontrivial task. This paper presents a hierarchical multiscale material model for these materials, coupled with a set-based, multilevel design approach based on Bayesian network classifiers. Bayesian network classifiers are used to map promising regions of the design space at each hierarchical modeling level, and then the maps are intersected to identify sets of multilevel or multiscale solutions that are likely to provide desirable system performance. Length scales range from the behavior of the structured microscale negative stiffness inclusions to the effective properties of mesoscale composite materials to the performance of an illustrative macroscale component — a vibrating beam coated with the high stiffness, high loss composite material.",2013
165,1265,Application of Regional Strain Energy in Topology Optimization With Inertia Relief Analysis,"In finite element analysis, inertia relief solves the response of an unconstrained structure subject to constant or slowly varying external loads with static analysis computational cost. It is very attractive to utilize it in topology optimization to design structures under unbalanced loads, such as in impact and drop phenomena. In this paper, regional strain energy formulation and inertia relief is integrated into topology optimization to design protective structure under unbalanced loads. For background, the equations of inertia relief are introduced and a commonly used solving method is revisited. Then the regional strain energy formulation for topology optimization with inertia relief is proposed and its sensitivity is derived from the adjoint method. Based on the solving method, the sensitivity is evaluated term by term to simplify the results. The simplified sensitivity can be calculated easily using the output of commercial finite element packages. Finally, the effectiveness of this formulation is shown in the first example and the proposed regional strain energy formulation for topology optimization with inertia relief are presented and discussed in the protective structure design examples.",2013
166,1266,Ceramic Matrix Composite Materials by Design Using Robust Variable Fidelity Optimization,"Ceramic matrix composites (CMC) have been widely studied to tailor desired properties at high temperatures. However, research applications involving design tool development for multi-phase material design are at an early stage of development. While numerical CMC modeling provides significant insight on the material performance, the computational cost of the numerical simulations and the type of variables involved in these models are a hindrance for the effective application of design methods. This technical challenge heightens with the need of considering the uncertainty of material processing and service. For this reason, few design researchers have addressed the design paradox that accompanies the rapid design space expansion in CMC material design.",2013
167,1267,Early-Stage Design of Rheologically Complex Materials via Material Function Design Targets,"Rheological material properties are high-dimensional function-valued quantities, such as frequency-dependent viscoelastic moduli or non-Newtonian shear viscosity. Here we describe a process to model and optimize design targets for such rheological material functions. For linear viscoelastic systems, we demonstrate that one can avoid specific a priori assumptions of spring-dashpot topology by writing governing equations in terms of a time-dependent relaxation modulus function. Our approach embraces rheological design freedom, connecting system-level performance to optimal material functions that transcend specific material classes or structure. This technique is therefore material agnostic, applying to any material class including polymers, colloids, metals, composites, or any rheologically complex material. These early-stage design targets allow for broadly creative ideation of possible material solutions, which can then be used for either material-specific selection or later-stage design of novel materials.",2013
168,1268,Continuous Preference Trend Mining for Optimal Product Design With Multiple Profit Cycles,"Product and design analytics is emerging as a promising area for the analysis of large-scale data and reflection of the extracted knowledge for the design of optimal system. The Continuous Preference Trend Mining (CPTM) algorithm and a framework that are proposed in this study address some fundamental challenges in the context of product and design analytics. The first contribution is the development of a new predictive trend mining technique that captures a hidden trend of customer purchase patterns from large accumulated transactional data. Different from traditional, static data mining algorithms, the CPTM does not assume the stationarity, and dynamically extract valuable knowledge of customers over time. By generating trend embedded future data, the CPTM algorithm not only shows higher prediction accuracy in comparison with static models, but also provide essential properties that could not be achieved with a previous proposed model: avoiding an over-fitting problem, identifying performance information of constructed model, and allowing a numeric prediction. The second contribution is a predictive design methodology in the early design stage. The framework enables engineering designers to optimize product design over multiple life cycles while reflecting customer preferences and technological obsolescence using the CPTM algorithm. For illustration, the developed framework is applied to an example of tablet PC design in leasing market and the result shows that the selection of optimal design is achieved over multiple life cycles.",2013
169,1269,Visual HDMR Model Refinement Through Iterative Interaction,"In engineering design, time-consuming simulations may be needed to find the input-output relationship of a system. High Dimensional Model Representation (HDMR) alleviates the need for intensive simulation by approximating the system’s design space with a surrogate model. Although HDMR can provide an overview, specific regions of interest to the designer may require higher accuracy. This paper presents a tool to visualize and interactively improve HDMR accuracy in specified regions of the design space. Regions of the HDMR are selected by iterative brushing in two-dimensional scatterplot planes. Once a region is chosen, designers may concentrate sampling within its bounds to improve the model locally. Regions can be also improved by modeling the error with a localized radial basis function (RBF) metamodel. The effect of local refinement was further evaluated with localized performance metrics. Testing of the tool shows that it can effectively display and improve HDMR models in regions of interest, if there are variables which have a dominating influence on the output.",2013
170,1270,Design Analytics in Consumer Product Design: A Simulated Study,"A growing area of research in the engineering community is the use of data and analytics for transforming information into knowledge to design better systems, products, and processes. Data-driven decisions can be made in the early, middle, and late stages in a design process where customer needs are identified and understood, a final concept for a design is chosen, and usage data from the deployed product is captured, respectively. Design Analytics (DA) is a paradigm for improving the core information-to-knowledge transformations in these stages of a design process resulting in better performing and functioning products that reflect both explicit and implicit customer needs. In this paper, a simulator is used to model usage of a hypothetical refrigerator and generate artificial data driven by four different customer behavior profiles with variation. The population of customers is randomly divided among the four behavior profiles so that the underlying customer preferences are unknown to the experimenter prior to data analysis. The purpose of the simulation is to illustrate the use of DA in the late stage of a design process to improve the transition from an existing product to the next generation product. Metrics are developed to analyze the product usage data, and both prevailing and subtle usage trends are identified. After conclusions are made, the study proceeds to the early and middle stages of a subsequent design process where a hypothetical next-generation refrigerator is conceptualized.",2013
171,1271,A Simulation Based Estimation of Crowd Ability and its Influence on Crowdsourced Evaluation of Design Concepts,"Crowdsourced evaluation is a promising method for evaluating attributes of design concepts that require human input. One factor in obtaining good evaluations is the ratio of high-ability to low-ability participants within the crowd. In this paper we introduce a Bayesian network model capable of finding participants with high design evaluation ability, so that their evaluations may be weighted more than those of the rest of the crowd. The Bayesian network model also estimates a score of how well each design concept performs with respect to a design attribute without knowledge of the true scores. Monte Carlo simulation studies tested the quality of the estimations on a variety of crowds consisting of participants with different evaluation ability. Results suggest that the Bayesian network model estimates design attribute performance scores much closer to their true values than simply weighting the evaluations from all participants in the crowd equally. This finding holds true even when the group of high ability participants is a small percentage of the entire crowd.",2013
172,1272,A Scalable Preference Elicitation Algorithm Using Group Generalized Binary Search,"We examine the problem of eliciting the most preferred designs of a user from a finite set of designs through iterative pairwise comparisons presented to the user. The key challenge is to select proper queries (i.e., presentations of design pairs to the user) in order to minimize the number of queries. Previous work formulated elicitation as a blackbox optimization problem with comparison (binary) outputs, and a heuristic search algorithm similar to Efficient Global Optimization (EGO) was used to solve it. In this paper, we propose a query algorithm that minimizes the expected number of queries directly, assuming that designs are embedded in a known space and user preference is a linear function of design variables. Besides its theoretical foundation, the proposed algorithm shows empirical performance better than the EGO search algorithm in both simulated and real-user experiments. A novel approximation scheme is also introduced to alleviate the scalability issue of the proposed algorithm, making it tractable for a large number of design variables or of candidate designs.",2013
173,1273,Selection of Precision Machining Cutting Parameters via a Modified Efficient Global Optimization Approach,"The primary objective in precision machining is usually to attain excellent dimensional accuracy and surface finish. In addition, complimentary objectives such as cost and production rate are also important. Proper selection of cutting parameters can profoundly affect both primary and secondary machining performance objectives. While simplified and/or empirical models exist for machining processes, none of those models provides accurate prediction of the dynamic cutting forces, which in turn govern the obtainable quality of the machined surfaces. Finite element analysis (FEA) via ABAQUS/Explicit is adopted in this paper for predicting the machining dynamic cutting forces. Rake and clearance angles, as well as cutting speed are set as the design variables for optimization. Since the machining model requires significant computational resources, economizing the number of FEA runs is desirable. The optimization approach adopted is based off Efficient Global Optimization (EGO), where Kriging models are trained to predict the underlying behavior of the machining process via a finite set of sample points. New sample points are then generated via a multi-objective genetic algorithm that seeks locations of optima and/or high uncertainty in the Kriging models. Machining performance of the new samples is then evaluated via FEA, the Kriging models are re-trained and the process is repeated until one of termination criteria is met. The application study presented is an orthogonal cutting test for ultra-precision micro-cutting using diamond tools.",2013
174,1274,Automatic Reasoning for Defining Lathe Operations for Mill-Turn Parts,"With the increase in computer-controlled hybrid machining (e.g. mill-turn machines), one needs to discern what features of a part are created during turning (i.e. with a lathe cutter) versus those created by milling. Given a generic part shape, it is desirable to extract the turnable and non-turnable features in order to obtain feasible machining plans. A novel approach for automating this division and for defining the resulting turning operations in a hybrid process is proposed in this paper. The algorithm is based on identifying the dominant rotational-axis and performing several non-uniform lateral cross-sections to quickly generate the “as lathed” model. The part is then subtracted from the original model to isolate the non-turnable features. Next, resulting model and features are translated to a label rich graph and fed into a grammar reasoning tool to produce feasible manufacturing plans. The setup design is also studied against the tolerances specified by the designer. Performance of the algorithm has been tested on several examples ranging from simple to complex parts.",2013
175,1275,Tolerance Analysis of Parallel Assemblies Using Tolerance-Maps® and a Functional Map Derived From Induced Deformations,"This paper concerns about modeling tolerance accumulation in parallel assemblies using a spatial math model, the T-Map. In this paper, a specific case in 3D is discussed where an Accumulation Tolerance-Map is modeled when two parts arranged in parallel support a target part between the datum and the functional target feature. By understanding how much of variation from the supporting parts contribute to variations of the target feature, a designer can better utilize the tolerance budget when assigning values to individual tolerances.",2013
176,1276,Tolerance Plugin Module in Integrated Design,"Increased use of recycled material in high-end structural components based on wrought alloys is the goal of the EC project Suplight. The project proposes a framework for multi objective optimization. In this paper, a tolerance plugin module used in this project is described. The tolerance plugin module aims at controlling if the geometrical variation requirements on part level are fulfilled. The variation in the part stems from variation in material and process parameters and the relationship between variation in process and material parameters is estimated using designed computer experiments. Moreover, the tolerance plugin module offers an automatically generated meta-model, based on principal component analysis, for handling part variation that allows for faster Monte Carlo simulations and a format that can be used in variation simulations in succeeding assembly steps. The functionality is illustrated using two cases studies; one for investigation of geometrical part variation due to a stamping process and one for investigation of geometrical part variation due to a sheet metal forming process.",2013
177,1277,Machining Feature Modeling and Process Intermediate Model Generation in Process Planning,"In process planning of machined part, machining feature recognition and representation, feature-based generative process planning, and the process intermediate model generation are the key issues. While many research results have been achieved in recent years, the complete modeling of machining features, process operations, and the 3D models in process planning are still need further research to make the techniques to be applied in practical CAPP systems. In this paper, a machining feature definition and classification method is proposed for the purpose of process planning based on 3D model. Machining features are defined as the surfaces formed by a serious of machining operation. The classification scheme of machining features is proposed for the purpose of feature recognition, feature-based machining operations reasoning, and knowledge representation. Recognized from B-Rep representation of design model, machining features are represented by adjacent graph and organized by feature relations. The machining process plan is modeled as operations and steps, which is the combination and sequencing of machining feature’s process steps. The process intermediate models (PIM) are important for process documentation, analysis and NC programming. An automatic PIM generation approach is proposed using local operations directly on B-Rep model. The proposed data structure and algorithm is adopted in the development of CAPP tool on solid modeler ACIS/HOOPS.",2013
178,1278,Design of Nonlinear Dynamic Systems Using Surrogate Models of Derivative Functions,"Optimization of nonlinear (or linear state-dependent) dynamic systems often requires system simulation. In many cases the associated state derivative evaluations are computationally expensive, resulting in simulations that are significantly slower than real-time. This makes the use of optimization techniques in the design of such systems impractical. Optimization of these systems is particularly challenging in cases where control and physical systems are designed simultaneously. In this article, an efficient two-loop method, based on surrogate modeling, is proposed for solving dynamic system design problems with computationally expensive derivative functions. A surrogate model is constructed for only the derivative function instead of the complete system analysis, as is the case in previous studies. This approach addresses the most expensive element of system analysis (i.e., the derivative function), while limiting surrogate model complexity. Simulation is performed based on the surrogate derivative functions, preserving the nature of the dynamic system, and improving estimation accuracy. The inner loop solves the system optimization problem for a given derivative function surrogate model, and the outer loop updates the surrogate model based on optimization results. This solution approach presents unique challenges. For example, the surrogate model approximates derivative functions that depend on both design and state variables. As a result, the method must not only ensure accuracy of the surrogate model near the optimal design point in the design space, but also the accuracy of the model in the state space near the state trajectory that corresponds to the optimal design. This method is demonstrated using two simple design examples, followed by a wind turbine design problem. In the last example, system dynamics are modeled using a linear state-dependent model where updating the system matrix based on state and design variable changes is computationally expensive.",2013
179,1279,Development of a Common Platform for Testing Metamodel Based Design Optimization Methods,"Metamodel based design optimization (MBDO) algorithms have attracted considerable interests in recent years due to their special capability in dealing with complex optimization problems with computationally expensive objective and constraint functions and local optima. Conventional unimodal-based optimization algorithms and stochastic global optimization algorithms either miss the global optimum frequently or require unacceptable computation time. In this work, a generic testbed/platform for evaluating various MBDO algorithms has been introduced. The purpose of the platform is to facilitate quantitative comparison of different MBDO algorithms using standard test problems, test procedures, and test outputs, as well as to improve the efficiency of new algorithm testing and improvement. The platform consists of a comprehensive test function database that contains about 100 benchmark functions and engineering problems. The testbed accepts any optimization algorithm to be tested, and only requires minor modifications to meet the test-bed requirements.",2013
180,1280,Improved Trust Region Based MPS Method for High-Dimensional Expensive Black-Box Problems,"Mode Pursuing Sampling (MPS) was developed as a global optimization algorithm for optimization problems involving expensive black box functions. MPS has been found to be effective and efficient for problems of low dimensionality, i.e., the number of design variables is less than ten. A previous conference publication integrated the concept of trust regions into the MPS framework to create a new algorithm, TRMPS, which dramatically improved performance and efficiency for high dimensional problems. However, although TRMPS performed better than MPS, it was unproven against other established algorithms such as GA. This paper introduces an improved algorithm, TRMPS2, which incorporates guided sampling and low function value criterion to further improve algorithm performance for high dimensional problems. TRMPS2 is benchmarked against MPS and GA using a suite of test problems. The results show that TRMPS2 performs better than MPS and GA on average for high dimensional, expensive, and black box (HEB) problems.",2013
181,1281,Indexing Methods for Discrete Mode Pursuing Sampling,"Discrete Mode Pursuing Sampling (D-MPS) is a method used for optimization of expensive black-box functions with discrete variables. The type of discrete space where all possible combinations of discrete values of all variables are valid design points is called a full grid of points or a “regular grid” in this paper. A regular structure for sampling data is a requirement when D-MPS is applied. This paper presents two new indexing methods, i.e. “",2013
182,1282,Diagnostic Analysis of Metamodels’ Multivariate Dependencies and Their Impacts in Many-Objective Design Optimization,"While computers are continually getting faster, physical models of complex systems grow more sophisticated and keep pace. Using metamodels can dramatically reduce the time it takes to evaluate a solution for a complex system; however, while the chief virtue of metamodels is that they approximate more computationally expensive models, this is also their main drawback. Metamodels are approximations, and as such they behave differently than the more sophisticated models they approximate. While the metamodels may be accurate approximations, they may also introduce new interdependencies in the response outputs that may hinder search algorithms during optimization. Understanding the impact of the approximation on the subsequent search thus becomes an important part of the problem as models that were computationally expensive ten to fifteen years ago may now run fast enough for use in optimization. In this paper, we use Sobol′ global sensitivity analysis to compare the search performance of a new auto-adaptive many objective evolutionary algorithm solving a challenging product family design problem with both the original analysis and a second-order response surface approximation of the original analysis. Interdependencies in the response outputs are found to result from the problem formulation used rather than the underlying model in this case. Search operator selection by the auto-adaptive evolutionary algorithm is shown to be consistent with the model sensitivities found by global sensitivity analysis.",2013
183,1283,"Classifier-Guided Sampling for Discrete Variable, Discontinuous Design Space Exploration","Estimation of density algorithms (EDAs) have been developed for optimization of discrete, continuous, or mixed discrete and continuous simulation-based design problems. EDAs construct a probability distribution on the set of highest performing designs and sample the distribution for the next generation of solutions. In previous work, the authors have demonstrated how classifier-guided sampling can also be used for discrete variable, discontinuous design space exploration. In this paper we develop the rationale for using classifier-guided sampling as a simple step beyond EDAs that not only improves the characterization of the highest performing designs but also identifies the poorly performing designs and exploits that information for faster convergence to optimal solutions. The resulting method is novel in its use of Bayesian priors to model the inherent uncertainty in a probability distribution that is based on a limited number of samples from the design space. The new classifier-guided method is applied to several example problems and convergence rates are presented that compare favorably to random search and a basic EDA implementation.",2013
184,1284,A Data Mining Trajectory Clustering Methodology for Modeling Indoor Design Space Utilization,"Traditionally, understanding indoor space utilization in a typical design setting has been based on observation methodologies, where researchers document team interactions, space utilization and design activities using qualitative observation techniques. The authors of this paper propose a data mining driven methodology aimed at modeling the utilization of indoor design spaces using trajectory pattern data. Using indoor Radio-frequency identification (RFID) technology, researchers are able to collect trajectory data which can then be used to quantify the distribution of space usage patterns over time and predict future regions of interest. The proposed methodology consists of two phases: i) trajectory partitioning and ii) line segment clustering. For the first phase, trajectories are partitioned into line segments, based on unique user characteristics. In the second phase, a data mining clustering algorithm is employed to group line segments into different clusters based on a distance function. Since individual trajectories may exhibit similar movement patterns, the proposed methodology can help designers better understand how design spaces are utilized and how team dynamics evolve over time, depending on the specific design task being executed. A 3,500 square foot design space was used for the semester long study that included design teams supervised by teaching assistants. The results provide insight into the underutilization of certain regions of the design space and proposes directions towards an optimal design space methodology.",2013
185,1285,A Z-Score-Based Method to Synthesize Anthropometric Datasets for Global User Populations,"Globalized marketplaces are necessitating the consideration of the needs of users from a variety of national and international regions. Relevant body dimensions are known to play a key role in influencing users’ physical interactions with products. The main challenge in designing these products is the unavailability of comprehensive anthropometric databases for detailed analyses and decision-making. This paper presents a new method to this end. Z-scores are computed for each body measure of every individual in a reference population; this can be any population for which a comprehensive database is available. Next, descriptive statistical information (e.g., means, standard deviations, by-percentile values) from numerous studies and surveys are used to estimate distributions of the required body dimensions. Finally, the z-score values from the reference population are utilized to sample from the aforementioned distributions in order to synthesize the requisite virtual target population of users. The z-score method is demonstrated in the context of two existing populations: U.S. military in the late 1980s (ANSUR) and Japanese youth from the early 1990s. Despite certain stated limitations, which are topics of future work in this line of research, the method is shown to be accurate, easy-to-apply, and robust in terms of underlying assumptions.",2013
186,1286,Building of Usage Scenarios Space for Investigating the Fall Situations of the Elderly People,"Assessing the number of users defined by a set of specific usage attributes in a given usage contextual situation is not always an obvious task in a market segmentation process. Although new approaches in design and marketing seem to be more sensitive to the adequacy of a design concept with the usage scenarios, these methods do not systematically consider the various usage situations. The present article puts forward a methodology intending to build a usage scenarios space in which the input data is thoroughly collected and validated. This methodology is applied to the complex and multifactorial issue of falls among the elderly in the Metropolitan France. In this paper, numerous medical publications have been made to study influential factors of fall situations. However, even solution providers for fall prevention and teleassistance ignore the real situational coverage of their solutions. As a result, “usage scenarios space” is built using an appropriate segmentation of usage contexts (here, fall situations) and user characteristics. These data are used for a design oracle to predict (simulate) the various and multiple usage scenarios.",2013
187,1287,Computer-Aided Customized Shape Design of an N95 Filtering Facepiece Respirator,"A respirator protects its user by sealing the user’s face and filtering hazardous particles from environment. However, faceseal leakages of users-respirators always happen. First, this study investigated a computer-aided technique for designing a well fitted N95 filtering facepiece respirator (FFR) for a subject. The customized N95 FFR includes a customized contact area, a center filtering area, two straps and a nasal clip. Five base contact areas of National Institution for Occupational Safety and Health (NIOSH) headforms were created and the customized contact area was modeled using the mapping relationship between the subject and a NIOSH headform. The center filtering area was designed by considering constraints of N95 FFR shape. Second, this study used simulation-based approaches, including the FE method and computational fluid dynamics (CFD) method, to assess the performance of the customized N95 FFR on the subject. The contact pressure and the faceseal leakage from the subject-customized N95 FFR combination were compared with the results from the subject-existing N95 FFRs combinations. The comparison showed that the customized N95 FFR provided the subject an optimized contact pressure distribution and no faceseal leakage.",2013
188,1288,Potential Methods for Prediction of Onset of Slip in Gait During the Transition From Double Support to Single Support,"This paper investigates two methods for the prediction of onset of slip in gait during the transition from single support to double support. The first method employs an optimization-based gait prediction simulation to determine the threshold of walking velocity that results in stable gait for one stride length and one subject. In order to determine the threshold, the simulation is carried out with progressively increasing walking velocities (initial conditions) until the gait becomes unstable. The zero moment point (ZMP) and support region are employed as criteria for stable gait, i.e. as long as the ZMP remains within the support region throughout the duration of gait, the gait is said to be stable. The second method employs a probabilistic simulation to predict the likelihood of slip, where the likelihood of slip is related to the available and required friction for gait.",2013
189,1289,Cognitive-Based Terminal State Prediction for Human Motion Planning,"Every day, people are presented with tasks that are completed with very little mechanical effort such as turning a door knob, turning a screw driver, and grabbing a cup to move it to a new location and/or orientation. These tasks are often overlooked in the mechanical study of human movement due to the fact they carry with them very little biomechanical costs or effort. However, from a cognitive standpoint, these tasks carry high complexity. For example, the simple task of grabbing a cup and flipping it over is very easy mechanically and can be effortlessly achieved by a two year old. However, it is impossible to predict or simulate this motion without major intervention in the form of explicit constraints defining the task. The model itself cannot decide in which orientation the hand should assume in order to grasp the object. Also, it cannot decide where on the object the hand should be placed. These aspects must be assumed by the researcher and constrained in the formulation. In other words, digital human models, such as optimization-based motion prediction models, are unable to plan actions. This implies that for each task, a unique optimization formulation is needed in order to predict the motion/posture needed to complete each task. This paper presents a new method for task planning prediction within optimization based posture and motion prediction. It provides a new single optimization formulation that allows for the prediction of multiple unique manual manipulation tasks. The method is based on observations made from experimental studies on cognitive motor planning.",2013
190,1290,Hierarchical Bayesian Parameter Estimation for Modeling and Analysis of User Affective Influence,"Traditional user experience (UX) models are mostly qualitative in terms of its measurement and structure. This paper proposes a quantitative UX model based on cumulative prospect theory. It takes a decision making perspective between two alternative design profiles. However, affective elements are well-known to have influence on human decision making, the prevailing computational models for analyzing and simulating human perception on UX are mainly cognition-based models. In order to incorporate both affective and cognitive factors in the decision making process, we manipulate the parameters involved in the cumulative prospect model to show the affective influence. Specifically, three different affective states are induced to shape the model parameters. A hierarchical Bayesian model with a technique called Markov chain Monte Carlo is used to estimate the parameters. A case study of aircraft cabin interior design is illustrated to show the proposed methodology.",2013
191,1291,Reconstructing Humans’ Hand Motion: Preliminary Results and Applications in the Design of Mechanical Fingers for Anthropomorphic Tasks,"This paper reports the development of a low-cost sensor-based glove device using commercially available components that can be used to obtain position, velocity and acceleration data for individual fingers of the hand. Optical tracking of the human hand and finger motion is a challenging task due to the large number of degrees of freedom (DOFs) packed in a relatively small space. We propose methods to simplify the hand motion capture by utilizing accelerometers and adopting a reduced marker protocol.",2013
192,1292,Pareto Front Identification via Objective Vector Jacobian Matrix Singularity,This paper presents a method to identify the exact Pareto front for a multi-objective optimization problem. The developed technique addresses the identification of the Pareto frontier in the cost space and the Pareto set in the design space for both constrained and unconstrained optimization problems. The proposed approach identifies a ,2013
193,1293,Multi-Objective Robust Optimization Using Differential Evolution and Sequential Quadratic Programming,"Multi-Objective Robust Optimization (MORO) can find Pareto solutions to multi-objective engineering problems while keeping the variation of the solutions being within an acceptable range when parameters vary. While the literature reports on many techniques in MORO, few papers focus on the implementation of Multi-Objective Differential Evolution (MODE) for robust optimization and the performance improvement of solutions. In this paper, MODE is first modified and implemented for robust optimization, formulating a new MODE-RO algorithm. To improve the solutions’ quality of MODE-RO, a new hybrid MODE-SQP-RO algorithm is further proposed, where Sequential Quadratic Programming (SQP) is incorporated to enhance the local search. In the hybrid algorithm, two criteria, indicating the convergence speed of MODE-RO and the switch between MODE and SQP are proposed respectively. One numerical and one engineering examples are tested to demonstrate the applicability and performance of the proposed algorithms. The results show that MODE-RO is effective in solving Multi-Objective Robust Optimization problems; while on the average, MODE-SQP-RO significantly improves the quality of robust solutions with comparable numbers of function evaluations.",2013
194,1294,Robustness Against Large Variations in Multi-Objective Optimization Problems,"In the presence of multiple optimal solutions in multi-modal optimization problems and in multi-objective optimization problems, the designer may be interested in the robustness of those solutions to make a decision. Here, the robustness is related to the sensitivity of the performance functions to uncertainties. The uncertainty sources include the uncertainties in the design variables, in the design environment parameters, in the model of objective functions and in the designer’s preference. There exist many robustness indices in the literature that deal with small variations in the design variables and design environment parameters, but few robustness indices consider large variations. In this paper, a new robustness index is introduced to deal with large variations in the design environment parameters. The proposed index is bounded between zero and one, and measures the probability of a solution to be optimal with respect to the values of the design environment parameters. The larger the robustness index, the more robust the solution with regard to large variations in the design environment parameters. Finally, two illustrative examples are given to highlight the contributions of this paper.",2013
195,1295,Sensitivity Analysis in Quantified Interval Constraint Satisfaction Problems,"Interval is an alternative to probability distribution in quantifying epistemic uncertainty for reliability analysis when there is a lack of data to fit a distribution with good confidence. It only requires the information of lower and upper bounds. The propagation of uncertainty is analyzed by solving interval-valued constraint satisfaction problems (CSPs). By introducing logic quantifiers, quantified constraint satisfaction problems (QCSPs) can capture more semantics and engineering intent than CSPs. Sensitivity analysis (SA) takes into account of variations associated with the structure and parameters of interval constraints to study to which extent they affect the output. In this paper, a global SA method is developed for QCSPs, where the effects of quantifiers and interval ranges on the constraints are analyzed based on several proposed metrics, which indicate the levels of indeterminacy for inputs and outputs as well as unsatisfiability of constraints. Two vehicle design problems are used to demonstrate the proposed approach.",2013
196,1296,Multi-Objective Optimization of a Disc Brake System by Using SPEA2 and RBFN,"Many engineering design optimization problems involve multiple conflicting objectives, which today often are obtained by computational expensive finite element simulations. Evolutionary multi-objective optimization (EMO) methods based on surrogate modeling is one approach of solving this class of problems. In this paper, multi-objective optimization of a disc brake system to a heavy truck by using EMO and radial basis function networks (RBFN) is presented. Three conflicting objectives are considered. These are: 1) minimizing the maximum temperature of the disc brake, 2) maximizing the brake energy of the system and 3) minimizing the mass of the back plate of the brake pad. An iterative Latin hypercube sampling method is used to construct the design of experiments (DoE) for the design variables. Next, thermo-mechanical finite element analysis of the disc brake, including frictional heating between the pad and the disc, is performed in order to determine the values of the first two objectives for the DoE. Surrogate models for the maximum temperature and the brake energy are created using RBFN with polynomial biases. Different radial basis functions are compared using statistical errors and cross validation errors (PRESS) to evaluate the accuracy of the surrogate models and to select the most accurate radial basis function. The multi-objective optimization problem is then solved by employing EMO using the strength Pareto evolutionary algorithm (SPEA2). Finally, the Pareto fronts generated by the proposed methodology are presented and discussed.",2013
197,1297,Optimization Algorithms and ODE’s in MDO,"There is a need for a stronger theoretical understanding of Multidisciplinary Design Optimization (MDO) within the field. Having developed a differential geometry framework in response to this need, we consider how standard optimization algorithms can be modeled using systems of ordinary differential equations (ODEs) while also reviewing optimization algorithms which have been derived from ODE solution methods. We then use some of the framework’s tools to show how our resultant systems of ODEs can be analyzed and their behaviour quantitatively evaluated. In doing so, we demonstrate the power and scope of our differential geometry framework, we provide new tools for analyzing MDO systems and their behaviour, and we suggest hitherto neglected optimization methods which may prove particularly useful within the MDO context.",2013
198,1298,Integrated Design and Multi-Objective Optimization of a Single Stage Heat-Pump Turbocompressor,"Small scale turbomachines in domestic heat pumps reach high efficiency and provide oil-free solutions which improve heat-exchanger performance and offer major advantages in the design of advanced thermodynamic cycles. An appropriate turbocompressor for domestic air based heat pumps requires the ability to operate on a wide range of inlet pressure, pressure ratios and mass flows, confronting the designer with the necessity to compromise between range and efficiency. Further the design of small-scale direct driven turbomachines is a complex and interdisciplinary task. Textbook design procedures propose to split such systems into subcomponents and to design and optimize each element individually. This common procedure, however, tends to neglect the interactions between the different components leading to suboptimal solutions. The authors propose an approach based on the integrated philosophy for designing and optimizing gas bearing supported, direct driven turbocompressors for applications with challenging requirements with regards to operation range and efficiency. Using previously validated reduced order models for the different components an integrated model of the compressor is implemented and the optimum system found via multi-objective optimization. It is shown that compared to standard design procedure the integrated approach yields an increase of the seasonal compressor efficiency of more than 12 points. Further a design optimization based sensitivity analysis allows to investigate the influence of design constraints determined prior to optimization such as impeller surface roughness, rotor material and impeller force. A relaxation of these constrains yields additional room for improvement. Reduced impeller force improves efficiency due to a smaller thrust bearing mainly, whereas a lighter rotor material improves rotordynamic performance. A hydraulically smoother impeller surface improves the overall efficiency considerably by reducing aerodynamic losses. A combination of the relaxation of the 3 design constraints yields an additional improvement of 6 points compared to the original optimization process. The integrated design and optimization procedure implemented in the case of a complex design problem thus clearly shows its advantages compared to traditional design methods by allowing a truly exhaustive search for optimum solutions throughout the complete design space. It can be used for both design optimization and for design analysis.",2013
199,1299,A Deterministic and Probabilistic Approach for Robust Optimal Design of a 6-DOF Haptic Device,"This work suggests a two-stage approach for robust optimal design of 6-DOF haptic devices based on a sequence of deterministic and probabilistic analyses with a multi-objective genetic algorithm and the Monte-Carlo method. The presented model-based design robust optimization approach consider simultaneously the kinematic, dynamic, and kinetostatic characteristics of the device in both a constant and a dexterous workspace in order to find a set of optimal design parameter values for structural configuration and dimensioning. Design evaluation is carried out based on local and global indices, like workspace volume, quasi-static torque requirements for the actuators, kinematic isotropy, dynamic isotropy, stiffness isotropy, and natural frequencies of the device. These indices were defined based on focused kinematic, dynamic, and stiffness models. A novel procedure to evaluate local indices at a singularity-free point in the dexterous workspace is presented. The deterministic optimization approach neglects the effects from variations of design variables, e.g. due to tolerances. A Monte-Carlo simulation was carried out to obtain the response variation of the design indices when independent design parameters are simultaneously regarded as uncertain variables. It has been observed that numerical evaluation of performance indices depends of the type of workspace used during optimization. To verify the effectiveness of the proposed procedure, the performance indices were evaluated and compared in constant orientation and in dexterous workspace.",2013
200,1300,Using a Goal-Switching Selection Operator in Multi-Objective Genetic Algorithm Optimization Problems,"This paper demonstrates how solution quality for multiobjective optimization problems can be improved by altering the selection phase of a multiobjective genetic algorithm. Rather than the traditional roulette selection used in algorithms like NSGA-II, this paper adds a goal switching technique to the selection operator. Goal switching in this context represents the rotation of the selection operator among a problem’s various objective functions to increase search diversity. This rotation can be specified over a set period of generations, evaluations, CPU time, or other factors defined by the designer. This technique is tested using a set period of generations before switching occurs, with only one objective considered at a time. Two test cases are explored, the first as identified in the Congress on Evolutionary Computation (CEC) 2009 special session and the second a case study concerning the market-driven design of a MP3 player product line. These problems were chosen because the first test case’s Pareto frontier is continuous and concave while being relatively easy to find. The second Pareto frontier is more difficult to obtain and the problem’s design space is significantly more complex. Selection operators of roulette and roulette with goal switching were tested with 3 to 7 design variables for the CEC 09 problem, and 81 design variables for the MP3 player problem. Results show that goal switching improves the number of Pareto frontier points found and can also lead to improvements in hypervolume and/or mean time to convergence.",2013
201,1301,Examining the Impact of Aggregated Design Impulses on Process Architecture in Distributed Design,"During the design of complex systems, a design process may be subjected to stochastic inputs, interruptions, and changes. These design impulses can have a significant impact on the transient response and converged equilibrium for the design system. We distinguish this research by focusing on the interactions between local and architectural impulses in the form of designer mistakes and dissolution, division, and combination impulses, respectively. We find that local impulses tend to slow convergence but systems subjected to dissolution/division impulses still favor parallel arrangements. The strategy to mitigate combination impulses is unaffected by the presence of local impulses.",2013
202,1302,Linking 10 Years of Modular Design Research: Alternative Methods and Tool Chain Sequences to Support Product Platform Design,"Modular product platforms have been shown to provide substantial cost and time savings while still allowing companies to offer a variety of products. As a result, a multitude of product platform methods have been developed over the last decade within the design research community. However, comparison and integration of suitable methods is difficult since the methods have, for the most part, been developed in isolation from one another. In reviewing the literature in modularity and product platforms, we create a generic set of twelve platform design activities. We then examine a set of product platform development processes used at several different companies, and from this form a generic sequence of the activities. We then associate the various developed methods to the sequence, thereby enabling the chaining together of the various modular and platform design methods developed by the community.",2013
203,1303,An Approach for Managing Engineering Changes in Product Families,Product development is characterized by continuous updating of existing solutions in order to cope with new market requirements. Families of product variants are used to satisfy the needs of new potential customers and penetrate new market niches.,2013
204,1304,Comprehensive Product Platform Planning (CP,"The development of products with a modular structure, where the constituent modules could be derived from a set of common platforms to suit different market niches, provides unique engineering and economic advantages. However, the quantitative design of such modular product platforms could become significantly challenging for complex products. The Comprehensive Product Platform Planning (CP",2013
205,1305,A Concept Selection Framework for Early Sorting of Reconfigurable System Designs,"A seven-step framework for sorting proposed concepts of system changes / reconfigurations is presented that seeks to characterize the overall ramifications on system architecture. This framework is intended for use immediately following a concept generation phase. The framework uses three simple questions: “What level of the system design does this concept apply to?” “What levels of the system design does the concept impact?” and “What is the severity of this impact?” A flowchart leads the designer through these questions and assigns each concept a classification from one to five based on the answers. Class one concepts have little to no impact on the rest of the system architecture. They can be included with little fear of massive change propagation and system redesign. Class five concepts carry large changes to system architecture and therefore should be included only if they can be shown to be highly beneficial, or if there remains enough design freedom such that the cost of changing the system architecture is minimal. Meanwhile, class five concepts are likely to have much higher potential to create revolutionary design. A case study is used to demonstrate the application of the sorting framework in the context of a Mars rover mission. Several example concepts are provided to illustrate key insights from the case study. Convergence of the framework is explored by comparing the authors’ results to a second test done by a new design team.",2013
206,1306,Green Profit Maximization Through Joint Pricing and Production Planning of New and Remanufactured Products,"To achieve “green profit” in their business, manufacturers who produce both new and remanufactured products must optimize their pricing and production decisions simultaneously. They must determine the buy-back price and take-back quantity of end-of-life products as well as the selling prices and production quantities of new and remanufactured products. With an aim to assist in optimal pricing and production planning, this paper presents a mixed-integer programming model that optimizes the three prices (of buyback, new and remanufactured products) and the corresponding production plan simultaneously. The model considers the two conflicting objectives of maximizing economic profitability and maximizing environmental impact saving. The model helps address potential barriers to remanufacturing, which include limited economic, and/or environmental sustainability of remanufacturing, imbalance between the supply of end-of-life products and the market demand for remanufactured products, and cannibalization of the sales of new products. The developed model is illustrated with an example of engine water pump.",2013
207,1307,Product-Service Integration for the Sustainable Publishing Process,"Publishing is the process of developing and producing content for distribution to the public. In the past, the publishing process heavily relies on printing as the method of content production. This causes voracious consumption and waste of natural resources. In today’s sweeping trend of digitization that is featured by the increasing popularity of various smart devices, the publishing process is undergoing a profound transition from the traditional printing-reliant publishing model to the new digital publishing model. Such a transition brings great opportunities for the publishing process to achieve better sustainability by evolving towards a product service system. This paper intends to advance the publishing process from the product service integration perspective. Above all, a general product-service integration framework is developed to describe the interdependent relationships among key stakeholders and elements in the publishing value chain. Furthermore, several specific publishing PSS are discussed. Finally, these publishing PSS are evaluated and compared from the value creation perspective.",2013
208,1308,A Product-Service System Model for Identifying Design Factors,"In competitive market environments, strategies that adding services to products for sales promotion are now moved to integrate products and services for satisfying diverse customer needs, and the number of these cases is gradually increasing. Trends of integrating products and services lead to the emergence of a product-service system (PSS). To implement and embody a PSS solution in new product development, a comprehensive design framework is allowed designers to facilitate the design factors of the PSS in complex business environments. The objective of this paper is to propose a PSS model to identify design factors for developing products and services by integrating object-oriented concepts and blueprinting in context of a business ecosystem. The proposed model is developed based on relationship between products and services matching with their design factors. The products and the services are then brought together to form a PSS. Functions and processes can be categorized to identify the design factors in different levels using the object-oriented concepts. Interaction between products and services lies on a PSS platform to form a product service system in blueprinting. To demonstrate of the effectiveness of the proposed model, we use a case study involving a smart phone.",2013
209,1309,A Probabilistic Graphical Model for Evaluating Variable Energy Consumption During the Use Stage of a Product’s Lifecycle,"Although energy consumption during use can cause a majority of a product’s environmental impact, the relationship between a product’s usage context and its environmental performance is rarely considered in design evaluations. Probabilistic graphical models (PGMs) provide the capability of evaluating uncertainty and variability of product use in addition to correlating the results with aspects of the usage context. This research demonstrates a method for representing the usage context as a PGM through the use of a lightweight vehicle design example. The demonstration PGM is constructed from factors such as driver behavior, alternative driving schedules, and residential density, which are related to local conditional probability distributions derived from publicly available data sources. Unique scenarios are then assembled from sets of conditions on these factors to provide insight into sources of variance in lifetime energy use. The vehicle example demonstrates that implementation of realistic usage scenarios via a PGM can provide a much higher fidelity investigation of energy savings during use than commonly found in the literature and that distinct scenarios can have significantly different implications for the effectiveness of lightweight vehicle designs.",2013
210,1310,An Envelope Approach to Time-Dependent Reliability Analysis for Mechanisms,"This work is concerned with the time-dependent mechanism reliability defined over a period of time where a certain motion output is required. An envelope approach is proposed to improve the accuracy of the time-dependent mechanism reliability analysis. The envelope function of the motion error over the time period is created. Since the envelope function is not explicitly related to time, the time-dependent problem is converted into a time-independent problem. Then the envelope function is approximated by piecewise hyper-planes. To find the expansion points of the hyper-planes, the approach linearizes the motion error at the means of random dimension variables, and this approximation is accurate because the tolerances or the variances of the dimension variables are small. Then the expansion points are found with the maximum probability density at the failure threshold. The time-dependent mechanism reliability is then estimated by a multivariable normal distribution function at the expansion points. As an example, analytical equations are derived for a four-bar function generating mechanism. The numerical example shows the significant accuracy improvement.",2013
211,1311,A Design Oriented Reliability Methodology for Fatigue Life Under Stochastic Loadings,"Fatigue damage analysis is critical for systems under stochastic loadings. To estimate the fatigue reliability at the design level, a hybrid reliability analysis method is proposed in this work. The First Order Reliability Method (FORM), the inverse FORM, and the peak distribution analysis are integrated for the fatigue reliability analysis at the early design stage. Equations for the mean value, the zero upcrossing rate, and the extreme stress distributions are derived for problems where stationary stochastic processes are involved. Then the fatigue damage is analyzed with the peak counting method. The developed methodology is demonstrated by a simple mathematical example and is then applied to the fatigue reliability analysis of a shaft under stochastic loadings. The results indicate the effectiveness of the proposed method in predicting fatigue damage and reliability.",2013
212,1312,Probabilistic Inverse Simulation and its Application in Vehicle Accident Reconstruction,"Inverse simulation is an inverse process of direct simulation. It determines unknown input variables of the direct simulation for a given set of simulation output variables. Uncertainties usually exist, making it difficult to solve inverse simulation problems. The objective of this research is to account for uncertainties in inverse simulation in order to produce high confidence in simulation results. The major approach is the use of the maximum likelihood methodology, which determines not only unknown deterministic input variables but also the realizations of random input variables. Both types of variables are solved on the condition that the joint probability density of all the random variables is maximum. The proposed methodology is applied to a traffic accident reconstruction problem where the simulation output (accident consequences) is known and the simulation input (velocities of the vehicle at the beginning of crash) is sought.",2013
213,1313,Robust Design of Gears With Material and Load Uncertainties,"Traditionally gears are designed using design standards such as AGMA, ISO, etc. These design standards include a large number of “design factors” accounting for various uncertainties related to geometry, load and material uncertainties. As the knowledge about these uncertainties increases, it becomes possible to include them systematically in the gear design procedure, thereby reducing the number of empirical design factors. In this paper a method is proposed to eliminate two design factors (viz., factor of safety in contact and reliability factor) used in standard AGMA-based design procedures through the formal introduction of uncertainty in the magnitude of load and material properties. The proposed method is illustrated via the design of an automotive gear with a desired reliability, cost, and robustness. The solutions obtained are encouraging and in-line with the existing knowledge about gear design, and thus reinforces the possibility of schematically reducing the aforementioned design factors.",2013
214,1314,Advanced Robust Optimization Approach for Design Optimization With Interval Uncertainty Using Sequential Quadratic Programming,"Uncertainty is inevitable in real world. It has to be taken into consideration, especially in engineering optimization; otherwise the obtained optimal solution may become infeasible. Robust optimization (RO) approaches have been proposed to deal with this issue. Most existing RO algorithms use double-looped structures in which a large amount of computational efforts have been spent in the inner loop optimization to determine the robustness of candidate solutions. In this paper, an advanced approach is presented where no optimization run is required to be performed for robustness evaluations in the inner loop. Instead, a concept of Utopian point is proposed and the corresponding maximum variable/parameter variation will be obtained by just solving a set of linear equations. The obtained robust optimal solution from the new approach may be conservative, but the deviation from the true robust optimal solution is very small given the significant improvement in the computational efficiency. Six numerical and engineering examples are tested to show the applicability and efficiency of the proposed approach, whose solutions and computational time are compared with those from a similar but double-looped approach, SQP-RO, proposed previously.",2013
215,1315,Time-Dependent Reliability of Dynamic Systems Using Subset Simulation With Splitting Over a Series of Correlated Time Intervals,"Time-dependent reliability is the probability that a system will perform its intended function successfully for a specified time. Unless many and often unrealistic assumptions are made, the accuracy and efficiency of time-dependent reliability estimation are major issues which may limit its practicality. Monte Carlo simulation (MCS) is accurate and easy to use but it is computationally prohibitive for high dimensional, long duration, time-dependent (dynamic) systems with a low failure probability. This work addresses systems with random parameters excited by stochastic processes. Their response is calculated by time integrating a set of differential equations at discrete times. The limit state functions are therefore, explicit in time and depend on time-invariant random variables and time-dependent stochastic processes. We present an improved subset simulation with splitting approach by partitioning the original high dimensional random process into a series of correlated, short duration, low dimensional random processes. Subset simulation reduces the computational cost by introducing appropriate intermediate failure sub-domains to express the low failure probability as a product of larger conditional failure probabilities. Splitting is an efficient sampling method to estimate the conditional probabilities. The proposed subset simulation with splitting not only estimates the time-dependent probability of failure at a given time but also estimates the cumulative distribution function up to that time with approximately the same cost. A vibration example involving a vehicle on a stochastic road demonstrates the advantages of the proposed approach.",2013
216,1316,Accounting for Test Variability Through Sizing Local Domains in Sequential Design Optimization With Concurrent Calibration-Based Model Validation,"We have recently proposed a new method for combined design optimization and calibration-based validation using a sequential approach with variable-size local domains of the design space and statistical bootstrap techniques. Our work was motivated by the fact that model validation in the entire design space may be neither affordable nor necessary. The method proceeds iteratively by obtaining test data at a design point, constructing around it a local domain in which the model is considered valid, and optimizing the design within this local domain. Due to test variability, it is important to know how many tests are needed to size each local domain of the sequential optimization process. Conducting an unnecessarily large number of tests may be inefficient, while a small number of tests may be insufficient to achieve the desired validity level. In this paper, we introduce a technique to determine the number of tests required to account for their variability by sizing the local domains accordingly. The goal is to achieve a desired level of model validation in each domain using the correlation between model data at the center and any other point in the local domain. The proposed technique is illustrated by means of a piston design example.",2013
217,1317,Reliability and Functionality of Repairable Systems Using a Minimal Set of Metrics: Design and Maintenance of a Smart Charging Microgrid,"The definition of reliability may not be readily applicable for repairable systems. Our recent work has shown that multiple metrics are needed to fully account for the performance of a repairable system under uncertainty. Optimal tradeoffs among a minimal set of metrics can be used in the design and maintenance of these systems. A minimal set of metrics provides the most information about the system with the smallest number of metrics using a set of desirable properties. Critical installations such as a remote microgrid powering a military installation require a careful consideration of cost and repair strategies. This is because of logistical challenges in performing repairs and supplying necessary spare parts, particularly in unsafe locations. This paper shows how a minimal set of metrics enhances decision making in such a scenario. It enables optimal tradeoffs between critical attributes in decision making, while guaranteeing that all important performance measures are satisfied. As a result, cost targets and inventory planning can be achieved in an optimal way. We demonstrate the value of the proposed approach using a US Army smart-charging microgrid installation.",2013
218,1318,Preposterior Analysis to Select Experimental Responses for Improving Identifiability in Model Uncertainty Quantification,"In physics-based engineering modeling and uncertainty quantification, distinguishing the effects of two main sources of uncertainty — calibration parameter uncertainty and model discrepancy — is challenging. Previous research has shown that identifiability can sometimes be improved by experimentally measuring multiple responses of the system that share a mutual dependence on a common set of calibration parameters. In this paper, we address the issue of how to select the most appropriate subset of responses to measure experimentally, to best enhance identifiability. We propose a preposterior analysis approach that, prior to conducting the physical experiments but after conducting computer simulations, can predict the degree of identifiability that will result using different subsets of responses to measure experimentally. We quantify identifiability via the posterior covariance of the calibration parameters, and predict it via the preposterior covariance from a modular Bayesian Monte Carlo analysis of a multi-response Gaussian process model. The proposed method is applied to a simply supported beam example to select two out of six responses to best improve identifiability. The estimated preposterior covariance is compared to the actual posterior covariance to demonstrate the effectiveness of the method.",2013
219,1319,A Maximum Confidence Enhancement Based Sequential Sampling Scheme for Simulation-Based Design,"This paper presents a maximum confidence enhancement based sequential sampling approach for simulation-based design under uncertainty. In the proposed approach, the ordinary Kriging method is adopted to construct surrogate models for all constraints and thus Monte Carlo simulation (MCS) is able to be used to estimate reliability and its sensitivity with respect to design variables. A cumulative confidence level is defined to quantify the accuracy of reliability estimation using MCS based on the Kriging models. To improve the efficiency of proposed approach, a maximum confidence enhancement based sequential sampling scheme is developed to update the Kriging models based on the maximum improvement of the defined cumulative confidence level, in which a sample that produces the largest improvement of the cumulative confidence level is selected to update the surrogate models. Moreover, a new design sensitivity estimation approach based upon constructed Kriging models is developed to estimate the reliability sensitivity information with respect to design variables without incurring any extra function evaluations. This enables to compute smooth sensitivity values and thus greatly enhances the efficiency and robustness of the design optimization process. Two case studies are used to demonstrate the proposed methodology.",2013
220,1320,Simulating Stochastic Diffusions by Quantum Walks,"Stochastic differential equation (SDE) and Fokker-Planck equation (FPE) are two general approaches to describe the stochastic drift-diffusion processes. Solving SDEs relies on the Monte Carlo samplings of individual system trajectory, whereas FPEs describe the time evolution of overall distributions via path integral alike methods. The large state space and required small step size are the major challenges of computational efficiency in solving FPE numerically. In this paper, a generic continuous-time quantum walk formulation is developed to simulate stochastic diffusion processes. Stochastic diffusion in one-dimensional state space is modeled as the dynamics of an imaginary-time quantum system. The proposed quantum computational approach also drastically accelerates the path integrals with large step sizes. The new approach is compared with the traditional path integral method and the Monte Carlo trajectory sampling.",2013
221,1321,First-Order Reliability Analysis of Vehicle Safety in Highway Horizontal Curves,"This study presents a reliability analysis of vehicle sideslip and rollover in highway horizontal curves, mainly focusing on exit ramps and interchanges. To accurately describe failure modes of a ground vehicle, analytic models for sideslip and rollover are derived considering nonlinear characteristics of vehicle behavior using the commercial software, TruckSim",2013
222,1322,Sampling-Based Approach for Design Optimization in the Presence of Interval Variables,"This paper proposes a methodology for sampling-based design optimization in the presence of interval variables. Assuming that an accurate surrogate model is available, the proposed method first searches the worst combination of interval variables for constraints when only interval variables are present or for probabilistic constraints when both interval and random variables are present. Due to the fact that the worst combination of interval variables for probability of failure does not always coincide with that for a performance function, the proposed method directly uses the probability of failure to obtain the worst combination of interval variables when both interval and random variables are present. To calculate sensitivities of constraints and probabilistic constraints with respect to interval variables by the sampling-based method, the behavior of interval variables at the worst case is defined by utilizing the Dirac delta function. Then, Monte Carlo simulation is applied to calculate constraints and probabilistic constraints with the worst combination of interval variables, and their sensitivities. The important merit of the proposed method is that it does not require gradients of performance functions and transformation from X-space to U-space for reliability analysis after the worst combination of interval variables is obtained, thus there is no approximation or restriction in calculating the sensitivities of constraints or probabilistic constraints. Numerical results indicate that the proposed method can search the worst case probability of failure with both efficiency and accuracy and that it can perform design optimization with mixture of random and interval variables by utilizing the worst case probability of failure search.",2013
223,1323,Stochastic Kriging for Random Simulation Metamodeling With Finite Sampling,"As a metamodeling method, Kriging has been intensively developed for deterministic design in the past few decades. However, Kriging is not able to deal with the uncertainty of many engineering processes. By incorporating the uncertainty of data, Stochastic Kriging methods has been developed to analyze and predict random simulation results, but the results cannot fit the problem with uncertainty well. In this paper, deterministic Kriging are extended to stochastic space theoretically, where a novel form of Stochastic Kriging that fully considers the intrinsic uncertainty of data and number of replications is proposed on the basis of finite inputs. It formulates a more reasonable optimization problem via a stochastic process, and then derives the spatial correlation models underlying a random simulation. The obtained results are more general than Kriging, which can fit well with many uncertainty-based problems. Three examples will illustrate the method’s application through comparison with the existing methods: the novel method shows that the results are much closer to reality.",2013
